[
  {
    "objectID": "posts/Visualization plotly/climate.html",
    "href": "posts/Visualization plotly/climate.html",
    "title": "Using Geographic, Boxplot, and Lineplot Visualizations to illustrate Global Warming",
    "section": "",
    "text": "import sqlite3\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\nfrom climate_database import (query_climate_database, temperature_coefficient_plot,\nquery_equator, temperature_box_plot, temperature_lineplot_equator)\n\n\n1. Database Creation\nFirst, create three separate tables: temperatures, countries, and stations table. To do so, I wrote a function named prepare_df to prepare the temperature dataset such that the data is easier to understand.\n\nThe temperatures table will contain station ID, year of measurement, month of measurement, and average temperature.\nThe countries table will contain country names and their corresponding country codes.\nThe stations table will contain station ID, latitude, longitude, elevation, and the name of the station.\n\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    df[\"NewID\"] = df[\"ID\"].str[0:2]\n    return(df)\n\n\nwith sqlite3.connect(\"HW1.db\") as conn: # create a database in current directory called temps.db\n    df_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(df_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    url = \"station-metadata.csv\"\n    stations = pd.read_csv(url)\n    stations[\"NewID\"] = stations[\"ID\"].str[0:2]\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries = pd.read_csv(\"countries.csv\")\n    countries = countries.rename(columns={\"FIPS 10-4\": \"NewID\"})\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\nTemperatures dataframe\n\ndf.head() \n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\nNewID\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\nUS\n\n\n1\nUSW00014924\n2016\n2\n-8.40\nUS\n\n\n2\nUSW00014924\n2016\n3\n-0.20\nUS\n\n\n3\nUSW00014924\n2016\n4\n3.21\nUS\n\n\n4\nUSW00014924\n2016\n5\n13.85\nUS\n\n\n\n\n\n\n\nStations dataframe\n\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\nNewID\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\nAC\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\nAE\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\nAE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\nAE\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\nAE\n\n\n\n\n\n\n\nCountries dataframe\n\ncountries.head()\n\n\n\n\n\n\n\n\nNewID\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\ndb_file = \"HW1.db\" #database we created \n\n\n\n2. Query Function\nBelow I created a query function query_climate_database which returns a dataframe based off of filters. The parameters specify the country, the year we start from, the year we end at, and the month we wish to analyze. The dataframe displays station name, latitude, longitude, and average temperature of the station during the month and year.\n\ninspect.getsource(query_climate_database)\n\n'def query_climate_database(db_file, country, year_begin, year_end, month):\\n    \"\"\"\\n    Query function to filter climate data \\n    \\n    Args:\\n    country: country of interest \\n    year_begin: start year desired \\n    year_end: end year desired \\n    month: a specific month \\n    \\n    Return:\\n    Dataframe of filtered data \\n    \"\"\"\\n    with sqlite3.connect(db_file) as conn:\\n        #select Name, Latitiude, and Longitude from stations table\\n        #select country from countries table,\\n        #select Year, Month, and Temperature from temperatures table\\n        cmd = f\"\"\"\\n        SELECT S.Name, S.LATITUDE, S.LONGITUDE, C.Name \"Country\", T.Year, T.Month, T.Temp\\n        FROM stations S \\n        LEFT JOIN countries C ON C.NewID = S.NewID\\n        RIGHT JOIN temperatures T ON S.ID = T.ID \\n        WHERE T.Year &lt;= {year_end} AND T.Year &gt;= {year_begin} AND T.Month = {month} AND C.Name = \"{country}\"\\n        \"\"\"\\n        #Combine tables on ID number and subset where the year is between year_end and year_begin, \\n        #subset where the month and country are provided parameters\\n        df = pd.read_sql_query(cmd, conn)\\n        return(df)\\n'\n\n\nBelow is an example with parameters country India, year_begin 1980, year_end 2020, and month 1\n\ndf = query_climate_database(db_file = \"HW1.db\", \n                            country = \"India\",\n                            year_begin = 1980, \n                            year_end = 2020, \n                            month = 1)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\n3. Geographic Mapbox Scatterplot\nWe use query_climate_database to create another function temperature_coefficient plot which answers how the average yearly change in temperature varies within a country. The function accepts the same parameters, as well as min_obs which will filters stations which do not have the specified number of data points. We also add **kwargs to customize our mapbox scatterplot we made using px.scatter_mapbox().\nInside our function, I wrote another function coef which conducts linear regression in order to compute how each station changes in average temperature over time.\nBelow we create a sample plot with the same parameters we used in the query function along with new **kwargs parameters for the scatter_mapbox.\n\ninspect.getsource(temperature_coefficient_plot)\n\n'def temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\\n    \"\"\"\\n    Construct a geographic mapbox scatterplot of yearly temperature increase\\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year \\n    month: a specific month \\n    min_obs: minimum required observations of station\\n    **kwargs: additional keyword arguments\\n    \\n    Return:\\n    Scatter_mapbox plot\\n    \"\"\"\\n    import calendar \\n    #get the average increase through taking the coefficient of linear regression \\n    def coef(data_group):\\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\\n        LR = LinearRegression()\\n        LR.fit(x, y)\\n        return LR.coef_[0]\\n    #call the query function we created above \\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\\n    #get the amount of observations per station\\n    test = df.groupby([\"NAME\"]).size()\\n    #index our dataframe such that the ones with more than min obs are kept\\n    names = test[test &gt;= min_obs].index\\n    df = df[df.NAME.isin(names)]\\n    #get the coefficients for each station during said month\\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef).round(4)\\n    plot_df = pd.DataFrame(coefs)\\n    #merge coefficients with dataframe above\\n    merged_df = pd.merge(df, plot_df, on=\\'NAME\\')\\n    merged_df = merged_df.rename(columns={0: \"Estimated Yearly Increase (Celcius)\"})\\n    #create a scatter_mapbox given coordinate points and hover over station name with the color equal to the coefficient increase\\n    fig = px.scatter_mapbox(merged_df, \\n                            lat = \"LATITUDE\",\\n                            lon = \"LONGITUDE\", \\n                            hover_name=\"NAME\", \\n                            color_continuous_midpoint = 0,\\n                            color = \"Estimated Yearly Increase (Celcius)\",\\n                            title = (f\"\"\"Estimates of yearly increase in {calendar.month_name[month]} \\n                             &lt;br&gt;temperature for stations in {country}, from {year_begin} to {year_end}\"\"\"),\\n                            **kwargs)\\n    return(fig)\\n'\n\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(db_file, \"India\", 1980, 2020, 1, 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\nfig.show()\n\n\n\n\nNow let us try this with a new example. Here I used the parameters country = Portugal, year_begin = 1980, year_end = 2020, and month = 2.\n\nfig = temperature_coefficient_plot(db_file, \"Portugal\", 1980, 2020, 2, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\n4. Additional Query Function and Plotly Boxplot and Lineplot\nI decided to write a query function which filters out only the equator countries. The reason why I did this was because I believe that it is valuable to see how the temperature changes each year from these equator countries because it may illustrate that even at the hottest parts of our world, the temperature increases, potentially infering global warming.\nHere I created a dataframe of just equator countries from 2010 - 2020.\n\ninspect.getsource(query_equator)\n\n'def query_equator(db_file, year_begin, year_end):\\n    \"\"\"\\n    Query function of only the Equator countries\\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year\\n    \\n    Return:\\n    Dataframe with desired filters \\n    \"\"\"\\n    with sqlite3.connect(db_file) as conn:\\n        #subset between year_begin and year_end and only the countries which lie on the equator \\n        cmd = \\\\\\n        f\"\"\"\\n        SELECT S.Name, S.LATITUDE, S.LONGITUDE, C.Name \"Country\", T.Year, T.Month, T.Temp\\n        FROM stations S \\n        LEFT JOIN countries C ON C.NewID = S.NewID\\n        RIGHT JOIN temperatures T ON S.ID = T.ID \\n        WHERE T.Year &lt;= {year_end} AND T.Year &gt;= {year_begin} AND S.LATITUDE &gt;= -5 AND S.LATITUDE &lt;= 5\\n        AND (C.Name = \"Congo, Democratic Republic of the\" OR C.Name = \"Gabon\" \\n        OR C.Name = \"Somalia\" \\n        OR C.Name = \"Congo, Republic of the\" OR C.Name = \"Uganda\" \\n        OR C.Name = \"Maldives\" OR C.Name = \"Indonesia\" OR C.Name = \"Kiribati\" \\n        OR C.Name = \"Sao Tome and Principe\" OR C.Name = \"Kenya\" \\n        OR C.Name = \"Ecuador\" OR C.Name = \"Colombia\" OR C.Name = \"Brazil\")\\n        \"\"\" \\n        df = pd.read_sql_query(cmd, conn)\\n        return(df)\\n'\n\n\n\ndf = query_equator(db_file, 2010, 2020)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n1\n27.69\n\n\n1\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n2\n28.11\n\n\n2\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n3\n28.85\n\n\n3\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n4\n27.79\n\n\n4\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n5\n28.27\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16227\nMASINDI\n1.683\n31.717\nUganda\n2011\n8\n21.98\n\n\n16228\nMASINDI\n1.683\n31.717\nUganda\n2011\n9\n22.09\n\n\n16229\nMASINDI\n1.683\n31.717\nUganda\n2011\n10\n23.01\n\n\n16230\nMASINDI\n1.683\n31.717\nUganda\n2011\n11\n23.29\n\n\n16231\nMASINDI\n1.683\n31.717\nUganda\n2011\n12\n24.50\n\n\n\n\n16232 rows × 7 columns\n\n\n\nThe temperature_box_plot function below addresses how the average temperature of countries on the equator change year to year. I made sure that the function uses the same station for every year such that the data is not unevenly weighted. The plot which this function produces is important because we can see if our temperature is rising or lowering as time goes on. And as we can see from the figure below, there is a slight gradual increase in the boxplot medians of Brazil, providing visual evidence that temperatures are increasing for equator countries.\n\ninspect.getsource(temperature_box_plot)\n\n'def temperature_box_plot(db_file, country, year_begin, year_end, **kwargs):\\n    \"\"\"\\n    Construct boxplot of yearly average temperatures of specified equator country \\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year \\n    \\n    Return:\\n    px.box plot of yearly average temperatures of specified equator country\\n    \"\"\"\\n    #call SQL query for just equator countries\\n    df = query_equator(db_file, year_begin, year_end)\\n    #subset only on country parameter given\\n    df = df[df.Country == country]\\n    #calculate mean temperature for each station\\'s year\\n    new = df.groupby([\"NAME\", \"Year\"])[\"Temp\"].aggregate(np.mean)\\n    #get the amount of observations per Name and Year \\n    test = df.groupby([\"NAME\", \"Year\"]).size()\\n    test = test.reset_index()\\n    #make sure that the name and year have at least year_end - year_begin + 1 observations such that our boxplot looks full\\n    names = test.groupby([\"NAME\"]).size() &gt;= year_end - year_begin + 1 \\n    names = names[names].index\\n    plot_df = pd.DataFrame(new)\\n    plot_df = plot_df.reset_index()\\n    plot_df = plot_df[plot_df.NAME.isin(names)]\\n    #create boxplot off of created dataframe\\n    fig = px.box(df,\\n                 x = \"Country\",\\n                 y = \"Temp\",\\n                 color = \"Year\",\\n                 title = f\"Boxplots of {country}\\'s Average Temperature by Year from {year_begin} to {year_end}\",\\n                 category_orders={\"Year\": [ix for ix in range(year_begin, year_end)]},\\n                 **kwargs\\n                )\\n    return fig\\n'\n\n\n\nfig = temperature_box_plot(db_file, \"Brazil\", 1980, 2020)\nfig.show()\n\n\n\n\nI constructed the temperature_lineplot_equator function to display a lineplot of how the temperature changes over time in each month, and each colored line on the lineplot represents a different station. I felt this was important because like the boxplot it was another way to visualize how there is an increase in temperature in equator countries. For instance, as we can see below the Average Temperature of Station in Indonesia from 1985 to 2020 increased for all 4 stations plotted.\n\ninspect.getsource(temperature_lineplot_equator)\n\n'def temperature_lineplot_equator(db_file, year_begin, year_end, country, **kwargs):\\n    \"\"\"\\n    Construct lineplot of yearly average temperatures of specified equator country in month\\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year \\n    month: specific month of year\\n    **kwargs: keyword arguments for lineplot\\n    \\n    Return:\\n    px.line plot of yearly average temperatures of specified equator country\\n    \"\"\"\\n    import calendar \\n    #call the SQL query for just equator countries\\n    df = query_equator(db_file, year_begin, year_end)\\n    #create date variable given year and month\\n    df[\"Date\"] = pd.to_datetime(df[\"Year\"].astype(str) + \"-\" + df[\"Month\"].astype(str))\\n    #subset on country parameter\\n    df = df[df.Country == country]\\n    #calculate mean for each station\\'s year and month\\n    new = df.groupby([\"NAME\", \"Year\", \"Month\"])[\"Temp\"].aggregate(np.mean)\\n    #get the number of observations per station year and month\\n    test = df.groupby([\"NAME\", \"Year\", \"Month\"]).size()\\n    test = test.reset_index()\\n    #make sure minimum observations are year_end - year_begin * 10 -5 such that our line plots are full\\n    names = test.groupby([\"NAME\"]).size() &gt;= (year_end - year_begin)*10 - 5  \\n    names = names[names].index\\n    plot_df = pd.DataFrame(new)\\n    plot_df = plot_df.reset_index()\\n    plot_df = plot_df[plot_df.NAME.isin(names)]\\n    #create lineplot based upon year and temperature and month face meaning we should get 12 different line plots\\n    fig = px.line(data_frame = plot_df,\\n                  x = \"Year\",\\n                  y = \"Temp\",\\n                  color = \"NAME\",\\n                  facet_row=\"Month\",\\n                  title = (f\"\"\"Average Temperature of Station in {country} &lt;br&gt;\\n                           by Year from {year_begin} to {year_end}\"\"\"),\\n                  height = 5000,\\n                  category_orders={\"Month\": [1,2,3,4,5,6,7,8,9,10,11,12]}\\n                  )\\n    return(fig)\\n'\n\n\n\nplot = temperature_lineplot_equator(db_file, 1985, 2020, \"Indonesia\")\nplot.show()\n\n\n\n\n\n\n5. Concluding Remarks\nAs we can see from the query functions and graphics constructed, there is overwhelming evidence for global warming. The mapbox plot, boxplot, and line plots all show that there is a majority increase in temperature, and this conclusion was able to be realized because the visualizations helped to easily explain the data."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html",
    "href": "posts/image classification/ClassificationDogCat (1).html",
    "title": "Dogs and Cats Classification",
    "section": "",
    "text": "In this post, we train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models and observe which ones the best!"
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#visualize-our-data",
    "href": "posts/image classification/ClassificationDogCat (1).html#visualize-our-data",
    "title": "Dogs and Cats Classification",
    "section": "Visualize our data",
    "text": "Visualize our data\nNow, let us create a function called cats_dogs_plot to visualize our dataset. We use dataset.take(1) to access the first batch of our data, which in this case is 64 images, and we use matplotlib to plot 3 images of cats in the first row and 3 images of dogs in the second row.\n\ndef cats_dogs_plot(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0): #if our label == 0 or equals a cat\n                if cats &lt;= 3: #once we get at least three cats, stop plotting\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(\"cats\")\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1): #if our label == 1 or equals a dog\n                if dogs &lt;= 6: #once we get at leaset three dogs, stop plotting\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(\"dogs\")\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\ncats_dogs_plot(train_ds)\n\n\n\n\n\n\n\n\nAs you can see, our function works as intended!"
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#analyzing-our-dataset",
    "href": "posts/image classification/ClassificationDogCat (1).html#analyzing-our-dataset",
    "title": "Dogs and Cats Classification",
    "section": "Analyzing our dataset",
    "text": "Analyzing our dataset\nNow, let us see how many dogs and cats images there are total in our training set. To do so, we create an iterator called labels_iterator and if the label equals 0, we add a tally to our cats variable and if the label equals 1, we add a tally to our dogs variable.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncats = dogs = 0\nfor element in labels_iterator: #for each label in our labels_iterator\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\n\ncats, dogs\n\n(4637, 4668)\n\n\n\ndogs/(cats+dogs)\n\n0.5016657710908113\n\n\nIt appears that our dataset is nearly balanced. There are 4637 cats and 4668 dogs, meaning that if we were to guess all dogs, we would have a 50.16 percent accuracy rate."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model stabilized around 56 to 60 percent\nCompared with our baseline of 50.16 percent, our model did a bit better, but we can still do much better\nThere is a significant overfitting issue as our training accuracy is around 98 percent while our validation accuracy is nearly thirty percent lower."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments-1",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments-1",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model now appears to be around 75 percent\nThis is significantly better than the first model’s accuracy of 55 percent and our baseline of 50.16 percent.\nThere does not appear to be overfitting as our validation accuracy is better than our training accuracy"
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments-2",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments-2",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model is between around 80 to 82 percent.\nThe accuracy of model3 is much higher than model1. In fact, it is around 30 percent better which is a very signficant difference in accuracy\nThere does not appear to be overfitting as the validation error is just slightly worse than the training error."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments-3",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments-3",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model stabilizes around 96 to 97 percent\nThis accuracy is significantly better than model1 and all the other models which we have tested\nNo overfitting issues seem to be present as our validation error actually outperforms our training error by one percent!"
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html",
    "href": "posts/Fakenews/Fakenews.html",
    "title": "Fake News?",
    "section": "",
    "text": "In this blog post, we will construct a fake news classifer using TensorFlow."
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#title-model",
    "href": "posts/Fakenews/Fakenews.html#title-model",
    "title": "Fake News?",
    "section": "Title Model",
    "text": "Title Model\nThe first model only uses the article title. Because our second model will be an exact copy of our first model, only using the article text instead of article title, we will create a function called input_features such that we can reuse the code and streamline our process. The parameter the function takes in will be whether the input is title_input or text_input.\n\ndef input_features(feature_input):\n  new_features = text_vectorize_layer(feature_input)\n  new_features = layers.Embedding(size_vocabulary, output_dim = 4, name=\"embedding\")(new_features)\n  new_features = layers.Dropout(0.2)(new_features)\n  new_features = layers.GlobalAveragePooling1D()(new_features)\n  new_features = layers.Dropout(0.2)(new_features)\n  new_features = layers.Dense(2, activation='relu', name=\"fake\")(new_features)\n\n  model = keras.Model(\n    # only using text\n    inputs = [feature_input],\n    outputs = new_features\n  )\n  return model\n\n\n# only using title\nmodel1 = input_features(title_input)\nmodel1.summary()\n\nModel: \"model_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding (Embedding)       (None, 500, 4)            8000      \n                                                                 \n dropout_10 (Dropout)        (None, 500, 4)            0         \n                                                                 \n global_average_pooling1d_5  (None, 4)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dropout_11 (Dropout)        (None, 4)                 0         \n                                                                 \n fake (Dense)                (None, 2)                 10        \n                                                                 \n=================================================================\nTotal params: 8010 (31.29 KB)\nTrainable params: 8010 (31.29 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nutils.plot_model(model1)\n\n\n\n\n\n\n\n\n\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 [==============================] - 12s 61ms/step - loss: 0.6932 - accuracy: 0.4756 - val_loss: 0.6931 - val_accuracy: 0.4742\nEpoch 2/20\n180/180 [==============================] - 1s 8ms/step - loss: 0.6931 - accuracy: 0.4766 - val_loss: 0.6931 - val_accuracy: 0.4798\nEpoch 3/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6931 - accuracy: 0.4756 - val_loss: 0.6931 - val_accuracy: 0.4856\nEpoch 4/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6925 - accuracy: 0.4748 - val_loss: 0.6913 - val_accuracy: 0.4798\nEpoch 5/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.6903 - accuracy: 0.4767 - val_loss: 0.6886 - val_accuracy: 0.4836\nEpoch 6/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.6872 - accuracy: 0.4777 - val_loss: 0.6852 - val_accuracy: 0.4734\nEpoch 7/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6831 - accuracy: 0.4782 - val_loss: 0.6805 - val_accuracy: 0.4776\nEpoch 8/20\n180/180 [==============================] - 1s 8ms/step - loss: 0.6782 - accuracy: 0.4762 - val_loss: 0.6761 - val_accuracy: 0.4704\nEpoch 9/20\n180/180 [==============================] - 1s 8ms/step - loss: 0.6735 - accuracy: 0.4737 - val_loss: 0.6715 - val_accuracy: 0.4702\nEpoch 10/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6677 - accuracy: 0.4755 - val_loss: 0.6637 - val_accuracy: 0.4873\nEpoch 11/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6614 - accuracy: 0.4817 - val_loss: 0.6590 - val_accuracy: 0.4736\nEpoch 12/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6567 - accuracy: 0.4768 - val_loss: 0.6523 - val_accuracy: 0.4829\nEpoch 13/20\n180/180 [==============================] - 1s 7ms/step - loss: 0.6515 - accuracy: 0.4771 - val_loss: 0.6495 - val_accuracy: 0.4667\nEpoch 14/20\n180/180 [==============================] - 2s 10ms/step - loss: 0.6464 - accuracy: 0.4763 - val_loss: 0.6417 - val_accuracy: 0.4776\nEpoch 15/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6414 - accuracy: 0.4774 - val_loss: 0.6404 - val_accuracy: 0.4702\nEpoch 16/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6362 - accuracy: 0.4756 - val_loss: 0.6344 - val_accuracy: 0.4771\nEpoch 17/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6308 - accuracy: 0.4791 - val_loss: 0.6264 - val_accuracy: 0.4838\nEpoch 18/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.6093 - accuracy: 0.6784 - val_loss: 0.5800 - val_accuracy: 0.8389\nEpoch 19/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5705 - accuracy: 0.8155 - val_loss: 0.5525 - val_accuracy: 0.8511\nEpoch 20/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5498 - accuracy: 0.8197 - val_loss: 0.5359 - val_accuracy: 0.8542\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning:\n\nInput dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n\n\n\nLet us construct a graph of the model’s training and validation accuracy. We create a function called validation_plot to streamline this process.\n\ndef validation_plot(model):\n  plt.plot(history.history[\"accuracy\"], label = \"training\")\n  plt.plot(history.history[\"val_accuracy\"], label = \"validation\")\n  plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n  plt.legend()\n  return plt\n\n\nvalidation_plot(history)\n\n\n\n\n\n\n\n\nAs we can see from our graph, the validation accuracy is around 86 to 97 percent and there does not appear to be overfitting, illustrating that our dropout layers are working as intended and we should not change them for our later models."
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#text-model",
    "href": "posts/Fakenews/Fakenews.html#text-model",
    "title": "Fake News?",
    "section": "Text Model",
    "text": "Text Model\nOur second model will only use the article text. Because we will reuse the same structure as the first model, we call our input_features function.\n\nmodel2 = input_features(text_input)\nmodel2.summary()\n\nModel: \"model_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding (Embedding)       (None, 500, 4)            8000      \n                                                                 \n dropout_12 (Dropout)        (None, 500, 4)            0         \n                                                                 \n global_average_pooling1d_6  (None, 4)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dropout_13 (Dropout)        (None, 4)                 0         \n                                                                 \n fake (Dense)                (None, 2)                 10        \n                                                                 \n=================================================================\nTotal params: 8010 (31.29 KB)\nTrainable params: 8010 (31.29 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nutils.plot_model(model2)\n\n\n\n\n\n\n\n\n\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 [==============================] - 11s 53ms/step - loss: 0.6860 - accuracy: 0.5298 - val_loss: 0.6788 - val_accuracy: 0.5224\nEpoch 2/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.6554 - accuracy: 0.6913 - val_loss: 0.6208 - val_accuracy: 0.8551\nEpoch 3/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.5951 - accuracy: 0.8422 - val_loss: 0.5605 - val_accuracy: 0.9082\nEpoch 4/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.5359 - accuracy: 0.8754 - val_loss: 0.5008 - val_accuracy: 0.9282\nEpoch 5/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.4800 - accuracy: 0.8984 - val_loss: 0.4502 - val_accuracy: 0.9358\nEpoch 6/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.4341 - accuracy: 0.9135 - val_loss: 0.4014 - val_accuracy: 0.9453\nEpoch 7/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3938 - accuracy: 0.9279 - val_loss: 0.3691 - val_accuracy: 0.9453\nEpoch 8/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3598 - accuracy: 0.9354 - val_loss: 0.3284 - val_accuracy: 0.9492\nEpoch 9/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.3299 - accuracy: 0.9397 - val_loss: 0.3094 - val_accuracy: 0.9542\nEpoch 10/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3048 - accuracy: 0.9437 - val_loss: 0.2876 - val_accuracy: 0.9536\nEpoch 11/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2839 - accuracy: 0.9484 - val_loss: 0.2624 - val_accuracy: 0.9576\nEpoch 12/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.2650 - accuracy: 0.9508 - val_loss: 0.2445 - val_accuracy: 0.9462\nEpoch 13/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.2492 - accuracy: 0.9523 - val_loss: 0.2247 - val_accuracy: 0.9624\nEpoch 14/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2346 - accuracy: 0.9563 - val_loss: 0.2226 - val_accuracy: 0.9616\nEpoch 15/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2251 - accuracy: 0.9553 - val_loss: 0.2088 - val_accuracy: 0.9607\nEpoch 16/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2115 - accuracy: 0.9591 - val_loss: 0.1950 - val_accuracy: 0.9651\nEpoch 17/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.2028 - accuracy: 0.9609 - val_loss: 0.1820 - val_accuracy: 0.9702\nEpoch 18/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1927 - accuracy: 0.9641 - val_loss: 0.1745 - val_accuracy: 0.9700\nEpoch 19/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.1864 - accuracy: 0.9642 - val_loss: 0.1678 - val_accuracy: 0.9696\nEpoch 20/20\n180/180 [==============================] - 3s 14ms/step - loss: 0.1773 - accuracy: 0.9664 - val_loss: 0.1633 - val_accuracy: 0.9671\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning:\n\nInput dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n\n\n\n\nvalidation_plot(history)\n\n\n\n\n\n\n\n\nAs we can see, our our validation accuracy is much better than our first model, sitting at around 96 percent. This is expected as text reveals a lot more than just the title because there are more words to analyze. Let us see if we can improve this model further through taking into account both text and title variables."
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#text-and-title-model",
    "href": "posts/Fakenews/Fakenews.html#text-and-title-model",
    "title": "Fake News?",
    "section": "Text and Title Model",
    "text": "Text and Title Model\nOur third model will use both the article text and title. Because we don’t want to share an embedding layer, we must make one for both title and text.\n\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\n\n# Because we do not want to share an embedding layer, we make one for title and text\ntitle_embedding = layers.Embedding(size_vocabulary, output_dim = 4)\ntext_embedding = layers.Embedding(size_vocabulary, output_dim = 4)\ntitle_features = title_embedding(title_features)\ntext_features = text_embedding(text_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n#cocatenate the two features together\ntogether = layers.concatenate([title_features, text_features], axis = 1)\ntogether = layers.Dropout(0.2)(together)\ntogether = layers.GlobalAveragePooling1D()(together)\ntogether = layers.Dropout(0.2)(together)\ntogether = layers.Dense(2, activation='relu', name = 'fake')(together)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = together\n)\n\nmodel3.summary()\n\nModel: \"model_10\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n title (InputLayer)          [(None, 1)]                  0         []                            \n                                                                                                  \n text (InputLayer)           [(None, 1)]                  0         []                            \n                                                                                                  \n text_vectorization (TextVe  (None, 500)                  0         ['title[0][0]']               \n ctorization)                                                                                     \n                                                                                                  \n text_vectorization_1 (Text  (None, 500)                  0         ['text[0][0]']                \n Vectorization)                                                                                   \n                                                                                                  \n embedding_2 (Embedding)     (None, 500, 4)               8000      ['text_vectorization[3][0]']  \n                                                                                                  \n embedding_3 (Embedding)     (None, 500, 4)               8000      ['text_vectorization_1[6][0]']\n                                                                                                  \n dense_2 (Dense)             (None, 500, 32)              160       ['embedding_2[0][0]']         \n                                                                                                  \n dense_3 (Dense)             (None, 500, 32)              160       ['embedding_3[0][0]']         \n                                                                                                  \n concatenate_1 (Concatenate  (None, 1000, 32)             0         ['dense_2[0][0]',             \n )                                                                   'dense_3[0][0]']             \n                                                                                                  \n dropout_16 (Dropout)        (None, 1000, 32)             0         ['concatenate_1[0][0]']       \n                                                                                                  \n global_average_pooling1d_8  (None, 32)                   0         ['dropout_16[0][0]']          \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n dropout_17 (Dropout)        (None, 32)                   0         ['global_average_pooling1d_8[0\n                                                                    ][0]']                        \n                                                                                                  \n fake (Dense)                (None, 2)                    66        ['dropout_17[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 16386 (64.01 KB)\nTrainable params: 16386 (64.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\nkeras.utils.plot_model(model3)\n\n\n\n\n\n\n\n\n\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 [==============================] - 19s 94ms/step - loss: 0.6873 - accuracy: 0.5399 - val_loss: 0.6697 - val_accuracy: 0.5378\nEpoch 2/20\n180/180 [==============================] - 6s 35ms/step - loss: 0.6137 - accuracy: 0.7468 - val_loss: 0.5256 - val_accuracy: 0.9135\nEpoch 3/20\n180/180 [==============================] - 3s 19ms/step - loss: 0.4502 - accuracy: 0.8619 - val_loss: 0.3536 - val_accuracy: 0.9429\nEpoch 4/20\n180/180 [==============================] - 6s 33ms/step - loss: 0.3293 - accuracy: 0.9011 - val_loss: 0.2595 - val_accuracy: 0.9518\nEpoch 5/20\n180/180 [==============================] - 4s 19ms/step - loss: 0.2588 - accuracy: 0.9289 - val_loss: 0.1988 - val_accuracy: 0.9584\nEpoch 6/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.2099 - accuracy: 0.9437 - val_loss: 0.1639 - val_accuracy: 0.9694\nEpoch 7/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1774 - accuracy: 0.9546 - val_loss: 0.1346 - val_accuracy: 0.9748\nEpoch 8/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1533 - accuracy: 0.9608 - val_loss: 0.1181 - val_accuracy: 0.9753\nEpoch 9/20\n180/180 [==============================] - 4s 22ms/step - loss: 0.1378 - accuracy: 0.9613 - val_loss: 0.1104 - val_accuracy: 0.9764\nEpoch 10/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1249 - accuracy: 0.9660 - val_loss: 0.0951 - val_accuracy: 0.9786\nEpoch 11/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1120 - accuracy: 0.9706 - val_loss: 0.0794 - val_accuracy: 0.9840\nEpoch 12/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1052 - accuracy: 0.9716 - val_loss: 0.0760 - val_accuracy: 0.9849\nEpoch 13/20\n180/180 [==============================] - 4s 22ms/step - loss: 0.0945 - accuracy: 0.9749 - val_loss: 0.0691 - val_accuracy: 0.9831\nEpoch 14/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.0877 - accuracy: 0.9757 - val_loss: 0.0590 - val_accuracy: 0.9874\nEpoch 15/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.0841 - accuracy: 0.9769 - val_loss: 0.0542 - val_accuracy: 0.9900\nEpoch 16/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.0786 - accuracy: 0.9791 - val_loss: 0.0491 - val_accuracy: 0.9902\nEpoch 17/20\n180/180 [==============================] - 4s 19ms/step - loss: 0.0722 - accuracy: 0.9794 - val_loss: 0.0436 - val_accuracy: 0.9922\nEpoch 18/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0697 - accuracy: 0.9815 - val_loss: 0.0440 - val_accuracy: 0.9911\nEpoch 19/20\n180/180 [==============================] - 3s 14ms/step - loss: 0.0656 - accuracy: 0.9825 - val_loss: 0.0394 - val_accuracy: 0.9916\nEpoch 20/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.0608 - accuracy: 0.9832 - val_loss: 0.0418 - val_accuracy: 0.9907\n\n\nAgain, let us look at our validation accuracy by calling validation_plot.\n\nvalidation_plot(history)\n\n\n\n\n\n\n\n\nAs we can see, our validation accuracy is very good! With an accuracy of 99 percent, the classification of fake news is nearly perfect!"
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#model-evaluation-on-testing-data",
    "href": "posts/Fakenews/Fakenews.html#model-evaluation-on-testing-data",
    "title": "Fake News?",
    "section": "4. Model Evaluation on Testing Data",
    "text": "4. Model Evaluation on Testing Data\nNow we test our best model on unseen data.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest = make_dataset(test_df)\n\n\nmodel3.evaluate(test)\n\n225/225 [==============================] - 3s 15ms/step - loss: 0.0510 - accuracy: 0.9858\n\n\n[0.051006849855184555, 0.9857900142669678]\n\n\nOur final accuracy is 98.58 percent which is very very good!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Fake News?\n\n\n\n\n\n\nKeras\n\n\nTensorFlow\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nDogs and Cats Classification\n\n\n\n\n\n\nKeras\n\n\nTensorFlow\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nFlask Tutorial\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nSQLITE\n\n\nHTML\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping ‘The Boys’ with Scrapy\n\n\n\n\n\n\nWebscraping\n\n\nScrapy\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Geographic, Boxplot, and Lineplot Visualizations to illustrate Global Warming\n\n\n\n\n\n\nSQLITE\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Tutorial HW0\n\n\n\n\n\n\nweek 0\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bruins/HW0- Blog Tutorial.html",
    "href": "posts/bruins/HW0- Blog Tutorial.html",
    "title": "Blog Tutorial HW0",
    "section": "",
    "text": "First read in the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nNow let’s look at the first few rows of the dataset.\n\npenguins.head()\n\nIt appears that there are a few numeric columns in our dataframe which would be easy to analyze. Let’s specifically look at two and plot them on a scatter plot.\n\n\nLet’s import the seaborn package and create a scatterplot\nFor this example, I chose ‘Culmen Length’ and ’Flipper Length). I imported the seaborn package as it has great functions for visualization. I chose to create a scatterplot because it is the best way to analyze numeric data against numeric data.\n\nimport seaborn as sns\n\nsns.scatterplot(\n    data = penguins,\n    x = \"Culmen Length (mm)\", \n    y = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nIt appears that there is a positive relationship between culmen length and flipper length.\n\n\nNow go one step further!\nLet’s go even further through analyzing if the type of species affects this relationship. To do this, we add the hue parameter. I also used the set_title function to make our plot look even better!\n\nsns.scatterplot(\n    data = penguins,\n    x = \"Culmen Length (mm)\", \n    y = \"Flipper Length (mm)\", \n    hue = \"Species\").set_title(\"Flipper Length vs Culmen Length in mm vs Species\")\n\nText(0.5, 1.0, 'Flipper Length vs Culmen Length in mm vs Species')\n\n\n\n\n\n\n\n\n\nAs we can see, species type does seem to affect the culmen length and flipper length values. For instance, there appears to be distinct groupings based upon species type, as the Gentoo Penguin appears to have the longest dimensions with the adelie penguins and chinstrap penguins having similar flipper lengths but differing culmen lengths.\n\n\nWe’re done!"
  },
  {
    "objectID": "posts/flask/Flask Tutorial.html",
    "href": "posts/flask/Flask Tutorial.html",
    "title": "Flask Tutorial",
    "section": "",
    "text": "Overview\nToday we are going to build a simple web application with Flask. The app we construct will let us doing the following: 1. Allow the user to submit messages 2. Allow the user to view a sample of submitted messages\n\n\n1. Enable Submissions\nTo start let us create a GitHub repository with our files. Here is the link to the one I created in this tutorial: https://github.com/TylerNguyen25/Flask-Webapp. Now, we create a submit html template so that we can do the following:\n\nA text box for submitting a message\nA text box for submitting the name of the use\nA “submit” button\n\nLet us start with a template called base such that we can extend our submit html template off it.\n\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}Test{% endblock %} - PIC16B Website&lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;PIC16B Example!&lt;/h1&gt;\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;li&gt;&lt;a href=\"{{ url_for('main') }}\"&gt;Submit Message&lt;/a&gt;&lt;/li&gt; #link where the main() function directs\n  &lt;li&gt;&lt;a href=\"{{ url_for('ask')}}\"&gt;View Message&lt;/a&gt;&lt;/li&gt; #link where the ask() function directs\n  &lt;li&gt;&lt;a href=\"{{ url_for('deleted')}}\"&gt;Delete Messages&lt;/a&gt;&lt;/li&gt; #link where the deleted() function directs\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n&lt;!-- Will be used for submit.html, view.html, and delete.html --&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nNow let us create our submit html!\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h2&gt;{% block title %}Submit Messages{% endblock %}&lt;/h2&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;form method=\"post\"&gt;\n      Your message:&lt;br&gt;\n      &lt;input name=\"message\" id=\"message\"&gt; &lt;br&gt; #creates textbox to input our message\n\n      Your name or handle:&lt;br&gt;\n      &lt;input name=\"name\" id=\"name\"&gt;&lt;br&gt; #creates textbok to input our name handle\n\n      &lt;input type=\"submit\" value=\"Submit form\"&gt;\n  &lt;/form&gt;\n  {% if note %}\n  &lt;b class = green&gt;{{note}}&lt;/b&gt;\n  {% endif %}\n{% endblock %}\n\nWith our templates, this is what our page looks like.\n\n\n\nsubmit.jpg\n\n\nNext we create a new .py file named app.py which will store our three Python functions for database management: get_message_db(), insert_message(request), and delete_messages().\nThis is what our first function get_message_db() looks like:\n\nfrom flask import Flask, render_template, request, redirect, url_for, abort, g\nimport sqlite3\n\napp = Flask(__name__)\n\ndef get_message_db():\n    try: #return database if it already exists\n        return g.message_db\n    except: #if it does not exist, create one\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (message TEXT, handle TEXT)' \n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\n\nThe function above returns our database if it already exists, and if it doesn’t we create one through the SQL command cmd.\nThe second funtion we write is insert_message(request), which will handle inserting a user’s message into the database.\n\ndef insert_message(request):\n    db = get_message_db()\n    conn = db.cursor()\n\n    msg = request.form['message'] #store the input message from submit.html\n    handle = request.form['name'] #store the input nameh handle from submit.html\n    cmd = f\"INSERT into messages VALUES ('{msg}','{handle}')\" #insert these values into the database \n    conn.execute(cmd)\n    db.commit()\n    db.close()\n\nThis function inserts the message and handle to the database which the user submitted on the submit.html page.\nAnd the final function we create for database management is delete_messages.\n\ndef delete_messages():\n    db = get_message_db()\n    conn = db.cursor()\n    cmd = 'DELETE FROM messages' #delete all rows in database \n    conn.execute(cmd)\n    db.commit()\n    db.close()\n\nThis function deletes all messages from the database so that the user can verify that their message is in fact displayed in the database.\nIn addition to these functions, we write a function called main() to render_template() the submit.html template. When the instance is a GET method, we simply render submit.html. On the other hand, when the instance is POST, we call insert_message(request) to record the message to the database and then finally render submit.html. We also pass a message letting the user know that their message was received.\n\n@app.route('/', methods = ['POST', 'GET'])\ndef main():\n    if request.method == 'GET': #if the request is GET\n        return render_template('submit.html') #render the submit.html file without doing anything\n    else: #if the request is POST \n        msg = \"Message received\"\n        insert_message(request)\n        return render_template('submit.html', note = msg ) #render the submit.html file while submitting requests.\n\n\n\n2. View Random Submissions\nAs of now, we have the ability to submit and record a message into a database. This section illustrates to users how to view these messages.\nTo do so, we write a function random_messages(n) which returns a collection of n random messages from our database.\n\ndef random_messages(n):\n    db = get_message_db()\n    conn = db.cursor()\n    cmd = f'SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}' #select n random messages from our database \n    messages = conn.execute(cmd).fetchall()\n    db.commit()\n    db.close()\n    return messages \n\nIn order to view these random message, we must create a ask.html file.\n\n{% extends \"base.html\" %}\n\n{% block header %}\n&lt;h2&gt;{% block title %}View Messages{% endblock %}&lt;/h2&gt;\n{% endblock %}\n\n{% block content %}\n&lt;ul&gt;\n    {% for message in messages %} #for each message out of our n random messages\n        &lt;p&gt;{{message[0]}}&lt;/p&gt; #display the message \n        &lt;p&gt;- &lt;em&gt;{{message[1]}}&lt;/em&gt;&lt;/p&gt; #display the name handle \n        &lt;br&gt;\n    {% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\nIn this html file, messages is a list of tuples. Hence in order to access the message and user handle, we must construct a for loop and then index each indiviual message tuple. Here, message[0] contains the message and message[1] contains the user handle.\nFinally, we create ask to render ask.html.\n\n@app.route('/view')\ndef ask():\n    messages = random_messages(5)\n    return render_template('ask.html', messages = messages) #render ask.html with 5 random messages \n\nIn ask() we called random_messages(5) to generate five random messages from the database and then pass it into the render_template as an argument. This is the list of tuples which the ask.html file iterates over.\nThe last function in our file is the deleted() function. This function calls delete_messages() to clean all messages in our databse and returns a message to the user after doing so.\n\n@app.route('/delete')\ndef deleted():\n    delete_messages()\n    msg = \"Database has been cleaned\"\n    return render_template('delete.html', note = msg)\n\n\n\n3. Custom Display\nFinally, we create a style.css file in order to customize the way our application appears to the user. In this csss file, we change the font, font-size, background-colors, and alignment of our displays.\n\nhtml{\n    font-family: Garamond, serif;\n}\n\nh1{\n    font-size: 32px;\n    text-align:center;\n    background-color:#C39E6D;\n}\n\nli{\n    background-color: #55565A;\n    font-size: 22px;\n    text-align:center;\n    list-style-type: none;\n}\n\nli a {\n    color: white;\n}\n\na:visited {\n    background-color: cyan;\n  }\n  \na:hover {\n    background-color: lightgreen;\n  }\n\np{\n    font-size: 22px;\n}\n\n.submit-link:hover,\n.view-link:hover {\n  background-color: #0056b3;\n}\n\n.green {\n    color:white;\n    background-color: green;\n}\n\n.red{\n    color:white;\n    background-color: red;\n}\n\n\n\n4. Demo\nFirst, we run the following command in the directory which holds our file\n\nexport FLASK_ENV=development\nflask run\n\nNow, suppose a user submits the message below.\n\n\n\ndemo1.JPG\n\n\nGoing to the view page by clicking on ‘View Message’, we would get the following output.\n\n\n\nDemo2.JPG\n\n\nSuppose we want to delete all messages from the database. Then when we click on ‘Delete Messages’ we get:\n\n\n\ndemo3.JPG\n\n\nAs we can see, our demo shows that our app runs just how we want it to!"
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html",
    "href": "posts/TMDB_scraper/TMDB.html",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "",
    "text": "What movie or TV shows share actors with your favorite movie or show?\nWe will use webscraping to solve the above question in this blog post.\nThis post has two parts. In the first, we go over webscraping for finding shared actors on TMDB. In the second, we use the webscraper’s results to make a visualization."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#a.-pick-a-movie-or-tv-show",
    "href": "posts/TMDB_scraper/TMDB.html#a.-pick-a-movie-or-tv-show",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "a. Pick a Movie or TV Show",
    "text": "a. Pick a Movie or TV Show\nFirst, we pick a tv show. I used my favorite show The Boys. The link to its TMDB page is here: https://www.themoviedb.org/tv/76479-the-boys"
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#b.-project-initialization",
    "href": "posts/TMDB_scraper/TMDB.html#b.-project-initialization",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "b. Project Initialization",
    "text": "b. Project Initialization\nWe will now create a GitHub repository which will contain our Scrapy files. After, we open a terminal in the location we wish our files to be and type:\nscrapy startproject TMDB_scraper\nscd TMDB_scraper"
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#c.-alter-settings",
    "href": "posts/TMDB_scraper/TMDB.html#c.-alter-settings",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "c. Alter Settings",
    "text": "c. Alter Settings\nNow lets alter the settings.py file. We need to modify the User_Agent variable to equal Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 such that we will not get 403 errors while scraping.\nWe also add the line CLOSESPIDER_PAGECOUNT = 20 to prevent our webscraper from scraping too many pages."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#a.-parseself-response",
    "href": "posts/TMDB_scraper/TMDB.html#a.-parseself-response",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "a. parse(self, response)",
    "text": "a. parse(self, response)\n\ndef parse(self, response):\n        \"\"\"\n        Sends webscraper to the full cast and crew page \n        \"\"\"\n        next_page = response.css(\"a[href*=cast]::attr(href)\").get() #css command to redirect to the full cast page url\n        yield response.follow(next_page, callback = self.parse_full_credits) #redirects to the new url page and executes parse_full_credits\n\nThe response variable inside parse is the page we are inspecting. The variable next_page corresponds to the next page we wish to scrape. The value inside response.css was the css command used to redirect to the full cast url page. Finally we used yield response.follow in order to go to this next url, and the callback parameter we wrote is the function which we next execute."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#b.-parse_full_creditsself-response",
    "href": "posts/TMDB_scraper/TMDB.html#b.-parse_full_creditsself-response",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "b. parse_full_credits(self, response)",
    "text": "b. parse_full_credits(self, response)\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Sends webscraper to the individual actor's page\n        \"\"\"\n        actor_list = response.css('ol.people.credits:not(.crew) div.info  a[href*= \"person/\"]::attr(href)').getall()[0:20]\n        for actor in actor_list: #for every actor in the actor list url, execute the callback command parse_actor_page \n            yield response.follow(actor, callback = self.parse_actor_page)\n\nThe function which parse yields does the same thing parse does except it inspects the new response variable passed which is the full cast page. The variable actor_list corresponds to the next page we wish to next scrape and I choose to include only the first 20 actors in this list. The css command we used was to redirect to each individual actor’s page. Finally, we also used response.follow again in order to go to the individual actor’s url page, and the callback function wish we need to execute next is self.parse_actor_page."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#c.-parse_actor_pageself-response",
    "href": "posts/TMDB_scraper/TMDB.html#c.-parse_actor_pageself-response",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "c. parse_actor_page(self, response)",
    "text": "c. parse_actor_page(self, response)\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Returns a dictionary of every actor and each movie they worked on\n        \"\"\"\n        actor_name = response.css(\"h2.title a::text\").get() #css command which displays actor name \n        movie = response.xpath(\"//div//h3[text() = 'Acting']/following-sibling::table[1]//bdi/text()\").getall() \n        #xpath command which displays each movie/TV show actor worked on\n        for movie_or_TV_name in movie:\n            yield { #create a dictionary with actor name and movie/TV show \n                \"actor\": actor_name,\n                \"movie_or_TV_name\" : movie_or_TV_name\n            }\n\nThis function does the same thing the above two functions except it inspects the new response variable passed which is the individual actors page. We extract the actor names and every movie/TV show this actor worked on and store them in the actor and movies variables. For the movie path, we have to only look under the Acting table and to do this we use the following-sibling pseudo-element to do so. Finally, we create a for loop to go over each work per actor, and uses these variables to create a dictionary of {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#insights-and-recommendations",
    "href": "posts/TMDB_scraper/TMDB.html#insights-and-recommendations",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "3. Insights and Recommendations",
    "text": "3. Insights and Recommendations\nWe now run the following command in our terminal to create a csv file named results2.csv containing our data:\nscrapy crawl tmdb_spider -o results2.csv -a subdir=76479-the-boys\nLet us now anaylze the results.\n\nimport pandas as pd \ndf = pd.read_csv(\"results2.csv\")\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nKarl Urban\nARK: The Animated Series\n\n\n1\nKarl Urban\nMortal Kombat 2\n\n\n2\nKarl Urban\nI, Object\n\n\n3\nKarl Urban\nGen V\n\n\n4\nKarl Urban\nThe Sea Beast\n\n\n...\n...\n...\n\n\n813\nJessie T. Usher\nLevel Up\n\n\n814\nJessie T. Usher\nThe Mentalist\n\n\n815\nJessie T. Usher\nHannah Montana\n\n\n816\nJessie T. Usher\nCriminal Minds\n\n\n817\nJessie T. Usher\nWithout a Trace\n\n\n\n\n818 rows × 2 columns\n\n\n\n\npopular = df.groupby(\"movie_or_TV_name\").count().sort_values(ascending = False, by = \"actor\")[0:24]\npopular.columns = [\"Actor Count\"]\npopular\n\n\n\n\n\n\n\n\nActor Count\n\n\nmovie_or_TV_name\n\n\n\n\n\nThe Boys\n21\n\n\nPrime Rewind: Inside The Boys\n10\n\n\nGen V\n8\n\n\nThe Boys Presents: Diabolical\n6\n\n\nLaw & Order\n4\n\n\nLaw & Order: Special Victims Unit\n4\n\n\nMano\n3\n\n\nThe Kelly Clarkson Show\n3\n\n\nThe Show\n3\n\n\nDeception\n3\n\n\nLate Night with Seth Meyers\n3\n\n\nTimeless\n3\n\n\nThe Equalizer\n3\n\n\nGen V - Prime Premiere\n3\n\n\nUndrafted\n2\n\n\nPerson of Interest\n2\n\n\nThe Ellen DeGeneres Show\n2\n\n\nParish\n2\n\n\nBones\n2\n\n\nDylan & Zoey\n2\n\n\nAunty Donna's Big Ol House of Fun\n2\n\n\nLevel Up\n2\n\n\nHalf & Half\n2\n\n\nSoul Food\n2\n\n\n\n\n\n\n\nAs we can see from the subset of the data above, some movies and TV shows are more popular than others. Obviously The Boys will be the most popular one because it is where we got our actor list from. So we create a pie graph which will show the next popular movies/TV shows.\n\nfrom plotly import express as px \nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nfig = px.pie(popular, values='Actor Count', names=popular.index, title='Top Recommendations')\nfig.update_traces(textposition='inside', textinfo='label+text')\nfig.show()\n\n\n\n\nAs we can see the pie graph clearly gives us the most popular movies. Obviously The Boys is first and we can see that the next three most popular are spin offs of the TV Show, Prime Rewind: Inside the Boys, Gen V, and The Boys Presents: Diabolical. The popular ones not related to the boys are Law & Order, Law  Order: Special Victims Unit, and Mano*.\nNice, now we are done!"
  }
]