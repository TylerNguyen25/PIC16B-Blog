[
  {
    "objectID": "posts/Visualization plotly/climate.html",
    "href": "posts/Visualization plotly/climate.html",
    "title": "Using Geographic, Boxplot, and Lineplot Visualizations to illustrate Global Warming",
    "section": "",
    "text": "import sqlite3\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\nfrom climate_database import (query_climate_database, temperature_coefficient_plot,\nquery_equator, temperature_box_plot, temperature_lineplot_equator)\n\n\n1. Database Creation\nFirst, create three separate tables: temperatures, countries, and stations table. To do so, I wrote a function named prepare_df to prepare the temperature dataset such that the data is easier to understand.\n\nThe temperatures table will contain station ID, year of measurement, month of measurement, and average temperature.\nThe countries table will contain country names and their corresponding country codes.\nThe stations table will contain station ID, latitude, longitude, elevation, and the name of the station.\n\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    df[\"NewID\"] = df[\"ID\"].str[0:2]\n    return(df)\n\n\nwith sqlite3.connect(\"HW1.db\") as conn: # create a database in current directory called temps.db\n    df_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(df_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    url = \"station-metadata.csv\"\n    stations = pd.read_csv(url)\n    stations[\"NewID\"] = stations[\"ID\"].str[0:2]\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries = pd.read_csv(\"countries.csv\")\n    countries = countries.rename(columns={\"FIPS 10-4\": \"NewID\"})\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\nTemperatures dataframe\n\ndf.head() \n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\nNewID\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\nUS\n\n\n1\nUSW00014924\n2016\n2\n-8.40\nUS\n\n\n2\nUSW00014924\n2016\n3\n-0.20\nUS\n\n\n3\nUSW00014924\n2016\n4\n3.21\nUS\n\n\n4\nUSW00014924\n2016\n5\n13.85\nUS\n\n\n\n\n\n\n\nStations dataframe\n\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\nNewID\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\nAC\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\nAE\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\nAE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\nAE\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\nAE\n\n\n\n\n\n\n\nCountries dataframe\n\ncountries.head()\n\n\n\n\n\n\n\n\nNewID\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\ndb_file = \"HW1.db\" #database we created \n\n\n\n2. Query Function\nBelow I created a query function query_climate_database which returns a dataframe based off of filters. The parameters specify the country, the year we start from, the year we end at, and the month we wish to analyze. The dataframe displays station name, latitude, longitude, and average temperature of the station during the month and year.\n\ninspect.getsource(query_climate_database)\n\n'def query_climate_database(db_file, country, year_begin, year_end, month):\\n    \"\"\"\\n    Query function to filter climate data \\n    \\n    Args:\\n    country: country of interest \\n    year_begin: start year desired \\n    year_end: end year desired \\n    month: a specific month \\n    \\n    Return:\\n    Dataframe of filtered data \\n    \"\"\"\\n    with sqlite3.connect(db_file) as conn:\\n        #select Name, Latitiude, and Longitude from stations table\\n        #select country from countries table,\\n        #select Year, Month, and Temperature from temperatures table\\n        cmd = f\"\"\"\\n        SELECT S.Name, S.LATITUDE, S.LONGITUDE, C.Name \"Country\", T.Year, T.Month, T.Temp\\n        FROM stations S \\n        LEFT JOIN countries C ON C.NewID = S.NewID\\n        RIGHT JOIN temperatures T ON S.ID = T.ID \\n        WHERE T.Year &lt;= {year_end} AND T.Year &gt;= {year_begin} AND T.Month = {month} AND C.Name = \"{country}\"\\n        \"\"\"\\n        #Combine tables on ID number and subset where the year is between year_end and year_begin, \\n        #subset where the month and country are provided parameters\\n        df = pd.read_sql_query(cmd, conn)\\n        return(df)\\n'\n\n\nBelow is an example with parameters country India, year_begin 1980, year_end 2020, and month 1\n\ndf = query_climate_database(db_file = \"HW1.db\", \n                            country = \"India\",\n                            year_begin = 1980, \n                            year_end = 2020, \n                            month = 1)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\n3. Geographic Mapbox Scatterplot\nWe use query_climate_database to create another function temperature_coefficient plot which answers how the average yearly change in temperature varies within a country. The function accepts the same parameters, as well as min_obs which will filters stations which do not have the specified number of data points. We also add **kwargs to customize our mapbox scatterplot we made using px.scatter_mapbox().\nInside our function, I wrote another function coef which conducts linear regression in order to compute how each station changes in average temperature over time.\nBelow we create a sample plot with the same parameters we used in the query function along with new **kwargs parameters for the scatter_mapbox.\n\ninspect.getsource(temperature_coefficient_plot)\n\n'def temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\\n    \"\"\"\\n    Construct a geographic mapbox scatterplot of yearly temperature increase\\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year \\n    month: a specific month \\n    min_obs: minimum required observations of station\\n    **kwargs: additional keyword arguments\\n    \\n    Return:\\n    Scatter_mapbox plot\\n    \"\"\"\\n    import calendar \\n    #get the average increase through taking the coefficient of linear regression \\n    def coef(data_group):\\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\\n        LR = LinearRegression()\\n        LR.fit(x, y)\\n        return LR.coef_[0]\\n    #call the query function we created above \\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\\n    #get the amount of observations per station\\n    test = df.groupby([\"NAME\"]).size()\\n    #index our dataframe such that the ones with more than min obs are kept\\n    names = test[test &gt;= min_obs].index\\n    df = df[df.NAME.isin(names)]\\n    #get the coefficients for each station during said month\\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef).round(4)\\n    plot_df = pd.DataFrame(coefs)\\n    #merge coefficients with dataframe above\\n    merged_df = pd.merge(df, plot_df, on=\\'NAME\\')\\n    merged_df = merged_df.rename(columns={0: \"Estimated Yearly Increase (Celcius)\"})\\n    #create a scatter_mapbox given coordinate points and hover over station name with the color equal to the coefficient increase\\n    fig = px.scatter_mapbox(merged_df, \\n                            lat = \"LATITUDE\",\\n                            lon = \"LONGITUDE\", \\n                            hover_name=\"NAME\", \\n                            color_continuous_midpoint = 0,\\n                            color = \"Estimated Yearly Increase (Celcius)\",\\n                            title = (f\"\"\"Estimates of yearly increase in {calendar.month_name[month]} \\n                             &lt;br&gt;temperature for stations in {country}, from {year_begin} to {year_end}\"\"\"),\\n                            **kwargs)\\n    return(fig)\\n'\n\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(db_file, \"India\", 1980, 2020, 1, 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\nfig.show()\n\n\n\n\nNow let us try this with a new example. Here I used the parameters country = Portugal, year_begin = 1980, year_end = 2020, and month = 2.\n\nfig = temperature_coefficient_plot(db_file, \"Portugal\", 1980, 2020, 2, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\n4. Additional Query Function and Plotly Boxplot and Lineplot\nI decided to write a query function which filters out only the equator countries. The reason why I did this was because I believe that it is valuable to see how the temperature changes each year from these equator countries because it may illustrate that even at the hottest parts of our world, the temperature increases, potentially infering global warming.\nHere I created a dataframe of just equator countries from 2010 - 2020.\n\ninspect.getsource(query_equator)\n\n'def query_equator(db_file, year_begin, year_end):\\n    \"\"\"\\n    Query function of only the Equator countries\\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year\\n    \\n    Return:\\n    Dataframe with desired filters \\n    \"\"\"\\n    with sqlite3.connect(db_file) as conn:\\n        #subset between year_begin and year_end and only the countries which lie on the equator \\n        cmd = \\\\\\n        f\"\"\"\\n        SELECT S.Name, S.LATITUDE, S.LONGITUDE, C.Name \"Country\", T.Year, T.Month, T.Temp\\n        FROM stations S \\n        LEFT JOIN countries C ON C.NewID = S.NewID\\n        RIGHT JOIN temperatures T ON S.ID = T.ID \\n        WHERE T.Year &lt;= {year_end} AND T.Year &gt;= {year_begin} AND S.LATITUDE &gt;= -5 AND S.LATITUDE &lt;= 5\\n        AND (C.Name = \"Congo, Democratic Republic of the\" OR C.Name = \"Gabon\" \\n        OR C.Name = \"Somalia\" \\n        OR C.Name = \"Congo, Republic of the\" OR C.Name = \"Uganda\" \\n        OR C.Name = \"Maldives\" OR C.Name = \"Indonesia\" OR C.Name = \"Kiribati\" \\n        OR C.Name = \"Sao Tome and Principe\" OR C.Name = \"Kenya\" \\n        OR C.Name = \"Ecuador\" OR C.Name = \"Colombia\" OR C.Name = \"Brazil\")\\n        \"\"\" \\n        df = pd.read_sql_query(cmd, conn)\\n        return(df)\\n'\n\n\n\ndf = query_equator(db_file, 2010, 2020)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n1\n27.69\n\n\n1\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n2\n28.11\n\n\n2\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n3\n28.85\n\n\n3\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n4\n27.79\n\n\n4\nPORTO_DE_MOZ\n-1.733\n-52.233\nBrazil\n2010\n5\n28.27\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16227\nMASINDI\n1.683\n31.717\nUganda\n2011\n8\n21.98\n\n\n16228\nMASINDI\n1.683\n31.717\nUganda\n2011\n9\n22.09\n\n\n16229\nMASINDI\n1.683\n31.717\nUganda\n2011\n10\n23.01\n\n\n16230\nMASINDI\n1.683\n31.717\nUganda\n2011\n11\n23.29\n\n\n16231\nMASINDI\n1.683\n31.717\nUganda\n2011\n12\n24.50\n\n\n\n\n16232 rows × 7 columns\n\n\n\nThe temperature_box_plot function below addresses how the average temperature of countries on the equator change year to year. I made sure that the function uses the same station for every year such that the data is not unevenly weighted. The plot which this function produces is important because we can see if our temperature is rising or lowering as time goes on. And as we can see from the figure below, there is a slight gradual increase in the boxplot medians of Brazil, providing visual evidence that temperatures are increasing for equator countries.\n\ninspect.getsource(temperature_box_plot)\n\n'def temperature_box_plot(db_file, country, year_begin, year_end, **kwargs):\\n    \"\"\"\\n    Construct boxplot of yearly average temperatures of specified equator country \\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year \\n    \\n    Return:\\n    px.box plot of yearly average temperatures of specified equator country\\n    \"\"\"\\n    #call SQL query for just equator countries\\n    df = query_equator(db_file, year_begin, year_end)\\n    #subset only on country parameter given\\n    df = df[df.Country == country]\\n    #calculate mean temperature for each station\\'s year\\n    new = df.groupby([\"NAME\", \"Year\"])[\"Temp\"].aggregate(np.mean)\\n    #get the amount of observations per Name and Year \\n    test = df.groupby([\"NAME\", \"Year\"]).size()\\n    test = test.reset_index()\\n    #make sure that the name and year have at least year_end - year_begin + 1 observations such that our boxplot looks full\\n    names = test.groupby([\"NAME\"]).size() &gt;= year_end - year_begin + 1 \\n    names = names[names].index\\n    plot_df = pd.DataFrame(new)\\n    plot_df = plot_df.reset_index()\\n    plot_df = plot_df[plot_df.NAME.isin(names)]\\n    #create boxplot off of created dataframe\\n    fig = px.box(df,\\n                 x = \"Country\",\\n                 y = \"Temp\",\\n                 color = \"Year\",\\n                 title = f\"Boxplots of {country}\\'s Average Temperature by Year from {year_begin} to {year_end}\",\\n                 category_orders={\"Year\": [ix for ix in range(year_begin, year_end)]},\\n                 **kwargs\\n                )\\n    return fig\\n'\n\n\n\nfig = temperature_box_plot(db_file, \"Brazil\", 1980, 2020)\nfig.show()\n\n\n\n\nI constructed the temperature_lineplot_equator function to display a lineplot of how the temperature changes over time in each month, and each colored line on the lineplot represents a different station. I felt this was important because like the boxplot it was another way to visualize how there is an increase in temperature in equator countries. For instance, as we can see below the Average Temperature of Station in Indonesia from 1985 to 2020 increased for all 4 stations plotted.\n\ninspect.getsource(temperature_lineplot_equator)\n\n'def temperature_lineplot_equator(db_file, year_begin, year_end, country, **kwargs):\\n    \"\"\"\\n    Construct lineplot of yearly average temperatures of specified equator country in month\\n    \\n    Args:\\n    country: desired country to display \\n    year_begin: desired start year \\n    year_end: desired end year \\n    month: specific month of year\\n    **kwargs: keyword arguments for lineplot\\n    \\n    Return:\\n    px.line plot of yearly average temperatures of specified equator country\\n    \"\"\"\\n    import calendar \\n    #call the SQL query for just equator countries\\n    df = query_equator(db_file, year_begin, year_end)\\n    #create date variable given year and month\\n    df[\"Date\"] = pd.to_datetime(df[\"Year\"].astype(str) + \"-\" + df[\"Month\"].astype(str))\\n    #subset on country parameter\\n    df = df[df.Country == country]\\n    #calculate mean for each station\\'s year and month\\n    new = df.groupby([\"NAME\", \"Year\", \"Month\"])[\"Temp\"].aggregate(np.mean)\\n    #get the number of observations per station year and month\\n    test = df.groupby([\"NAME\", \"Year\", \"Month\"]).size()\\n    test = test.reset_index()\\n    #make sure minimum observations are year_end - year_begin * 10 -5 such that our line plots are full\\n    names = test.groupby([\"NAME\"]).size() &gt;= (year_end - year_begin)*10 - 5  \\n    names = names[names].index\\n    plot_df = pd.DataFrame(new)\\n    plot_df = plot_df.reset_index()\\n    plot_df = plot_df[plot_df.NAME.isin(names)]\\n    #create lineplot based upon year and temperature and month face meaning we should get 12 different line plots\\n    fig = px.line(data_frame = plot_df,\\n                  x = \"Year\",\\n                  y = \"Temp\",\\n                  color = \"NAME\",\\n                  facet_row=\"Month\",\\n                  title = (f\"\"\"Average Temperature of Station in {country} &lt;br&gt;\\n                           by Year from {year_begin} to {year_end}\"\"\"),\\n                  height = 5000,\\n                  category_orders={\"Month\": [1,2,3,4,5,6,7,8,9,10,11,12]}\\n                  )\\n    return(fig)\\n'\n\n\n\nplot = temperature_lineplot_equator(db_file, 1985, 2020, \"Indonesia\")\nplot.show()\n\n\n\n\n\n\n5. Concluding Remarks\nAs we can see from the query functions and graphics constructed, there is overwhelming evidence for global warming. The mapbox plot, boxplot, and line plots all show that there is a majority increase in temperature, and this conclusion was able to be realized because the visualizations helped to easily explain the data."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html",
    "href": "posts/image classification/ClassificationDogCat (1).html",
    "title": "Dogs and Cats Classification",
    "section": "",
    "text": "In this post, we train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models and observe which ones the best!"
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#visualize-our-data",
    "href": "posts/image classification/ClassificationDogCat (1).html#visualize-our-data",
    "title": "Dogs and Cats Classification",
    "section": "Visualize our data",
    "text": "Visualize our data\nNow, let us create a function called cats_dogs_plot to visualize our dataset. We use dataset.take(1) to access the first batch of our data, which in this case is 64 images, and we use matplotlib to plot 3 images of cats in the first row and 3 images of dogs in the second row.\n\ndef cats_dogs_plot(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0): #if our label == 0 or equals a cat\n                if cats &lt;= 3: #once we get at least three cats, stop plotting\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(\"cats\")\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1): #if our label == 1 or equals a dog\n                if dogs &lt;= 6: #once we get at leaset three dogs, stop plotting\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(\"dogs\")\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\ncats_dogs_plot(train_ds)\n\n\n\n\n\n\n\n\nAs you can see, our function works as intended!"
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#analyzing-our-dataset",
    "href": "posts/image classification/ClassificationDogCat (1).html#analyzing-our-dataset",
    "title": "Dogs and Cats Classification",
    "section": "Analyzing our dataset",
    "text": "Analyzing our dataset\nNow, let us see how many dogs and cats images there are total in our training set. To do so, we create an iterator called labels_iterator and if the label equals 0, we add a tally to our cats variable and if the label equals 1, we add a tally to our dogs variable.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncats = dogs = 0\nfor element in labels_iterator: #for each label in our labels_iterator\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\n\ncats, dogs\n\n(4637, 4668)\n\n\n\ndogs/(cats+dogs)\n\n0.5016657710908113\n\n\nIt appears that our dataset is nearly balanced. There are 4637 cats and 4668 dogs, meaning that if we were to guess all dogs, we would have a 50.16 percent accuracy rate."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model stabilized around 56 to 60 percent\nCompared with our baseline of 50.16 percent, our model did a bit better, but we can still do much better\nThere is a significant overfitting issue as our training accuracy is around 98 percent while our validation accuracy is nearly thirty percent lower."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments-1",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments-1",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model now appears to be around 75 percent\nThis is significantly better than the first model’s accuracy of 55 percent and our baseline of 50.16 percent.\nThere does not appear to be overfitting as our validation accuracy is better than our training accuracy"
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments-2",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments-2",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model is between around 80 to 82 percent.\nThe accuracy of model3 is much higher than model1. In fact, it is around 30 percent better which is a very signficant difference in accuracy\nThere does not appear to be overfitting as the validation error is just slightly worse than the training error."
  },
  {
    "objectID": "posts/image classification/ClassificationDogCat (1).html#comments-3",
    "href": "posts/image classification/ClassificationDogCat (1).html#comments-3",
    "title": "Dogs and Cats Classification",
    "section": "Comments",
    "text": "Comments\n\nThe accuracy of our model stabilizes around 96 to 97 percent\nThis accuracy is significantly better than model1 and all the other models which we have tested\nNo overfitting issues seem to be present as our validation error actually outperforms our training error by one percent!"
  },
  {
    "objectID": "posts/finalproject/FinalProject (1).html",
    "href": "posts/finalproject/FinalProject (1).html",
    "title": "Trail Recommendations",
    "section": "",
    "text": "Using the internet, it’s very easy to find information about hiking trails throughout the United States, but there’s a problem: it’s a lot easier to browse through lists of trails available online, like on the website trailforks.com, and filtering by location. What are you supposed to do if you don’t know much about an area but still want to go hiking, or what if you want to do certain things on your hike but don’t know where to go? You could do lots of research online yourself, reading articles or sifting through different locations, but this process can be very difficult, especially if you want to go somewhere that you’ve never been.\nOur project addresses finding new places to visit based upon what type of hiking trails the user wants to see (which the user would describe) based upon previously hiked trails. For instance, if the user previously hiked Half Dome in Yosemite, then we give the user other trails and locations in the United States most similar to Half Dome according to reviews on TripAdvisor. This would be mainly for tourists who want to visit parts of the country they have not been to.\nThe below code is for importing images.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/finalproject/FinalProject (1).html#load-data",
    "href": "posts/finalproject/FinalProject (1).html#load-data",
    "title": "Trail Recommendations",
    "section": "Load Data",
    "text": "Load Data\nNow let us load the data we scraped by TripAdvisor as well as a Excel file containing coordinate points of our national parks so that we can create a geograpical plot later\n\ndf = pd.read_csv('https://raw.githubusercontent.com/torwar02/trails/main/trails/national_parks.csv')\n\n\ndf2 = pd.read_excel('https://raw.githubusercontent.com/torwar02/trails/main/trails/coords.xlsx')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nPark\nState(s)\nPark Established\nArea\nVisitors (2018)\n\n\n\n\n0\n44.35\n-68.21\nAcadia\nMaine\nFebruary 26, 1919\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\n-14.25\n-170.68\nAmerican Samoa\nAmerican Samoa\nOctober 31, 1988\n8,256.67 acres (33.4 km2)\n28626\n\n\n2\n38.68\n-109.57\nArches\nUtah\nNovember 12, 1971\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3\n43.75\n-102.50\nBadlands\nSouth Dakota\nNovember 10, 1978\n242,755.94 acres (982.4 km2)\n1008942\n\n\n4\n29.25\n-103.25\nBig Bend\nTexas\nJune 12, 1944\n801,163.21 acres (3,242.2 km2)\n440091\n\n\n\n\n\n\n\nTo merge the two files together, we utilize regex. Get string preceding ‘National Park’ in df such that we can merge with df2 on National Park name\n\nimport re\npattern = r'(.*?)(?:\\s+National Park)?$'\nresult = re.findall(pattern, df['national_park'].iloc[0])\npark = []\nfor row in df['national_park']:\n    test_park = re.findall(pattern, row)\n    park.append(test_park[0])\ndf['park'] = park\nnational_parks = pd.merge(df, df2, left_on='park', right_on='Park')\nnational_parks = national_parks.drop(columns = ['park', 'Park', 'State(s)', 'Park Established'])\nnational_parks.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575"
  },
  {
    "objectID": "posts/finalproject/FinalProject (1).html#word-embedding-and-comment-similarity-score",
    "href": "posts/finalproject/FinalProject (1).html#word-embedding-and-comment-similarity-score",
    "title": "Trail Recommendations",
    "section": "Word Embedding and Comment Similarity Score",
    "text": "Word Embedding and Comment Similarity Score\nFirst let us go over what Word Embedding is. Word embedding in NLP is an important technique that is used for representing words for text analysis in the form of real-valued vectors. In this approach, words and documents are represented in the form of numeric vectors allowing similar words to have similar vector representations. The extracted features are fed into a machine learning model so as to work with text data and preserve the semantic and syntactic information. This information once received in its converted form is used by NLP algorithms that easily digest these learned representations and process textual information.\n\nComment Similarity Function\nNow let us create a function called comment_similarity which takes in our national_parks.csv file we just created via the park_data parameter, a comment_index parameter, and an all_comments parameter which is our word embedding vector representation of all comments in our csv file.\n\nall_docs = [nlp(row) for row in national_parks['comment_text']] #getting vector representation of all comments in our csv file\n\n\ndef comment_similarity(parks_data, comment_index, all_comments):\n    example_comment = parks_data.loc[comment_index, 'comment_text']\n    reference_comment = nlp(example_comment) #vectorize our reference sentence\n    simularity_score = []\n    row_id = []\n    for i in range(len(all_comments)):\n        sim_score = all_comments[i].similarity(reference_comment)\n        simularity_score.append(sim_score)\n        row_id.append(i)\n    simularity_docs = pd.DataFrame(list(zip(row_id, simularity_score)), columns = ['Comment_ID', 'sims'])\n    simularity_docs_sorted = simularity_docs.sort_values(by = 'sims', ascending = False)\n    most_similar_comments = simularity_docs_sorted['Comment_ID'][1:2]\n    new_reviews = national_parks.iloc[most_similar_comments.values]\n    return(new_reviews)\n\nNow let us show what our returned dataframe looks like\n\nshowcase = comment_similarity(national_parks, 0, all_docs)\nshowcase\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1552\nGrand Canyon National Park\nArizona (AZ)\nGrand Canyon South Rim\nCanyons\n5.0\nThe views do not disappoint!\n5.0 of 5 bubbles\nWe were staying with family in Sun City (near ...\n36.06\n-112.14\n1,201,647.03 acres (4,862.9 km2)\n6380495\n\n\n\n\n\n\n\nAs we can see, we return a dataframe of the most similar review to the review with the index 999. To see how similar this similar review is to our inputted review let us output both comments.\nFirst the original comment\n\nexample_comment = national_parks.loc[0, 'comment_text']\nexample_comment\n\n\"I have hiked to the fire tower a few times. Its a great hike, and not too strenuous elevation gains.  If the NO rangers are up there ( in the summer) they used to allow you to go up the tower. We had to turn back on 3/20 because of hard pack solid ice. We had our Katoohla micro spikes on, and solid hiking poles, and knew they simply  wouldn't be enough if the ice was on the steeper sections.  We walked into the trailhead because the access road gate is still closed. After deciding to cross the lot and hike Beech Cliff Loop, which was much more clear of ice, and has excellent views of Echo Lake and the ocean out toward  Southwest Harbor. We returned to BH to hear of the recovery of a young couple from Rutland Massachusetts  who had fallen 100 feet to their death on Dorr Mountain Gorge Trail. The tragedy attributed to ice on the trails. Anyone not experienced with full crampon travel, and ice climbing training should never attempt to hike or climb on solid ice. The danger is severe.. \"\n\n\nNow the similar comment.\n\nshowcase['comment_text'].iloc[0]\n\n'We were staying with family in Sun City (near the Phoenix airport) and drove in our rental vehicle the approximate 3.5 hour drive to the south entrance of the Grand Canyon.  The park entrance was easy to find.  Parking this year was $35/vehicle.  I was skeptical going in, as several friends had this excursion on their \"bucket list\" while others simply raved.  I worried I would be disappointed.  However, the views absolutely spectacular!  We self-guided/toured.  We both experienced some vertigo and were careful to hang on to the railings provided, or sit on available benches as needed. Also bring water.  With the high elevation, it is easier to get winded, and water helps. We did have a hiker in front of us fall a few times from experiencing vertigo,and with assistance from others were able to help him get off the stairs and onto level ground to sit down.  He was embarrassed but grateful.  It could (and does) happen to anyone.  There were some areas that were roped off due to ice and snow and I was amazed how many people stupidly ignored the warnings and bypassed the barriers to get closer to the edge of the Canyon for selfies!   Check the weather in advance and dress appropriately.  The temperature was 30 degrees cooler in the Canyon than in the Phoenix area.  There were many families present and some pushing young ones in strollers.  On Feb 10, it was a chilly, windy, 40 degrees F.   There are lots of signs at various points educating you on the history of rocks, the Colorado river running through the Canyon, etc., and a small museum you can enter about 1.5 hours into the walk.  After our hike, we were exhausted and wind blown, and caught a shuttle back to the parking lot.  Kudos to those who can manage to walk the entire thing.  We didn\\'t see everything the south side had to offer.  In our vehicle, we exited the park from the east side and for some 50+ miles, still saw the Grand Canyon from out the driver\\'s side window. There were several spots along the way to stop and take more photos.  All in all, it was a physically and mentally stimulating journey that I highly recommend.'\n\n\nAs we can see the comments are very similar! They both talk about the dangers of the trail and how they both saw people fall.\n\n\nTotal Trail Simularity\nNow let us create a function called total_similarity which takes in the same parameters as our last function except takes in the trail name instead of comment_index. We do so because we want to get all 10 comments per trail. Our total_similarity function calls comment_similarity to get the most similar comment per each individual comment of the 10 trails. As a result, we get 10 total similar trails returned to us.\n\ndef total_similarity(trail, parks_data, all_comments):\n    trail_subset = parks_data[parks_data['trail'] == trail].index\n    total_df = []\n    for number in trail_subset:\n        total_df.append(comment_similarity(national_parks, number, all_docs))\n    df = pd.concat(total_df)\n    return(df)\n\n\noutput = total_similarity(\"Landscape Arch\", national_parks, all_docs)\noutput\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nAs we can see we get 10 similar trails to our desired trail Landscape Arch\n\n\nPlotly Function\nNow let us construct a geographical plot function called plotting_parks to get the location of these trails on a map. This is so that the user can better visualize where in the United States they may have to travel to. The function also analyzes other metrics from national_parks.csv such as visitors in 2018, type of activity, trail name, and overall TripAdvisor rating. This function calls total_similarity in order to get the dataframe with the most similar reviews!\n\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\ndef plotting_parks(trail, parks_data, all_comments, **kwargs):\n    output = total_similarity(trail, parks_data, all_comments)\n    fig = px.scatter_mapbox(output, lon = \"Longitude\", lat = \"Latitude\", color = \"overall_rating\",\n                        color_continuous_midpoint = 2.5, hover_name = \"national_park\", height = 600,\n                        hover_data = [\"Visitors (2018)\", \"activity\", \"trail\", \"overall_rating\"],\n                        title = \"Recommended National Park Trails\",\n                        size_max=50,\n                        **kwargs,\n                        )\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # produce a color map\nfig = plotting_parks(\"Landscape Arch\", national_parks, all_docs, mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\n\n\nfig.show()\n\n\n\n\nGreat, as we can see, we get a geo plot of the most similar National Park trails in the United States to Landscape Arch!"
  },
  {
    "objectID": "posts/finalproject/FinalProject (1).html#database_info.py",
    "href": "posts/finalproject/FinalProject (1).html#database_info.py",
    "title": "Trail Recommendations",
    "section": "database_info.py",
    "text": "database_info.py\nEverything relevant to managing the datbase is stored in a different python file called database_info.py. Here I can show you the structure of both databases:\n\nMaking the databases\ndef make_db(state):\n    conn = sqlite3.connect(\"trails.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    name VARCHAR(255),\n    coords VARCHAR(255),\n    Distance VARCHAR(255),\n    'Avg time' VARCHAR(255),\n    Climb VARCHAR(255),\n    Descent VARCHAR(255),\n    Activities VARCHAR(255),\n    'Riding Area' VARCHAR(255),\n    'Difficulty Rating' VARCHAR(255),\n    'Dogs Allowed' VARCHAR(255),\n    'Local Popularity' VARCHAR(255),\n    'Altitude start' VARCHAR(255),\n    'Altitude end' VARCHAR(255),\n    Grade VARCHAR(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\n    \ndef make_db_parks(state):\n    conn = sqlite3.connect(\"trails_new.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    Name VARCHAR(255),\n    Location VARCHAR(255),\n    Coords VARCHAR(255),\n    'Trails (view details)' SMALLINT(255),\n    'Total Distance' VARCHAR(255),\n    'State Ranking' VARCHAR(255),\n    'Access Road/Trail' SMALLINT(255),\n    White SMALLINT(255),\n    Green SMALLINT(255),\n    Blue SMALLINT(255),\n    Black SMALLINT(255),\n    'Double Black Diamond' SMALLINT(255),\n    Proline SMALLINT(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\nThese two functions were run in order to actually create the datbaase for the first time. They contain the variables as mentioned previously, mostly in the form of text.\n\n\nAdding information\nIf you recall from the scraping functions, there was a function call that would add information from each park to the SQL database. Here’s the source code for those functions:\ndef get_db():\n    conn = sqlite3.connect(\"trails.db\")\n    return conn\n    \ndef add_trails(df,state):\n    conn = get_db()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\n    \ndef get_db_new():\n    conn = sqlite3.connect(\"trails_new.db\")\n    return conn\n    \ndef add_trails_new(df,state):\n    conn = get_db_new()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\nThe functions get_db and get_db_new (most things relating to scraper_parks are labeled new since we did this second) establish connections to their respective databases. add_trails and add_trails_new, therefore, are actually responsible for adding entries to each database. Note that they take a df as one input (which contains the scraped info) and a state name, which sends the information to the correct table.\n\n\nMiscellaneous Tables\nThere are several dictionaries and lists that we generated in order to make the functions easier to run:\nstates = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"idaho-3166\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"new-hampshire\", \"new-jersey\", \"new-mexico\", \"new-york\", \"north-carolina\", \"north-dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"rhode-island\", \"south-carolina\", \"south-dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"west-virginia\", \"Wisconsin\", \"Wyoming\"]\n\nstate_name_code_name_dict = {\n    'Alabama': 'Alabama',\n    'Alaska': 'Alaska',\n    'Arizona': 'Arizona',\n    'Arkansas': 'Arkansas',\n    'California': 'California',\n    'Colorado': 'Colorado',\n    'Connecticut': 'Connecticut',\n    'Delaware': 'Delaware',\n    'Florida': 'Florida',\n    'Georgia': 'Georgia',\n    'Hawaii': 'Hawaii',\n    'idaho-3166': 'Idaho',\n    'Illinois': 'Illinois',\n    'Indiana': 'Indiana',\n    'Iowa': 'Iowa',\n    'Kansas': 'Kansas',\n    'Kentucky': 'Kentucky',\n    'Louisiana': 'Louisiana',\n    'Maine': 'Maine',\n    'Maryland': 'Maryland',\n    'Massachusetts': 'Massachusetts',\n    'Michigan': 'Michigan',\n    'Minnesota': 'Minnesota',\n    'Mississippi': 'Mississippi',\n    'Missouri': 'Missouri',\n    'Montana': 'Montana',\n    'Nebraska': 'Nebraska',\n    'Nevada': 'Nevada',\n    'new-hampshire': 'NewHampshire',\n    'new-jersey': 'NewJersey',\n    'new-mexico': 'NewMexico',\n    'new-york': 'NewYork',\n    'north-carolina': 'NorthCarolina',\n    'north-dakota': 'NorthDakota',\n    'Ohio': 'Ohio',\n    'Oklahoma': 'Oklahoma',\n    'Oregon': 'Oregon',\n    'Pennsylvania': 'Pennsylvania',\n    'rhode-island': 'RhodeIsland',\n    'south-carolina': 'SouthCarolina',\n    'south-dakota': 'SouthDakota',\n    'Tennessee': 'Tennessee',\n    'Texas': 'Texas',\n    'Utah': 'Utah',\n    'Vermont': 'Vermont',\n    'Virginia': 'Virginia',\n    'Washington': 'Washington',\n    'west-virginia': 'WestVirginia',\n    'Wisconsin': 'Wisconsin',\n    'Wyoming': 'Wyoming'\n}\n\n\nstate_dictionary = {'Alabama': 11, 'Alaska': 11, 'Arizona': 49, 'Arkansas': 16, 'California': 152, 'Colorado': 69, 'Connecticut': 56, 'Delaware': 4, 'Florida': 18, 'Georgia': 17, 'Hawaii': 5, 'idaho-3166': 31, 'Illinois': 51, 'Indiana': 10, 'Iowa': 8, 'Kansas': 3, 'Kentucky': 9, 'Louisiana': 2, 'Maine': 27, 'Maryland': 16, 'Massachusetts': 146, 'Michigan': 55, 'Minnesota': 36, 'Mississippi': 3, 'Missouri': 11, 'Montana': 41, 'Nebraska': 3, 'Nevada': 16, 'new-hampshire': 41, 'new-jersey': 40, 'new-mexico': 25, 'new-york': 60, 'north-carolina': 26, 'north-dakota': 7, 'Ohio': 29, 'Oklahoma': 4, 'Oregon': 38, 'Pennsylvania': 54, 'rhode-island': 9, 'south-carolina': 6, 'south-dakota': 7, 'Tennessee': 16, 'Texas': 50, 'Utah': 62, 'Vermont': 25, 'Virginia': 27, 'Washington': 92, 'west-virginia': 18, 'Wisconsin': 25, 'Wyoming': 19}\n\nstate_parks_dictionary = {'Alabama': 1, 'Alaska': 1, 'Arizona': 3, 'Arkansas': 2, 'California': 8, 'Colorado': 4, 'Connecticut': 7, 'Delaware': 1, 'Florida': 2, 'Georgia': 2, 'Hawaii': 1, 'idaho-3166': 2, 'Illinois': 10, 'Indiana': 1, 'Iowa': 1, 'Kansas': 1, 'Kentucky': 1, 'Louisiana': 1, 'Maine': 3, 'Maryland': 1, 'Massachusetts': 7, 'Michigan': 5, 'Minnesota': 3, 'Mississippi': 1, 'Missouri': 2, 'Montana': 2, 'Nebraska': 1, 'Nevada': 1, 'new-hampshire': 3, 'new-jersey': 3, 'new-mexico': 2, 'new-york': 5, 'north-carolina': 3, 'north-dakota': 1, 'Ohio': 4, 'Oklahoma': 1, 'Oregon': 3, 'Pennsylvania': 3, 'rhode-island': 1, 'south-carolina': 1, 'south-dakota': 1, 'Tennessee': 2, 'Texas': 4, 'Utah': 3, 'Vermont': 2, 'Virginia': 2, 'Washington': 6, 'west-virginia': 2, 'Wisconsin': 3, 'Wyoming': 1}\nstate_dictionary and state_parks_dictionary store the number of pages required for each state. states simply contains the names of all the states in alphabetical order, and state_name_code_dict helps sort between the name of a state and the way in which it is displayed on TrailForks URLs."
  },
  {
    "objectID": "posts/finalproject/FinalProject (1).html#connecting-national-parks-to-individual-trailpark-info",
    "href": "posts/finalproject/FinalProject (1).html#connecting-national-parks-to-individual-trailpark-info",
    "title": "Trail Recommendations",
    "section": "Connecting National Parks to Individual Trail/Park Info",
    "text": "Connecting National Parks to Individual Trail/Park Info\nNow we need to make sure to connect the data that we’ve collected here with the actual table generated by the recommender to give the user more information. Let’s take a look at our output from the similarity score model:\n\noutput\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nBecause we have two different SQL databases, one for nation-wide park data (trails_new.db) and one with state-wide trail data (trails.db), let’s split this into two different frames.\n\ncalifornia_df = output[output['state'] == 'California (CA)']\nnon_california_df = output[output['state'] != 'California (CA)']\n\nNow we’ll get our databases in our notebook:\n\nimport wget\nurl = 'https://raw.githubusercontent.com/torwar02/trails/main/trails/trails.db'\nfilename = wget.download(url)\nprint(filename)\n\n  0% [                                                                          ]       0 / 3534848  0% [                                                                          ]    8192 / 3534848  0% [                                                                          ]   16384 / 3534848  0% [                                                                          ]   24576 / 3534848  0% [                                                                          ]   32768 / 3534848  1% [                                                                          ]   40960 / 3534848  1% [.                                                                         ]   49152 / 3534848  1% [.                                                                         ]   57344 / 3534848  1% [.                                                                         ]   65536 / 3534848  2% [.                                                                         ]   73728 / 3534848  2% [.                                                                         ]   81920 / 3534848  2% [.                                                                         ]   90112 / 3534848  2% [..                                                                        ]   98304 / 3534848  3% [..                                                                        ]  106496 / 3534848  3% [..                                                                        ]  114688 / 3534848  3% [..                                                                        ]  122880 / 3534848  3% [..                                                                        ]  131072 / 3534848  3% [..                                                                        ]  139264 / 3534848  4% [...                                                                       ]  147456 / 3534848  4% [...                                                                       ]  155648 / 3534848  4% [...                                                                       ]  163840 / 3534848  4% [...                                                                       ]  172032 / 3534848  5% [...                                                                       ]  180224 / 3534848  5% [...                                                                       ]  188416 / 3534848  5% [....                                                                      ]  196608 / 3534848  5% [....                                                                      ]  204800 / 3534848  6% [....                                                                      ]  212992 / 3534848  6% [....                                                                      ]  221184 / 3534848  6% [....                                                                      ]  229376 / 3534848  6% [....                                                                      ]  237568 / 3534848  6% [.....                                                                     ]  245760 / 3534848  7% [.....                                                                     ]  253952 / 3534848  7% [.....                                                                     ]  262144 / 3534848  7% [.....                                                                     ]  270336 / 3534848  7% [.....                                                                     ]  278528 / 3534848  8% [......                                                                    ]  286720 / 3534848  8% [......                                                                    ]  294912 / 3534848  8% [......                                                                    ]  303104 / 3534848  8% [......                                                                    ]  311296 / 3534848  9% [......                                                                    ]  319488 / 3534848  9% [......                                                                    ]  327680 / 3534848  9% [.......                                                                   ]  335872 / 3534848  9% [.......                                                                   ]  344064 / 3534848  9% [.......                                                                   ]  352256 / 3534848 10% [.......                                                                   ]  360448 / 3534848 10% [.......                                                                   ]  368640 / 3534848 10% [.......                                                                   ]  376832 / 3534848 10% [........                                                                  ]  385024 / 3534848 11% [........                                                                  ]  393216 / 3534848 11% [........                                                                  ]  401408 / 3534848 11% [........                                                                  ]  409600 / 3534848 11% [........                                                                  ]  417792 / 3534848 12% [........                                                                  ]  425984 / 3534848 12% [.........                                                                 ]  434176 / 3534848 12% [.........                                                                 ]  442368 / 3534848 12% [.........                                                                 ]  450560 / 3534848 12% [.........                                                                 ]  458752 / 3534848 13% [.........                                                                 ]  466944 / 3534848 13% [.........                                                                 ]  475136 / 3534848 13% [..........                                                                ]  483328 / 3534848 13% [..........                                                                ]  491520 / 3534848 14% [..........                                                                ]  499712 / 3534848 14% [..........                                                                ]  507904 / 3534848 14% [..........                                                                ]  516096 / 3534848 14% [..........                                                                ]  524288 / 3534848 15% [...........                                                               ]  532480 / 3534848 15% [...........                                                               ]  540672 / 3534848 15% [...........                                                               ]  548864 / 3534848 15% [...........                                                               ]  557056 / 3534848 15% [...........                                                               ]  565248 / 3534848 16% [............                                                              ]  573440 / 3534848 16% [............                                                              ]  581632 / 3534848 16% [............                                                              ]  589824 / 3534848 16% [............                                                              ]  598016 / 3534848 17% [............                                                              ]  606208 / 3534848 17% [............                                                              ]  614400 / 3534848 17% [.............                                                             ]  622592 / 3534848 17% [.............                                                             ]  630784 / 3534848 18% [.............                                                             ]  638976 / 3534848 18% [.............                                                             ]  647168 / 3534848 18% [.............                                                             ]  655360 / 3534848 18% [.............                                                             ]  663552 / 3534848 19% [..............                                                            ]  671744 / 3534848 19% [..............                                                            ]  679936 / 3534848 19% [..............                                                            ]  688128 / 3534848 19% [..............                                                            ]  696320 / 3534848 19% [..............                                                            ]  704512 / 3534848 20% [..............                                                            ]  712704 / 3534848 20% [...............                                                           ]  720896 / 3534848 20% [...............                                                           ]  729088 / 3534848 20% [...............                                                           ]  737280 / 3534848 21% [...............                                                           ]  745472 / 3534848 21% [...............                                                           ]  753664 / 3534848 21% [...............                                                           ]  761856 / 3534848 21% [................                                                          ]  770048 / 3534848 22% [................                                                          ]  778240 / 3534848 22% [................                                                          ]  786432 / 3534848 22% [................                                                          ]  794624 / 3534848 22% [................                                                          ]  802816 / 3534848 22% [................                                                          ]  811008 / 3534848 23% [.................                                                         ]  819200 / 3534848 23% [.................                                                         ]  827392 / 3534848 23% [.................                                                         ]  835584 / 3534848 23% [.................                                                         ]  843776 / 3534848 24% [.................                                                         ]  851968 / 3534848 24% [..................                                                        ]  860160 / 3534848 24% [..................                                                        ]  868352 / 3534848 24% [..................                                                        ]  876544 / 3534848 25% [..................                                                        ]  884736 / 3534848 25% [..................                                                        ]  892928 / 3534848 25% [..................                                                        ]  901120 / 3534848 25% [...................                                                       ]  909312 / 3534848 25% [...................                                                       ]  917504 / 3534848 26% [...................                                                       ]  925696 / 3534848 26% [...................                                                       ]  933888 / 3534848 26% [...................                                                       ]  942080 / 3534848 26% [...................                                                       ]  950272 / 3534848 27% [....................                                                      ]  958464 / 3534848 27% [....................                                                      ]  966656 / 3534848 27% [....................                                                      ]  974848 / 3534848 27% [....................                                                      ]  983040 / 3534848 28% [....................                                                      ]  991232 / 3534848 28% [....................                                                      ]  999424 / 3534848 28% [.....................                                                     ] 1007616 / 3534848 28% [.....................                                                     ] 1015808 / 3534848 28% [.....................                                                     ] 1024000 / 3534848 29% [.....................                                                     ] 1032192 / 3534848 29% [.....................                                                     ] 1040384 / 3534848 29% [.....................                                                     ] 1048576 / 3534848 29% [......................                                                    ] 1056768 / 3534848 30% [......................                                                    ] 1064960 / 3534848 30% [......................                                                    ] 1073152 / 3534848 30% [......................                                                    ] 1081344 / 3534848 30% [......................                                                    ] 1089536 / 3534848 31% [......................                                                    ] 1097728 / 3534848 31% [.......................                                                   ] 1105920 / 3534848 31% [.......................                                                   ] 1114112 / 3534848 31% [.......................                                                   ] 1122304 / 3534848 31% [.......................                                                   ] 1130496 / 3534848 32% [.......................                                                   ] 1138688 / 3534848 32% [........................                                                  ] 1146880 / 3534848 32% [........................                                                  ] 1155072 / 3534848 32% [........................                                                  ] 1163264 / 3534848 33% [........................                                                  ] 1171456 / 3534848 33% [........................                                                  ] 1179648 / 3534848 33% [........................                                                  ] 1187840 / 3534848 33% [.........................                                                 ] 1196032 / 3534848 34% [.........................                                                 ] 1204224 / 3534848 34% [.........................                                                 ] 1212416 / 3534848 34% [.........................                                                 ] 1220608 / 3534848 34% [.........................                                                 ] 1228800 / 3534848 34% [.........................                                                 ] 1236992 / 3534848 35% [..........................                                                ] 1245184 / 3534848 35% [..........................                                                ] 1253376 / 3534848 35% [..........................                                                ] 1261568 / 3534848 35% [..........................                                                ] 1269760 / 3534848 36% [..........................                                                ] 1277952 / 3534848 36% [..........................                                                ] 1286144 / 3534848 36% [...........................                                               ] 1294336 / 3534848 36% [...........................                                               ] 1302528 / 3534848 37% [...........................                                               ] 1310720 / 3534848 37% [...........................                                               ] 1318912 / 3534848 37% [...........................                                               ] 1327104 / 3534848 37% [...........................                                               ] 1335296 / 3534848 38% [............................                                              ] 1343488 / 3534848 38% [............................                                              ] 1351680 / 3534848 38% [............................                                              ] 1359872 / 3534848 38% [............................                                              ] 1368064 / 3534848 38% [............................                                              ] 1376256 / 3534848 39% [............................                                              ] 1384448 / 3534848 39% [.............................                                             ] 1392640 / 3534848 39% [.............................                                             ] 1400832 / 3534848 39% [.............................                                             ] 1409024 / 3534848 40% [.............................                                             ] 1417216 / 3534848 40% [.............................                                             ] 1425408 / 3534848 40% [..............................                                            ] 1433600 / 3534848 40% [..............................                                            ] 1441792 / 3534848 41% [..............................                                            ] 1449984 / 3534848 41% [..............................                                            ] 1458176 / 3534848 41% [..............................                                            ] 1466368 / 3534848 41% [..............................                                            ] 1474560 / 3534848 41% [...............................                                           ] 1482752 / 3534848 42% [...............................                                           ] 1490944 / 3534848 42% [...............................                                           ] 1499136 / 3534848 42% [...............................                                           ] 1507328 / 3534848 42% [...............................                                           ] 1515520 / 3534848 43% [...............................                                           ] 1523712 / 3534848 43% [................................                                          ] 1531904 / 3534848 43% [................................                                          ] 1540096 / 3534848 43% [................................                                          ] 1548288 / 3534848 44% [................................                                          ] 1556480 / 3534848 44% [................................                                          ] 1564672 / 3534848 44% [................................                                          ] 1572864 / 3534848 44% [.................................                                         ] 1581056 / 3534848 44% [.................................                                         ] 1589248 / 3534848 45% [.................................                                         ] 1597440 / 3534848 45% [.................................                                         ] 1605632 / 3534848 45% [.................................                                         ] 1613824 / 3534848 45% [.................................                                         ] 1622016 / 3534848 46% [..................................                                        ] 1630208 / 3534848 46% [..................................                                        ] 1638400 / 3534848 46% [..................................                                        ] 1646592 / 3534848 46% [..................................                                        ] 1654784 / 3534848 47% [..................................                                        ] 1662976 / 3534848 47% [..................................                                        ] 1671168 / 3534848 47% [...................................                                       ] 1679360 / 3534848 47% [...................................                                       ] 1687552 / 3534848 47% [...................................                                       ] 1695744 / 3534848 48% [...................................                                       ] 1703936 / 3534848 48% [...................................                                       ] 1712128 / 3534848 48% [....................................                                      ] 1720320 / 3534848 48% [....................................                                      ] 1728512 / 3534848 49% [....................................                                      ] 1736704 / 3534848 49% [....................................                                      ] 1744896 / 3534848 49% [....................................                                      ] 1753088 / 3534848 49% [....................................                                      ] 1761280 / 3534848 50% [.....................................                                     ] 1769472 / 3534848 50% [.....................................                                     ] 1777664 / 3534848 50% [.....................................                                     ] 1785856 / 3534848 50% [.....................................                                     ] 1794048 / 3534848 50% [.....................................                                     ] 1802240 / 3534848 51% [.....................................                                     ] 1810432 / 3534848 51% [......................................                                    ] 1818624 / 3534848 51% [......................................                                    ] 1826816 / 3534848 51% [......................................                                    ] 1835008 / 3534848 52% [......................................                                    ] 1843200 / 3534848 52% [......................................                                    ] 1851392 / 3534848 52% [......................................                                    ] 1859584 / 3534848 52% [.......................................                                   ] 1867776 / 3534848 53% [.......................................                                   ] 1875968 / 3534848 53% [.......................................                                   ] 1884160 / 3534848 53% [.......................................                                   ] 1892352 / 3534848 53% [.......................................                                   ] 1900544 / 3534848 53% [.......................................                                   ] 1908736 / 3534848 54% [........................................                                  ] 1916928 / 3534848 54% [........................................                                  ] 1925120 / 3534848 54% [........................................                                  ] 1933312 / 3534848 54% [........................................                                  ] 1941504 / 3534848 55% [........................................                                  ] 1949696 / 3534848 55% [........................................                                  ] 1957888 / 3534848 55% [.........................................                                 ] 1966080 / 3534848 55% [.........................................                                 ] 1974272 / 3534848 56% [.........................................                                 ] 1982464 / 3534848 56% [.........................................                                 ] 1990656 / 3534848 56% [.........................................                                 ] 1998848 / 3534848 56% [..........................................                                ] 2007040 / 3534848 57% [..........................................                                ] 2015232 / 3534848 57% [..........................................                                ] 2023424 / 3534848 57% [..........................................                                ] 2031616 / 3534848 57% [..........................................                                ] 2039808 / 3534848 57% [..........................................                                ] 2048000 / 3534848 58% [...........................................                               ] 2056192 / 3534848 58% [...........................................                               ] 2064384 / 3534848 58% [...........................................                               ] 2072576 / 3534848 58% [...........................................                               ] 2080768 / 3534848 59% [...........................................                               ] 2088960 / 3534848 59% [...........................................                               ] 2097152 / 3534848 59% [............................................                              ] 2105344 / 3534848 59% [............................................                              ] 2113536 / 3534848 60% [............................................                              ] 2121728 / 3534848 60% [............................................                              ] 2129920 / 3534848 60% [............................................                              ] 2138112 / 3534848 60% [............................................                              ] 2146304 / 3534848 60% [.............................................                             ] 2154496 / 3534848 61% [.............................................                             ] 2162688 / 3534848 61% [.............................................                             ] 2170880 / 3534848 61% [.............................................                             ] 2179072 / 3534848 61% [.............................................                             ] 2187264 / 3534848 62% [.............................................                             ] 2195456 / 3534848 62% [..............................................                            ] 2203648 / 3534848 62% [..............................................                            ] 2211840 / 3534848 62% [..............................................                            ] 2220032 / 3534848 63% [..............................................                            ] 2228224 / 3534848 63% [..............................................                            ] 2236416 / 3534848 63% [..............................................                            ] 2244608 / 3534848 63% [...............................................                           ] 2252800 / 3534848 63% [...............................................                           ] 2260992 / 3534848 64% [...............................................                           ] 2269184 / 3534848 64% [...............................................                           ] 2277376 / 3534848 64% [...............................................                           ] 2285568 / 3534848 64% [................................................                          ] 2293760 / 3534848 65% [................................................                          ] 2301952 / 3534848 65% [................................................                          ] 2310144 / 3534848 65% [................................................                          ] 2318336 / 3534848 65% [................................................                          ] 2326528 / 3534848 66% [................................................                          ] 2334720 / 3534848 66% [.................................................                         ] 2342912 / 3534848 66% [.................................................                         ] 2351104 / 3534848 66% [.................................................                         ] 2359296 / 3534848 66% [.................................................                         ] 2367488 / 3534848 67% [.................................................                         ] 2375680 / 3534848 67% [.................................................                         ] 2383872 / 3534848 67% [..................................................                        ] 2392064 / 3534848 67% [..................................................                        ] 2400256 / 3534848 68% [..................................................                        ] 2408448 / 3534848 68% [..................................................                        ] 2416640 / 3534848 68% [..................................................                        ] 2424832 / 3534848 68% [..................................................                        ] 2433024 / 3534848 69% [...................................................                       ] 2441216 / 3534848 69% [...................................................                       ] 2449408 / 3534848 69% [...................................................                       ] 2457600 / 3534848 69% [...................................................                       ] 2465792 / 3534848 69% [...................................................                       ] 2473984 / 3534848 70% [...................................................                       ] 2482176 / 3534848 70% [....................................................                      ] 2490368 / 3534848 70% [....................................................                      ] 2498560 / 3534848 70% [....................................................                      ] 2506752 / 3534848 71% [....................................................                      ] 2514944 / 3534848 71% [....................................................                      ] 2523136 / 3534848 71% [....................................................                      ] 2531328 / 3534848 71% [.....................................................                     ] 2539520 / 3534848 72% [.....................................................                     ] 2547712 / 3534848 72% [.....................................................                     ] 2555904 / 3534848 72% [.....................................................                     ] 2564096 / 3534848 72% [.....................................................                     ] 2572288 / 3534848 73% [......................................................                    ] 2580480 / 3534848 73% [......................................................                    ] 2588672 / 3534848 73% [......................................................                    ] 2596864 / 3534848 73% [......................................................                    ] 2605056 / 3534848 73% [......................................................                    ] 2613248 / 3534848 74% [......................................................                    ] 2621440 / 3534848 74% [.......................................................                   ] 2629632 / 3534848 74% [.......................................................                   ] 2637824 / 3534848 74% [.......................................................                   ] 2646016 / 3534848 75% [.......................................................                   ] 2654208 / 3534848 75% [.......................................................                   ] 2662400 / 3534848 75% [.......................................................                   ] 2670592 / 3534848 75% [........................................................                  ] 2678784 / 3534848 76% [........................................................                  ] 2686976 / 3534848 76% [........................................................                  ] 2695168 / 3534848 76% [........................................................                  ] 2703360 / 3534848 76% [........................................................                  ] 2711552 / 3534848 76% [........................................................                  ] 2719744 / 3534848 77% [.........................................................                 ] 2727936 / 3534848 77% [.........................................................                 ] 2736128 / 3534848 77% [.........................................................                 ] 2744320 / 3534848 77% [.........................................................                 ] 2752512 / 3534848 78% [.........................................................                 ] 2760704 / 3534848 78% [.........................................................                 ] 2768896 / 3534848 78% [..........................................................                ] 2777088 / 3534848 78% [..........................................................                ] 2785280 / 3534848 79% [..........................................................                ] 2793472 / 3534848 79% [..........................................................                ] 2801664 / 3534848 79% [..........................................................                ] 2809856 / 3534848 79% [..........................................................                ] 2818048 / 3534848 79% [...........................................................               ] 2826240 / 3534848 80% [...........................................................               ] 2834432 / 3534848 80% [...........................................................               ] 2842624 / 3534848 80% [...........................................................               ] 2850816 / 3534848 80% [...........................................................               ] 2859008 / 3534848 81% [............................................................              ] 2867200 / 3534848 81% [............................................................              ] 2875392 / 3534848 81% [............................................................              ] 2883584 / 3534848 81% [............................................................              ] 2891776 / 3534848 82% [............................................................              ] 2899968 / 3534848 82% [............................................................              ] 2908160 / 3534848 82% [.............................................................             ] 2916352 / 3534848 82% [.............................................................             ] 2924544 / 3534848 82% [.............................................................             ] 2932736 / 3534848 83% [.............................................................             ] 2940928 / 3534848 83% [.............................................................             ] 2949120 / 3534848 83% [.............................................................             ] 2957312 / 3534848 83% [..............................................................            ] 2965504 / 3534848 84% [..............................................................            ] 2973696 / 3534848 84% [..............................................................            ] 2981888 / 3534848 84% [..............................................................            ] 2990080 / 3534848 84% [..............................................................            ] 2998272 / 3534848 85% [..............................................................            ] 3006464 / 3534848 85% [...............................................................           ] 3014656 / 3534848 85% [...............................................................           ] 3022848 / 3534848 85% [...............................................................           ] 3031040 / 3534848 85% [...............................................................           ] 3039232 / 3534848 86% [...............................................................           ] 3047424 / 3534848 86% [...............................................................           ] 3055616 / 3534848 86% [................................................................          ] 3063808 / 3534848 86% [................................................................          ] 3072000 / 3534848 87% [................................................................          ] 3080192 / 3534848 87% [................................................................          ] 3088384 / 3534848 87% [................................................................          ] 3096576 / 3534848 87% [................................................................          ] 3104768 / 3534848 88% [.................................................................         ] 3112960 / 3534848 88% [.................................................................         ] 3121152 / 3534848 88% [.................................................................         ] 3129344 / 3534848 88% [.................................................................         ] 3137536 / 3534848 88% [.................................................................         ] 3145728 / 3534848 89% [..................................................................        ] 3153920 / 3534848 89% [..................................................................        ] 3162112 / 3534848 89% [..................................................................        ] 3170304 / 3534848 89% [..................................................................        ] 3178496 / 3534848 90% [..................................................................        ] 3186688 / 3534848 90% [..................................................................        ] 3194880 / 3534848 90% [...................................................................       ] 3203072 / 3534848 90% [...................................................................       ] 3211264 / 3534848 91% [...................................................................       ] 3219456 / 3534848 91% [...................................................................       ] 3227648 / 3534848 91% [...................................................................       ] 3235840 / 3534848 91% [...................................................................       ] 3244032 / 3534848 92% [....................................................................      ] 3252224 / 3534848 92% [....................................................................      ] 3260416 / 3534848 92% [....................................................................      ] 3268608 / 3534848 92% [....................................................................      ] 3276800 / 3534848 92% [....................................................................      ] 3284992 / 3534848 93% [....................................................................      ] 3293184 / 3534848 93% [.....................................................................     ] 3301376 / 3534848 93% [.....................................................................     ] 3309568 / 3534848 93% [.....................................................................     ] 3317760 / 3534848 94% [.....................................................................     ] 3325952 / 3534848 94% [.....................................................................     ] 3334144 / 3534848 94% [.....................................................................     ] 3342336 / 3534848 94% [......................................................................    ] 3350528 / 3534848 95% [......................................................................    ] 3358720 / 3534848 95% [......................................................................    ] 3366912 / 3534848 95% [......................................................................    ] 3375104 / 3534848 95% [......................................................................    ] 3383296 / 3534848 95% [......................................................................    ] 3391488 / 3534848 96% [.......................................................................   ] 3399680 / 3534848 96% [.......................................................................   ] 3407872 / 3534848 96% [.......................................................................   ] 3416064 / 3534848 96% [.......................................................................   ] 3424256 / 3534848 97% [.......................................................................   ] 3432448 / 3534848 97% [........................................................................  ] 3440640 / 3534848 97% [........................................................................  ] 3448832 / 3534848 97% [........................................................................  ] 3457024 / 3534848 98% [........................................................................  ] 3465216 / 3534848 98% [........................................................................  ] 3473408 / 3534848 98% [........................................................................  ] 3481600 / 3534848 98% [......................................................................... ] 3489792 / 3534848 98% [......................................................................... ] 3497984 / 3534848 99% [......................................................................... ] 3506176 / 3534848 99% [......................................................................... ] 3514368 / 3534848 99% [......................................................................... ] 3522560 / 3534848 99% [......................................................................... ] 3530752 / 3534848100% [..........................................................................] 3534848 / 3534848trails (1).db\n\n\n\nurl = 'https://raw.githubusercontent.com/torwar02/trails/main/trails/trails_new.db'\nfilename = wget.download(url)\nprint(filename)\n\n  0% [                                                                          ]       0 / 1339392  0% [                                                                          ]    8192 / 1339392  1% [                                                                          ]   16384 / 1339392  1% [.                                                                         ]   24576 / 1339392  2% [.                                                                         ]   32768 / 1339392  3% [..                                                                        ]   40960 / 1339392  3% [..                                                                        ]   49152 / 1339392  4% [...                                                                       ]   57344 / 1339392  4% [...                                                                       ]   65536 / 1339392  5% [....                                                                      ]   73728 / 1339392  6% [....                                                                      ]   81920 / 1339392  6% [....                                                                      ]   90112 / 1339392  7% [.....                                                                     ]   98304 / 1339392  7% [.....                                                                     ]  106496 / 1339392  8% [......                                                                    ]  114688 / 1339392  9% [......                                                                    ]  122880 / 1339392  9% [.......                                                                   ]  131072 / 1339392 10% [.......                                                                   ]  139264 / 1339392 11% [........                                                                  ]  147456 / 1339392 11% [........                                                                  ]  155648 / 1339392 12% [.........                                                                 ]  163840 / 1339392 12% [.........                                                                 ]  172032 / 1339392 13% [.........                                                                 ]  180224 / 1339392 14% [..........                                                                ]  188416 / 1339392 14% [..........                                                                ]  196608 / 1339392 15% [...........                                                               ]  204800 / 1339392 15% [...........                                                               ]  212992 / 1339392 16% [............                                                              ]  221184 / 1339392 17% [............                                                              ]  229376 / 1339392 17% [.............                                                             ]  237568 / 1339392 18% [.............                                                             ]  245760 / 1339392 18% [..............                                                            ]  253952 / 1339392 19% [..............                                                            ]  262144 / 1339392 20% [..............                                                            ]  270336 / 1339392 20% [...............                                                           ]  278528 / 1339392 21% [...............                                                           ]  286720 / 1339392 22% [................                                                          ]  294912 / 1339392 22% [................                                                          ]  303104 / 1339392 23% [.................                                                         ]  311296 / 1339392 23% [.................                                                         ]  319488 / 1339392 24% [..................                                                        ]  327680 / 1339392 25% [..................                                                        ]  335872 / 1339392 25% [...................                                                       ]  344064 / 1339392 26% [...................                                                       ]  352256 / 1339392 26% [...................                                                       ]  360448 / 1339392 27% [....................                                                      ]  368640 / 1339392 28% [....................                                                      ]  376832 / 1339392 28% [.....................                                                     ]  385024 / 1339392 29% [.....................                                                     ]  393216 / 1339392 29% [......................                                                    ]  401408 / 1339392 30% [......................                                                    ]  409600 / 1339392 31% [.......................                                                   ]  417792 / 1339392 31% [.......................                                                   ]  425984 / 1339392 32% [.......................                                                   ]  434176 / 1339392 33% [........................                                                  ]  442368 / 1339392 33% [........................                                                  ]  450560 / 1339392 34% [.........................                                                 ]  458752 / 1339392 34% [.........................                                                 ]  466944 / 1339392 35% [..........................                                                ]  475136 / 1339392 36% [..........................                                                ]  483328 / 1339392 36% [...........................                                               ]  491520 / 1339392 37% [...........................                                               ]  499712 / 1339392 37% [............................                                              ]  507904 / 1339392 38% [............................                                              ]  516096 / 1339392 39% [............................                                              ]  524288 / 1339392 39% [.............................                                             ]  532480 / 1339392 40% [.............................                                             ]  540672 / 1339392 40% [..............................                                            ]  548864 / 1339392 41% [..............................                                            ]  557056 / 1339392 42% [...............................                                           ]  565248 / 1339392 42% [...............................                                           ]  573440 / 1339392 43% [................................                                          ]  581632 / 1339392 44% [................................                                          ]  589824 / 1339392 44% [.................................                                         ]  598016 / 1339392 45% [.................................                                         ]  606208 / 1339392 45% [.................................                                         ]  614400 / 1339392 46% [..................................                                        ]  622592 / 1339392 47% [..................................                                        ]  630784 / 1339392 47% [...................................                                       ]  638976 / 1339392 48% [...................................                                       ]  647168 / 1339392 48% [....................................                                      ]  655360 / 1339392 49% [....................................                                      ]  663552 / 1339392 50% [.....................................                                     ]  671744 / 1339392 50% [.....................................                                     ]  679936 / 1339392 51% [......................................                                    ]  688128 / 1339392 51% [......................................                                    ]  696320 / 1339392 52% [......................................                                    ]  704512 / 1339392 53% [.......................................                                   ]  712704 / 1339392 53% [.......................................                                   ]  720896 / 1339392 54% [........................................                                  ]  729088 / 1339392 55% [........................................                                  ]  737280 / 1339392 55% [.........................................                                 ]  745472 / 1339392 56% [.........................................                                 ]  753664 / 1339392 56% [..........................................                                ]  761856 / 1339392 57% [..........................................                                ]  770048 / 1339392 58% [..........................................                                ]  778240 / 1339392 58% [...........................................                               ]  786432 / 1339392 59% [...........................................                               ]  794624 / 1339392 59% [............................................                              ]  802816 / 1339392 60% [............................................                              ]  811008 / 1339392 61% [.............................................                             ]  819200 / 1339392 61% [.............................................                             ]  827392 / 1339392 62% [..............................................                            ]  835584 / 1339392 62% [..............................................                            ]  843776 / 1339392 63% [...............................................                           ]  851968 / 1339392 64% [...............................................                           ]  860160 / 1339392 64% [...............................................                           ]  868352 / 1339392 65% [................................................                          ]  876544 / 1339392 66% [................................................                          ]  884736 / 1339392 66% [.................................................                         ]  892928 / 1339392 67% [.................................................                         ]  901120 / 1339392 67% [..................................................                        ]  909312 / 1339392 68% [..................................................                        ]  917504 / 1339392 69% [...................................................                       ]  925696 / 1339392 69% [...................................................                       ]  933888 / 1339392 70% [....................................................                      ]  942080 / 1339392 70% [....................................................                      ]  950272 / 1339392 71% [....................................................                      ]  958464 / 1339392 72% [.....................................................                     ]  966656 / 1339392 72% [.....................................................                     ]  974848 / 1339392 73% [......................................................                    ]  983040 / 1339392 74% [......................................................                    ]  991232 / 1339392 74% [.......................................................                   ]  999424 / 1339392 75% [.......................................................                   ] 1007616 / 1339392 75% [........................................................                  ] 1015808 / 1339392 76% [........................................................                  ] 1024000 / 1339392 77% [.........................................................                 ] 1032192 / 1339392 77% [.........................................................                 ] 1040384 / 1339392 78% [.........................................................                 ] 1048576 / 1339392 78% [..........................................................                ] 1056768 / 1339392 79% [..........................................................                ] 1064960 / 1339392 80% [...........................................................               ] 1073152 / 1339392 80% [...........................................................               ] 1081344 / 1339392 81% [............................................................              ] 1089536 / 1339392 81% [............................................................              ] 1097728 / 1339392 82% [.............................................................             ] 1105920 / 1339392 83% [.............................................................             ] 1114112 / 1339392 83% [..............................................................            ] 1122304 / 1339392 84% [..............................................................            ] 1130496 / 1339392 85% [..............................................................            ] 1138688 / 1339392 85% [...............................................................           ] 1146880 / 1339392 86% [...............................................................           ] 1155072 / 1339392 86% [................................................................          ] 1163264 / 1339392 87% [................................................................          ] 1171456 / 1339392 88% [.................................................................         ] 1179648 / 1339392 88% [.................................................................         ] 1187840 / 1339392 89% [..................................................................        ] 1196032 / 1339392 89% [..................................................................        ] 1204224 / 1339392 90% [..................................................................        ] 1212416 / 1339392 91% [...................................................................       ] 1220608 / 1339392 91% [...................................................................       ] 1228800 / 1339392 92% [....................................................................      ] 1236992 / 1339392 92% [....................................................................      ] 1245184 / 1339392 93% [.....................................................................     ] 1253376 / 1339392 94% [.....................................................................     ] 1261568 / 1339392 94% [......................................................................    ] 1269760 / 1339392 95% [......................................................................    ] 1277952 / 1339392 96% [.......................................................................   ] 1286144 / 1339392 96% [.......................................................................   ] 1294336 / 1339392 97% [.......................................................................   ] 1302528 / 1339392 97% [........................................................................  ] 1310720 / 1339392 98% [........................................................................  ] 1318912 / 1339392 99% [......................................................................... ] 1327104 / 1339392 99% [......................................................................... ] 1335296 / 1339392100% [..........................................................................] 1339392 / 1339392trails_new (1).db\n\n\nThere’s a bit of an issue, though. Let’s look at our table names:\n\nimport sqlite3\ndb_path = 'trails_new.db'\n\nconn = sqlite3.connect(db_path) #Establish connection with DB\ncur = conn.cursor()\n\ncur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") #This specifically grabs all table names from our datbaase.\ntables = cur.fetchall()\ntable_names = [table[0] for table in tables] #Places them into a list\nprint(\"List of tables in the database:\", table_names)\nconn.close()\n\nList of tables in the database: ['Maine', 'California', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'idaho-3166', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'new-hampshire', 'new-jersey', 'new-mexico', 'new-york', 'north-carolina', 'north-dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'rhode-island', 'south-carolina', 'south-dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'west-virginia', 'Wisconsin', 'Wyoming']\n\n\nOur tables aren’t completely in alphabetical order (I was testing around with Maine first, for instance). And some of them aren’t two words, like south-dakota, for instance. But if we compare this to what we have in output:\n\nset(output['state'])\n\n{'California (CA)',\n 'Montana (MT)',\n 'South Dakota (SD)',\n 'Utah (UT)',\n 'Washington (WA)',\n 'Wyoming (WY)'}\n\n\nHere we have nice, capitalized state names with two-letter abbreviations. So, then, how are we going to fix this? We’re going to create a dictionary that essentially works as a mapping that takes what we have in output and matches it to what exists in table_names based on some matching criteria:\n\n# Extract unique states and sort them\nunique_states_in_output = sorted(set(output['state']), key=str.lower)\ntable_names = sorted(table_names, key=str.lower)\n\n\n\ndef compare_letters(state_name, table_name):\n    clean_state_name = ''.join(filter(str.isalpha, state_name)).lower() #Eliminate non-alphabetical characters, condense together\n    clean_table_name = ''.join(filter(str.isalpha, table_name)).lower()\n    return sorted(clean_state_name) == sorted(clean_table_name) #Gives a boolean value.\n\nstate_name_to_table_name = {} #Create new dictionary\nfor state_with_abbreviation in unique_states_in_output:\n    state_name = state_with_abbreviation.split(' (')[0]  # Get rid of the parentheses in the abbreviation (like 'South Dakota (SD)')\n    match = next((table for table in table_names if compare_letters(state_name, table)), None) #Generator based on whether or not names are the same\n    if match:\n        state_name_to_table_name[state_with_abbreviation] = match #Update dict if match found\n\nprint(state_name_to_table_name)\n\n{'California (CA)': 'California', 'Montana (MT)': 'Montana', 'South Dakota (SD)': 'south-dakota', 'Utah (UT)': 'Utah', 'Washington (WA)': 'Washington', 'Wyoming (WY)': 'Wyoming'}\n\n\nNow that’s what we’re looking for! We do a few important things here:\nFirstly, we make sure to get both the states that we have in output and the tables in table_names in alphabetical order. The reason why we do key=str.lower is because some of the table names are written in uppercase while others are in lowercase. This makes it case-insensitive.\nThen we create a helper function called compare_letters which takes two state names (one from output, one from the database) and compares them to see if they have the same letters. We do this by filtering out non-alphabetical characters, spaces, and making everything lowercase and just checking if they have the same letters. The function will just return True or False depending on whether or not they match.\nWe actually use state_name_to_table_name in the for loop below this. We go through each of the states in output. Then, we extract just the part of the state name that comes before the two-letter abbreviation, and then we create a generator that individualls calls compare_letters on each of the names. If it returns True, then we have a match, which then causes the dictionary to be updated. Otherwise, nothing happens and we simply move onto the next entry (that’s why the second argument of next is none).\n##Logic for linking databases\nOur goal is to now go through each recommendation, and match up either the park or trail information corresponding to it (assuming that it’s present in the database). One issue that can arise with this, however, is that the name of the park in output might be different from that of the database. To mitigate this, we’re going to instead compare the coordinates of what’s in output to the rows inside of trails_new.db and trails.db. The idea is that if two parks are close enough to each other in terms of their coordinates, then they should represent the same thing. So, we’re going to make two functions that do similar (but different) things. One will be called fetch_park_info_based_on_coords which will look at parks (i.e., outside of California), and the other will be called fetch_trail_info_based_on_coords\n\ndef fetch_park_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor() #Connect to database\n\n    for table_name in state_name_to_table_name.values(): #This is what we made earlier\n        cursor.execute(f\"SELECT * FROM \\\"{table_name}\\\"\") #Grab everything from the table\n        rows = cursor.fetchall()\n\n        for row in rows: #For each row\n            coords_text = row[2]  # Coords are in the third column\n            try:\n                coords = eval(coords_text) #Kept as a tuple, essentially\n                lat_diff = abs(coords[0] - latitude)\n                long_diff = abs(coords[1] - longitude)\n\n                if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                    return row[3:]  # Don't need name and coords\n            except:\n                continue\n\n    conn.close()\n    return None\n\ndef fetch_trail_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n    table_name = 'California'  #Only getting CA trails\n\n    cursor.execute(f\"SELECT * FROM {table_name}\") #Grab everything\n    rows = cursor.fetchall()\n\n    for row in rows:\n        coords_text = row[1]  # Coords are in column 2\n        try:\n            coords = eval(coords_text)\n            lat_diff = abs(coords[0] - latitude)\n            long_diff = abs(coords[1] - longitude)\n\n            if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                return row[2:]\n        except:\n            continue  # Skip rows with invalid 'Coords'\n\n    conn.close()\n    return None\n\nOkay, so, it will make a lot more sense if we actually inspect the structure of our database again. Click the link below to see screenshots of two .csv files: the first of parks in Wyoming, and the second is of trails in California:\nhttps://imgur.com/a/6fCixEt\nWith that out of the way, let’s dive into the code. We go through the mapping dictionary that we made previously and we grab all of the possible parks from each one. Then, we look at the third column (i.e., row[2], which represents the third entry in the row) which corresponds to the coordinates (see screenshot), and we record the absolute difference in the coordinates between a given latitude and longitude (we’ll be taking those from output–they’re individual columns rather than a tuple). If both of them are within a specified margin of error, then we’ve found our match. Note that we’re only going to return everything starting from the third column: the first 2 are just the name and coordinates of the trail.\nFor fetch_trail_info_based_on_coords, we have a very similar set-up except for the fact that the coordinates are in the second column, and we’re interested in returning everything after the first two.\nNow, let’s move on so we can see how we actually use these functions!"
  },
  {
    "objectID": "posts/finalproject/FinalProject (1).html#putting-it-all-together",
    "href": "posts/finalproject/FinalProject (1).html#putting-it-all-together",
    "title": "Trail Recommendations",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe first thing we’re going to do is to specify the names of the new columns that we want to put into california_df and non_california_df. I’ve just grabbed these from the database:\n\nnew_columns = [\n    'Trails (view details)', 'Total Distance', 'State Ranking',\n    'Access Road/Trail', 'White', 'Green', 'Blue', 'Black',\n    'Double Black Diamond', 'Proline'\n]\nnew_trail_columns = [\n    'Distance', 'Avg time', 'Climb', 'Descent', 'Activities',\n    'Riding Area', 'Difficulty Rating', 'Dogs Allowed',\n    'Local Popularity', 'Altitude start', 'Altitude end', 'Grade'\n]\n\nNow, all we need to do is iterate through the rows of non_california_df to match up the entires!\n\nmargin_lat = 0.1  # Decently generous\nmargin_long = 0.1\nfor index, row in non_california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']): #Some parks have NA coordinates\n        continue\n    park_info = fetch_park_info_based_on_coords('trails_new (1).db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n    #Remember, this grabs almost all of the columns if a match is found\n    if park_info:\n        non_california_df.loc[index, new_columns] = park_info #We can mass-add new columns\n\nIn the above code, we use the fetch_park_info_based_on_coords function to essentially create a new data frame that contains the information that we want once we match the coordinates. Then, we insert all of these as new columns, taking advantage of the .loc() method from pandas. Now let’s do the same thing for the California df:\n\n\nfor index, row in california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']):\n        continue\n\n    park_info = fetch_trail_info_based_on_coords('trails.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n\n    if park_info and len(park_info) == len(new_trail_columns):\n        california_df.loc[index, new_trail_columns] = park_info\n    else:\n        pass\n\nOkay, let’s take a look at our results!\n\nnon_california_df\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\n...\nTrails (view details)\nTotal Distance\nState Ranking\nAccess Road/Trail\nWhite\nGreen\nBlue\nBlack\nDouble Black Diamond\nProline\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n...\n60\n194 miles\n#9,609\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n...\n26\n53 miles\n#4,761\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n...\n25\n177 miles\n#9,011\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n9 rows × 22 columns\n\n\n\nSuccess! It looks like we unfortunately have a few NA values. Unfortunately, it’s hard to guarantee precision in the coordinates. We only had one trail for California:\n\ncalifornia_df\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n\n\n\n\n\nWait, really? I thought we would’ve had this for sure in our database…\nOn closer inspection, we actually do, but hte coordinates on TrailForks versus what we got from the National Park data is a bit off. On the TrailForks page for Zabriskie Point, the coordinates are (36.420820, -116.810120), which is just outside the margin of error."
  },
  {
    "objectID": "posts/final3/FinalProject (1).html",
    "href": "posts/final3/FinalProject (1).html",
    "title": "Trail Recommendations",
    "section": "",
    "text": "Using the internet, it’s very easy to find information about hiking trails throughout the United States, but there’s a problem: it’s a lot easier to browse through lists of trails available online, like on the website trailforks.com, and filtering by location. What are you supposed to do if you don’t know much about an area but still want to go hiking, or what if you want to do certain things on your hike but don’t know where to go? You could do lots of research online yourself, reading articles or sifting through different locations, but this process can be very difficult, especially if you want to go somewhere that you’ve never been.\nOur project addresses finding new places to visit based upon what type of hiking trails the user wants to see (which the user would describe) based upon previously hiked trails. For instance, if the user previously hiked Half Dome in Yosemite, then we give the user other trails and locations in the United States most similar to Half Dome according to reviews on TripAdvisor. This would be mainly for tourists who want to visit parts of the country they have not been to.\nThe below code is for importing images.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/final3/FinalProject (1).html#load-data",
    "href": "posts/final3/FinalProject (1).html#load-data",
    "title": "Trail Recommendations",
    "section": "Load Data",
    "text": "Load Data\nNow let us load the data we scraped by TripAdvisor as well as a Excel file containing coordinate points of our national parks so that we can create a geograpical plot later\n\ndf = pd.read_csv('https://raw.githubusercontent.com/torwar02/trails/main/trails/national_parks.csv')\n\n\ndf2 = pd.read_excel('https://raw.githubusercontent.com/torwar02/trails/main/trails/coords.xlsx')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nPark\nState(s)\nPark Established\nArea\nVisitors (2018)\n\n\n\n\n0\n44.35\n-68.21\nAcadia\nMaine\nFebruary 26, 1919\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\n-14.25\n-170.68\nAmerican Samoa\nAmerican Samoa\nOctober 31, 1988\n8,256.67 acres (33.4 km2)\n28626\n\n\n2\n38.68\n-109.57\nArches\nUtah\nNovember 12, 1971\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3\n43.75\n-102.50\nBadlands\nSouth Dakota\nNovember 10, 1978\n242,755.94 acres (982.4 km2)\n1008942\n\n\n4\n29.25\n-103.25\nBig Bend\nTexas\nJune 12, 1944\n801,163.21 acres (3,242.2 km2)\n440091\n\n\n\n\n\n\n\nTo merge the two files together, we utilize regex. Get string preceding ‘National Park’ in df such that we can merge with df2 on National Park name\n\nimport re\npattern = r'(.*?)(?:\\s+National Park)?$'\nresult = re.findall(pattern, df['national_park'].iloc[0])\npark = []\nfor row in df['national_park']:\n    test_park = re.findall(pattern, row)\n    park.append(test_park[0])\ndf['park'] = park\nnational_parks = pd.merge(df, df2, left_on='park', right_on='Park')\nnational_parks = national_parks.drop(columns = ['park', 'Park', 'State(s)', 'Park Established'])\nnational_parks.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575"
  },
  {
    "objectID": "posts/final3/FinalProject (1).html#word-embedding-and-comment-similarity-score",
    "href": "posts/final3/FinalProject (1).html#word-embedding-and-comment-similarity-score",
    "title": "Trail Recommendations",
    "section": "Word Embedding and Comment Similarity Score",
    "text": "Word Embedding and Comment Similarity Score\nFirst let us go over what Word Embedding is. Word embedding in NLP is an important technique that is used for representing words for text analysis in the form of real-valued vectors. In this approach, words and documents are represented in the form of numeric vectors allowing similar words to have similar vector representations. The extracted features are fed into a machine learning model so as to work with text data and preserve the semantic and syntactic information. This information once received in its converted form is used by NLP algorithms that easily digest these learned representations and process textual information.\n\nComment Similarity Function\nNow let us create a function called comment_similarity which takes in our national_parks.csv file we just created via the park_data parameter, a comment_index parameter, and an all_comments parameter which is our word embedding vector representation of all comments in our csv file.\n\nall_docs = [nlp(row) for row in national_parks['comment_text']] #getting vector representation of all comments in our csv file\n\n\ndef comment_similarity(parks_data, comment_index, all_comments):\n    example_comment = parks_data.loc[comment_index, 'comment_text']\n    reference_comment = nlp(example_comment) #vectorize our reference sentence\n    simularity_score = []\n    row_id = []\n    for i in range(len(all_comments)):\n        sim_score = all_comments[i].similarity(reference_comment)\n        simularity_score.append(sim_score)\n        row_id.append(i)\n    simularity_docs = pd.DataFrame(list(zip(row_id, simularity_score)), columns = ['Comment_ID', 'sims'])\n    simularity_docs_sorted = simularity_docs.sort_values(by = 'sims', ascending = False)\n    most_similar_comments = simularity_docs_sorted['Comment_ID'][1:2]\n    new_reviews = national_parks.iloc[most_similar_comments.values]\n    return(new_reviews)\n\nNow let us show what our returned dataframe looks like\n\nshowcase = comment_similarity(national_parks, 0, all_docs)\nshowcase\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1552\nGrand Canyon National Park\nArizona (AZ)\nGrand Canyon South Rim\nCanyons\n5.0\nThe views do not disappoint!\n5.0 of 5 bubbles\nWe were staying with family in Sun City (near ...\n36.06\n-112.14\n1,201,647.03 acres (4,862.9 km2)\n6380495\n\n\n\n\n\n\n\nAs we can see, we return a dataframe of the most similar review to the review with the index 999. To see how similar this similar review is to our inputted review let us output both comments.\nFirst the original comment\n\nexample_comment = national_parks.loc[0, 'comment_text']\nexample_comment\n\n\"I have hiked to the fire tower a few times. Its a great hike, and not too strenuous elevation gains.  If the NO rangers are up there ( in the summer) they used to allow you to go up the tower. We had to turn back on 3/20 because of hard pack solid ice. We had our Katoohla micro spikes on, and solid hiking poles, and knew they simply  wouldn't be enough if the ice was on the steeper sections.  We walked into the trailhead because the access road gate is still closed. After deciding to cross the lot and hike Beech Cliff Loop, which was much more clear of ice, and has excellent views of Echo Lake and the ocean out toward  Southwest Harbor. We returned to BH to hear of the recovery of a young couple from Rutland Massachusetts  who had fallen 100 feet to their death on Dorr Mountain Gorge Trail. The tragedy attributed to ice on the trails. Anyone not experienced with full crampon travel, and ice climbing training should never attempt to hike or climb on solid ice. The danger is severe.. \"\n\n\nNow the similar comment.\n\nshowcase['comment_text'].iloc[0]\n\n'We were staying with family in Sun City (near the Phoenix airport) and drove in our rental vehicle the approximate 3.5 hour drive to the south entrance of the Grand Canyon.  The park entrance was easy to find.  Parking this year was $35/vehicle.  I was skeptical going in, as several friends had this excursion on their \"bucket list\" while others simply raved.  I worried I would be disappointed.  However, the views absolutely spectacular!  We self-guided/toured.  We both experienced some vertigo and were careful to hang on to the railings provided, or sit on available benches as needed. Also bring water.  With the high elevation, it is easier to get winded, and water helps. We did have a hiker in front of us fall a few times from experiencing vertigo,and with assistance from others were able to help him get off the stairs and onto level ground to sit down.  He was embarrassed but grateful.  It could (and does) happen to anyone.  There were some areas that were roped off due to ice and snow and I was amazed how many people stupidly ignored the warnings and bypassed the barriers to get closer to the edge of the Canyon for selfies!   Check the weather in advance and dress appropriately.  The temperature was 30 degrees cooler in the Canyon than in the Phoenix area.  There were many families present and some pushing young ones in strollers.  On Feb 10, it was a chilly, windy, 40 degrees F.   There are lots of signs at various points educating you on the history of rocks, the Colorado river running through the Canyon, etc., and a small museum you can enter about 1.5 hours into the walk.  After our hike, we were exhausted and wind blown, and caught a shuttle back to the parking lot.  Kudos to those who can manage to walk the entire thing.  We didn\\'t see everything the south side had to offer.  In our vehicle, we exited the park from the east side and for some 50+ miles, still saw the Grand Canyon from out the driver\\'s side window. There were several spots along the way to stop and take more photos.  All in all, it was a physically and mentally stimulating journey that I highly recommend.'\n\n\nAs we can see the comments are very similar! They both talk about the dangers of the trail and how they both saw people fall.\n\n\nTotal Trail Simularity\nNow let us create a function called total_similarity which takes in the same parameters as our last function except takes in the trail name instead of comment_index. We do so because we want to get all 10 comments per trail. Our total_similarity function calls comment_similarity to get the most similar comment per each individual comment of the 10 trails. As a result, we get 10 total similar trails returned to us.\n\ndef total_similarity(trail, parks_data, all_comments):\n    trail_subset = parks_data[parks_data['trail'] == trail].index\n    total_df = []\n    for number in trail_subset:\n        total_df.append(comment_similarity(national_parks, number, all_docs))\n    df = pd.concat(total_df)\n    return(df)\n\n\noutput = total_similarity(\"Landscape Arch\", national_parks, all_docs)\noutput\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nAs we can see we get 10 similar trails to our desired trail Landscape Arch\n\n\nPlotly Function\nNow let us construct a geographical plot function called plotting_parks to get the location of these trails on a map. This is so that the user can better visualize where in the United States they may have to travel to. The function also analyzes other metrics from national_parks.csv such as visitors in 2018, type of activity, trail name, and overall TripAdvisor rating. This function calls total_similarity in order to get the dataframe with the most similar reviews!\n\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\ndef plotting_parks(trail, parks_data, all_comments, **kwargs):\n    output = total_similarity(trail, parks_data, all_comments)\n    fig = px.scatter_mapbox(output, lon = \"Longitude\", lat = \"Latitude\", color = \"overall_rating\",\n                        color_continuous_midpoint = 2.5, hover_name = \"national_park\", height = 600,\n                        hover_data = [\"Visitors (2018)\", \"activity\", \"trail\", \"overall_rating\"],\n                        title = \"Recommended National Park Trails\",\n                        size_max=50,\n                        **kwargs,\n                        )\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # produce a color map\nfig = plotting_parks(\"Landscape Arch\", national_parks, all_docs, mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\n\n\nfig.show()\n\n\n\n\nGreat, as we can see, we get a geo plot of the most similar National Park trails in the United States to Landscape Arch!"
  },
  {
    "objectID": "posts/final3/FinalProject (1).html#database_info.py",
    "href": "posts/final3/FinalProject (1).html#database_info.py",
    "title": "Trail Recommendations",
    "section": "database_info.py",
    "text": "database_info.py\nEverything relevant to managing the datbase is stored in a different python file called database_info.py. Here I can show you the structure of both databases:\n\nMaking the databases\ndef make_db(state):\n    conn = sqlite3.connect(\"trails.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    name VARCHAR(255),\n    coords VARCHAR(255),\n    Distance VARCHAR(255),\n    'Avg time' VARCHAR(255),\n    Climb VARCHAR(255),\n    Descent VARCHAR(255),\n    Activities VARCHAR(255),\n    'Riding Area' VARCHAR(255),\n    'Difficulty Rating' VARCHAR(255),\n    'Dogs Allowed' VARCHAR(255),\n    'Local Popularity' VARCHAR(255),\n    'Altitude start' VARCHAR(255),\n    'Altitude end' VARCHAR(255),\n    Grade VARCHAR(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\n    \ndef make_db_parks(state):\n    conn = sqlite3.connect(\"trails_new.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    Name VARCHAR(255),\n    Location VARCHAR(255),\n    Coords VARCHAR(255),\n    'Trails (view details)' SMALLINT(255),\n    'Total Distance' VARCHAR(255),\n    'State Ranking' VARCHAR(255),\n    'Access Road/Trail' SMALLINT(255),\n    White SMALLINT(255),\n    Green SMALLINT(255),\n    Blue SMALLINT(255),\n    Black SMALLINT(255),\n    'Double Black Diamond' SMALLINT(255),\n    Proline SMALLINT(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\nThese two functions were run in order to actually create the datbaase for the first time. They contain the variables as mentioned previously, mostly in the form of text.\n\n\nAdding information\nIf you recall from the scraping functions, there was a function call that would add information from each park to the SQL database. Here’s the source code for those functions:\ndef get_db():\n    conn = sqlite3.connect(\"trails.db\")\n    return conn\n    \ndef add_trails(df,state):\n    conn = get_db()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\n    \ndef get_db_new():\n    conn = sqlite3.connect(\"trails_new.db\")\n    return conn\n    \ndef add_trails_new(df,state):\n    conn = get_db_new()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\nThe functions get_db and get_db_new (most things relating to scraper_parks are labeled new since we did this second) establish connections to their respective databases. add_trails and add_trails_new, therefore, are actually responsible for adding entries to each database. Note that they take a df as one input (which contains the scraped info) and a state name, which sends the information to the correct table.\n\n\nMiscellaneous Tables\nThere are several dictionaries and lists that we generated in order to make the functions easier to run:\nstates = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"idaho-3166\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"new-hampshire\", \"new-jersey\", \"new-mexico\", \"new-york\", \"north-carolina\", \"north-dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"rhode-island\", \"south-carolina\", \"south-dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"west-virginia\", \"Wisconsin\", \"Wyoming\"]\n\nstate_name_code_name_dict = {\n    'Alabama': 'Alabama',\n    'Alaska': 'Alaska',\n    'Arizona': 'Arizona',\n    'Arkansas': 'Arkansas',\n    'California': 'California',\n    'Colorado': 'Colorado',\n    'Connecticut': 'Connecticut',\n    'Delaware': 'Delaware',\n    'Florida': 'Florida',\n    'Georgia': 'Georgia',\n    'Hawaii': 'Hawaii',\n    'idaho-3166': 'Idaho',\n    'Illinois': 'Illinois',\n    'Indiana': 'Indiana',\n    'Iowa': 'Iowa',\n    'Kansas': 'Kansas',\n    'Kentucky': 'Kentucky',\n    'Louisiana': 'Louisiana',\n    'Maine': 'Maine',\n    'Maryland': 'Maryland',\n    'Massachusetts': 'Massachusetts',\n    'Michigan': 'Michigan',\n    'Minnesota': 'Minnesota',\n    'Mississippi': 'Mississippi',\n    'Missouri': 'Missouri',\n    'Montana': 'Montana',\n    'Nebraska': 'Nebraska',\n    'Nevada': 'Nevada',\n    'new-hampshire': 'NewHampshire',\n    'new-jersey': 'NewJersey',\n    'new-mexico': 'NewMexico',\n    'new-york': 'NewYork',\n    'north-carolina': 'NorthCarolina',\n    'north-dakota': 'NorthDakota',\n    'Ohio': 'Ohio',\n    'Oklahoma': 'Oklahoma',\n    'Oregon': 'Oregon',\n    'Pennsylvania': 'Pennsylvania',\n    'rhode-island': 'RhodeIsland',\n    'south-carolina': 'SouthCarolina',\n    'south-dakota': 'SouthDakota',\n    'Tennessee': 'Tennessee',\n    'Texas': 'Texas',\n    'Utah': 'Utah',\n    'Vermont': 'Vermont',\n    'Virginia': 'Virginia',\n    'Washington': 'Washington',\n    'west-virginia': 'WestVirginia',\n    'Wisconsin': 'Wisconsin',\n    'Wyoming': 'Wyoming'\n}\n\n\nstate_dictionary = {'Alabama': 11, 'Alaska': 11, 'Arizona': 49, 'Arkansas': 16, 'California': 152, 'Colorado': 69, 'Connecticut': 56, 'Delaware': 4, 'Florida': 18, 'Georgia': 17, 'Hawaii': 5, 'idaho-3166': 31, 'Illinois': 51, 'Indiana': 10, 'Iowa': 8, 'Kansas': 3, 'Kentucky': 9, 'Louisiana': 2, 'Maine': 27, 'Maryland': 16, 'Massachusetts': 146, 'Michigan': 55, 'Minnesota': 36, 'Mississippi': 3, 'Missouri': 11, 'Montana': 41, 'Nebraska': 3, 'Nevada': 16, 'new-hampshire': 41, 'new-jersey': 40, 'new-mexico': 25, 'new-york': 60, 'north-carolina': 26, 'north-dakota': 7, 'Ohio': 29, 'Oklahoma': 4, 'Oregon': 38, 'Pennsylvania': 54, 'rhode-island': 9, 'south-carolina': 6, 'south-dakota': 7, 'Tennessee': 16, 'Texas': 50, 'Utah': 62, 'Vermont': 25, 'Virginia': 27, 'Washington': 92, 'west-virginia': 18, 'Wisconsin': 25, 'Wyoming': 19}\n\nstate_parks_dictionary = {'Alabama': 1, 'Alaska': 1, 'Arizona': 3, 'Arkansas': 2, 'California': 8, 'Colorado': 4, 'Connecticut': 7, 'Delaware': 1, 'Florida': 2, 'Georgia': 2, 'Hawaii': 1, 'idaho-3166': 2, 'Illinois': 10, 'Indiana': 1, 'Iowa': 1, 'Kansas': 1, 'Kentucky': 1, 'Louisiana': 1, 'Maine': 3, 'Maryland': 1, 'Massachusetts': 7, 'Michigan': 5, 'Minnesota': 3, 'Mississippi': 1, 'Missouri': 2, 'Montana': 2, 'Nebraska': 1, 'Nevada': 1, 'new-hampshire': 3, 'new-jersey': 3, 'new-mexico': 2, 'new-york': 5, 'north-carolina': 3, 'north-dakota': 1, 'Ohio': 4, 'Oklahoma': 1, 'Oregon': 3, 'Pennsylvania': 3, 'rhode-island': 1, 'south-carolina': 1, 'south-dakota': 1, 'Tennessee': 2, 'Texas': 4, 'Utah': 3, 'Vermont': 2, 'Virginia': 2, 'Washington': 6, 'west-virginia': 2, 'Wisconsin': 3, 'Wyoming': 1}\nstate_dictionary and state_parks_dictionary store the number of pages required for each state. states simply contains the names of all the states in alphabetical order, and state_name_code_dict helps sort between the name of a state and the way in which it is displayed on TrailForks URLs."
  },
  {
    "objectID": "posts/final3/FinalProject (1).html#connecting-national-parks-to-individual-trailpark-info",
    "href": "posts/final3/FinalProject (1).html#connecting-national-parks-to-individual-trailpark-info",
    "title": "Trail Recommendations",
    "section": "Connecting National Parks to Individual Trail/Park Info",
    "text": "Connecting National Parks to Individual Trail/Park Info\nNow we need to make sure to connect the data that we’ve collected here with the actual table generated by the recommender to give the user more information. Let’s take a look at our output from the similarity score model:\n\noutput\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nBecause we have two different SQL databases, one for nation-wide park data (trails_new.db) and one with state-wide trail data (trails.db), let’s split this into two different frames.\n\ncalifornia_df = output[output['state'] == 'California (CA)']\nnon_california_df = output[output['state'] != 'California (CA)']\n\nNow we’ll get our databases in our notebook:\nThere’s a bit of an issue, though. Let’s look at our table names:\n\nimport sqlite3\ndb_path = 'trails_new.db'\n\nconn = sqlite3.connect(db_path) #Establish connection with DB\ncur = conn.cursor()\n\ncur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") #This specifically grabs all table names from our datbaase.\ntables = cur.fetchall()\ntable_names = [table[0] for table in tables] #Places them into a list\nprint(\"List of tables in the database:\", table_names)\nconn.close()\n\nList of tables in the database: ['Maine', 'California', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'idaho-3166', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'new-hampshire', 'new-jersey', 'new-mexico', 'new-york', 'north-carolina', 'north-dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'rhode-island', 'south-carolina', 'south-dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'west-virginia', 'Wisconsin', 'Wyoming']\n\n\nOur tables aren’t completely in alphabetical order (I was testing around with Maine first, for instance). And some of them aren’t two words, like south-dakota, for instance. But if we compare this to what we have in output:\n\nset(output['state'])\n\n{'California (CA)',\n 'Montana (MT)',\n 'South Dakota (SD)',\n 'Utah (UT)',\n 'Washington (WA)',\n 'Wyoming (WY)'}\n\n\nHere we have nice, capitalized state names with two-letter abbreviations. So, then, how are we going to fix this? We’re going to create a dictionary that essentially works as a mapping that takes what we have in output and matches it to what exists in table_names based on some matching criteria:\n\n# Extract unique states and sort them\nunique_states_in_output = sorted(set(output['state']), key=str.lower)\ntable_names = sorted(table_names, key=str.lower)\n\n\n\ndef compare_letters(state_name, table_name):\n    clean_state_name = ''.join(filter(str.isalpha, state_name)).lower() #Eliminate non-alphabetical characters, condense together\n    clean_table_name = ''.join(filter(str.isalpha, table_name)).lower()\n    return sorted(clean_state_name) == sorted(clean_table_name) #Gives a boolean value.\n\nstate_name_to_table_name = {} #Create new dictionary\nfor state_with_abbreviation in unique_states_in_output:\n    state_name = state_with_abbreviation.split(' (')[0]  # Get rid of the parentheses in the abbreviation (like 'South Dakota (SD)')\n    match = next((table for table in table_names if compare_letters(state_name, table)), None) #Generator based on whether or not names are the same\n    if match:\n        state_name_to_table_name[state_with_abbreviation] = match #Update dict if match found\n\nprint(state_name_to_table_name)\n\n{'California (CA)': 'California', 'Montana (MT)': 'Montana', 'South Dakota (SD)': 'south-dakota', 'Utah (UT)': 'Utah', 'Washington (WA)': 'Washington', 'Wyoming (WY)': 'Wyoming'}\n\n\nNow that’s what we’re looking for! We do a few important things here:\nFirstly, we make sure to get both the states that we have in output and the tables in table_names in alphabetical order. The reason why we do key=str.lower is because some of the table names are written in uppercase while others are in lowercase. This makes it case-insensitive.\nThen we create a helper function called compare_letters which takes two state names (one from output, one from the database) and compares them to see if they have the same letters. We do this by filtering out non-alphabetical characters, spaces, and making everything lowercase and just checking if they have the same letters. The function will just return True or False depending on whether or not they match.\nWe actually use state_name_to_table_name in the for loop below this. We go through each of the states in output. Then, we extract just the part of the state name that comes before the two-letter abbreviation, and then we create a generator that individualls calls compare_letters on each of the names. If it returns True, then we have a match, which then causes the dictionary to be updated. Otherwise, nothing happens and we simply move onto the next entry (that’s why the second argument of next is none).\n##Logic for linking databases\nOur goal is to now go through each recommendation, and match up either the park or trail information corresponding to it (assuming that it’s present in the database). One issue that can arise with this, however, is that the name of the park in output might be different from that of the database. To mitigate this, we’re going to instead compare the coordinates of what’s in output to the rows inside of trails_new.db and trails.db. The idea is that if two parks are close enough to each other in terms of their coordinates, then they should represent the same thing. So, we’re going to make two functions that do similar (but different) things. One will be called fetch_park_info_based_on_coords which will look at parks (i.e., outside of California), and the other will be called fetch_trail_info_based_on_coords\n\ndef fetch_park_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor() #Connect to database\n\n    for table_name in state_name_to_table_name.values(): #This is what we made earlier\n        cursor.execute(f\"SELECT * FROM \\\"{table_name}\\\"\") #Grab everything from the table\n        rows = cursor.fetchall()\n\n        for row in rows: #For each row\n            coords_text = row[2]  # Coords are in the third column\n            try:\n                coords = eval(coords_text) #Kept as a tuple, essentially\n                lat_diff = abs(coords[0] - latitude)\n                long_diff = abs(coords[1] - longitude)\n\n                if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                    return row[3:]  # Don't need name and coords\n            except:\n                continue\n\n    conn.close()\n    return None\n\ndef fetch_trail_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n    table_name = 'California'  #Only getting CA trails\n\n    cursor.execute(f\"SELECT * FROM {table_name}\") #Grab everything\n    rows = cursor.fetchall()\n\n    for row in rows:\n        coords_text = row[1]  # Coords are in column 2\n        try:\n            coords = eval(coords_text)\n            lat_diff = abs(coords[0] - latitude)\n            long_diff = abs(coords[1] - longitude)\n\n            if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                return row[2:]\n        except:\n            continue  # Skip rows with invalid 'Coords'\n\n    conn.close()\n    return None\n\nOkay, so, it will make a lot more sense if we actually inspect the structure of our database again. Click the link below to see screenshots of two .csv files: the first of parks in Wyoming, and the second is of trails in California:\nhttps://imgur.com/a/6fCixEt\nWith that out of the way, let’s dive into the code. We go through the mapping dictionary that we made previously and we grab all of the possible parks from each one. Then, we look at the third column (i.e., row[2], which represents the third entry in the row) which corresponds to the coordinates (see screenshot), and we record the absolute difference in the coordinates between a given latitude and longitude (we’ll be taking those from output–they’re individual columns rather than a tuple). If both of them are within a specified margin of error, then we’ve found our match. Note that we’re only going to return everything starting from the third column: the first 2 are just the name and coordinates of the trail.\nFor fetch_trail_info_based_on_coords, we have a very similar set-up except for the fact that the coordinates are in the second column, and we’re interested in returning everything after the first two.\nNow, let’s move on so we can see how we actually use these functions!"
  },
  {
    "objectID": "posts/final3/FinalProject (1).html#putting-it-all-together",
    "href": "posts/final3/FinalProject (1).html#putting-it-all-together",
    "title": "Trail Recommendations",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe first thing we’re going to do is to specify the names of the new columns that we want to put into california_df and non_california_df. I’ve just grabbed these from the database:\n\nnew_columns = [\n    'Trails (view details)', 'Total Distance', 'State Ranking',\n    'Access Road/Trail', 'White', 'Green', 'Blue', 'Black',\n    'Double Black Diamond', 'Proline'\n]\nnew_trail_columns = [\n    'Distance', 'Avg time', 'Climb', 'Descent', 'Activities',\n    'Riding Area', 'Difficulty Rating', 'Dogs Allowed',\n    'Local Popularity', 'Altitude start', 'Altitude end', 'Grade'\n]\n\nNow, all we need to do is iterate through the rows of non_california_df to match up the entires!\n\nmargin_lat = 0.1  # Decently generous\nmargin_long = 0.1\nfor index, row in non_california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']): #Some parks have NA coordinates\n        continue\n    park_info = fetch_park_info_based_on_coords('trails_new (1).db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n    #Remember, this grabs almost all of the columns if a match is found\n    if park_info:\n        non_california_df.loc[index, new_columns] = park_info #We can mass-add new columns\n\nIn the above code, we use the fetch_park_info_based_on_coords function to essentially create a new data frame that contains the information that we want once we match the coordinates. Then, we insert all of these as new columns, taking advantage of the .loc() method from pandas. Now let’s do the same thing for the California df:\n\n\nfor index, row in california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']):\n        continue\n\n    park_info = fetch_trail_info_based_on_coords('trails.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n\n    if park_info and len(park_info) == len(new_trail_columns):\n        california_df.loc[index, new_trail_columns] = park_info\n    else:\n        pass\n\nOkay, let’s take a look at our results!\n\nnon_california_df\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\n...\nTrails (view details)\nTotal Distance\nState Ranking\nAccess Road/Trail\nWhite\nGreen\nBlue\nBlack\nDouble Black Diamond\nProline\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n...\n60\n194 miles\n#9,609\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n...\n26\n53 miles\n#4,761\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n...\n25\n177 miles\n#9,011\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n9 rows × 22 columns\n\n\n\nSuccess! It looks like we unfortunately have a few NA values. Unfortunately, it’s hard to guarantee precision in the coordinates. We only had one trail for California:\n\ncalifornia_df\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n\n\n\n\n\nWait, really? I thought we would’ve had this for sure in our database…\nOn closer inspection, we actually do, but hte coordinates on TrailForks versus what we got from the National Park data is a bit off. On the TrailForks page for Zabriskie Point, the coordinates are (36.420820, -116.810120), which is just outside the margin of error."
  },
  {
    "objectID": "posts/bruins/HW0- Blog Tutorial.html",
    "href": "posts/bruins/HW0- Blog Tutorial.html",
    "title": "Blog Tutorial HW0",
    "section": "",
    "text": "First read in the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nNow let’s look at the first few rows of the dataset.\n\npenguins.head()\n\nIt appears that there are a few numeric columns in our dataframe which would be easy to analyze. Let’s specifically look at two and plot them on a scatter plot.\n\n\nLet’s import the seaborn package and create a scatterplot\nFor this example, I chose ‘Culmen Length’ and ’Flipper Length). I imported the seaborn package as it has great functions for visualization. I chose to create a scatterplot because it is the best way to analyze numeric data against numeric data.\n\nimport seaborn as sns\n\nsns.scatterplot(\n    data = penguins,\n    x = \"Culmen Length (mm)\", \n    y = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nIt appears that there is a positive relationship between culmen length and flipper length.\n\n\nNow go one step further!\nLet’s go even further through analyzing if the type of species affects this relationship. To do this, we add the hue parameter. I also used the set_title function to make our plot look even better!\n\nsns.scatterplot(\n    data = penguins,\n    x = \"Culmen Length (mm)\", \n    y = \"Flipper Length (mm)\", \n    hue = \"Species\").set_title(\"Flipper Length vs Culmen Length in mm vs Species\")\n\nText(0.5, 1.0, 'Flipper Length vs Culmen Length in mm vs Species')\n\n\n\n\n\n\n\n\n\nAs we can see, species type does seem to affect the culmen length and flipper length values. For instance, there appears to be distinct groupings based upon species type, as the Gentoo Penguin appears to have the longest dimensions with the adelie penguins and chinstrap penguins having similar flipper lengths but differing culmen lengths.\n\n\nWe’re done!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Fake News?\n\n\n\n\n\n\nKeras\n\n\nTensorFlow\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nTrail Recommendations\n\n\n\n\n\n\nSpaCy\n\n\nWord Embedding\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nTrail Recommendations\n\n\n\n\n\n\nSpaCy\n\n\nWord Embedding\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nTrail Recommendations\n\n\n\n\n\n\nSpaCy\n\n\nWord Embedding\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nDogs and Cats Classification\n\n\n\n\n\n\nKeras\n\n\nTensorFlow\n\n\nPython\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nFlask Tutorial\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nSQLITE\n\n\nHTML\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping ‘The Boys’ with Scrapy\n\n\n\n\n\n\nWebscraping\n\n\nScrapy\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Geographic, Boxplot, and Lineplot Visualizations to illustrate Global Warming\n\n\n\n\n\n\nSQLITE\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Tutorial HW0\n\n\n\n\n\n\nweek 0\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nTyler Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html",
    "href": "posts/Fakenews/Fakenews.html",
    "title": "Fake News?",
    "section": "",
    "text": "In this blog post, we will construct a fake news classifer using TensorFlow."
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#title-model",
    "href": "posts/Fakenews/Fakenews.html#title-model",
    "title": "Fake News?",
    "section": "Title Model",
    "text": "Title Model\nThe first model only uses the article title. Because our second model will be an exact copy of our first model, only using the article text instead of article title, we will create a function called input_features such that we can reuse the code and streamline our process. The parameter the function takes in will be whether the input is title_input or text_input.\n\ndef input_features(feature_input):\n  new_features = text_vectorize_layer(feature_input)\n  new_features = layers.Embedding(size_vocabulary, output_dim = 4, name=\"embedding\")(new_features)\n  new_features = layers.Dropout(0.2)(new_features)\n  new_features = layers.GlobalAveragePooling1D()(new_features)\n  new_features = layers.Dropout(0.2)(new_features)\n  new_features = layers.Dense(2, activation='relu', name=\"fake\")(new_features)\n\n  model = keras.Model(\n    # only using text\n    inputs = [feature_input],\n    outputs = new_features\n  )\n  return model\n\n\n# only using title\nmodel1 = input_features(title_input)\nmodel1.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 4)              │           8,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 500, 4)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 4)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 4)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              10 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 8,010 (31.29 KB)\n\n\n\n Trainable params: 8,010 (31.29 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n\nutils.plot_model(model1)\n\n\n\n\n\n\n\n\n\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 7ms/step - accuracy: 0.5148 - loss: 0.6924 - val_accuracy: 0.5164 - val_loss: 0.6917\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - accuracy: 0.5295 - loss: 0.6908 - val_accuracy: 0.5227 - val_loss: 0.6900\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5261 - loss: 0.6897 - val_accuracy: 0.5236 - val_loss: 0.6873\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.5202 - loss: 0.6872 - val_accuracy: 0.5249 - val_loss: 0.6822\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.5546 - loss: 0.6798 - val_accuracy: 0.5713 - val_loss: 0.6645\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6439 - loss: 0.6627 - val_accuracy: 0.7667 - val_loss: 0.6440\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.6987 - loss: 0.6412 - val_accuracy: 0.7818 - val_loss: 0.6219\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.7153 - loss: 0.6207 - val_accuracy: 0.7664 - val_loss: 0.6011\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7382 - loss: 0.6026 - val_accuracy: 0.7602 - val_loss: 0.5827\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7418 - loss: 0.5840 - val_accuracy: 0.7791 - val_loss: 0.5612\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7498 - loss: 0.5664 - val_accuracy: 0.7844 - val_loss: 0.5416\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7545 - loss: 0.5494 - val_accuracy: 0.7768 - val_loss: 0.5277\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7566 - loss: 0.5406 - val_accuracy: 0.7773 - val_loss: 0.5168\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7634 - loss: 0.5235 - val_accuracy: 0.7662 - val_loss: 0.5171\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7611 - loss: 0.5181 - val_accuracy: 0.7889 - val_loss: 0.4891\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.7694 - loss: 0.5052 - val_accuracy: 0.7933 - val_loss: 0.4845\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.7798 - loss: 0.4998 - val_accuracy: 0.7933 - val_loss: 0.4780\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7750 - loss: 0.4895 - val_accuracy: 0.8073 - val_loss: 0.4682\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7781 - loss: 0.4832 - val_accuracy: 0.7827 - val_loss: 0.4692\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7829 - loss: 0.4803 - val_accuracy: 0.7900 - val_loss: 0.4582\n\n\nLet us construct a graph of the model’s training and validation accuracy. We create a function called validation_plot to streamline this process.\n\ndef validation_plot(model):\n  plt.plot(history.history[\"accuracy\"], label = \"training\")\n  plt.plot(history.history[\"val_accuracy\"], label = \"validation\")\n  plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n  plt.legend()\n  return plt\n\n\nvalidation_plot(history)\n\n\n\n\n\n\n\n\nAs we can see from our graph, the validation accuracy is around 79 and there does not appear to be overfitting, illustrating that our dropout layers are working as intended and we should not change them for our later models."
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#text-model",
    "href": "posts/Fakenews/Fakenews.html#text-model",
    "title": "Fake News?",
    "section": "Text Model",
    "text": "Text Model\nOur second model will only use the article text. Because we will reuse the same structure as the first model, we call our input_features function.\n\nmodel2 = input_features(text_input)\nmodel2.summary()\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 4)              │           8,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 4)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 4)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 4)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              10 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 8,010 (31.29 KB)\n\n\n\n Trainable params: 8,010 (31.29 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nutils.plot_model(model2)\n\n\n\n\n\n\n\n\n\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.5694 - loss: 0.6811 - val_accuracy: 0.7224 - val_loss: 0.6273\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.7645 - loss: 0.6114 - val_accuracy: 0.8229 - val_loss: 0.5489\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.8602 - loss: 0.5391 - val_accuracy: 0.9027 - val_loss: 0.4703\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9010 - loss: 0.4652 - val_accuracy: 0.9380 - val_loss: 0.4079\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9188 - loss: 0.4071 - val_accuracy: 0.9489 - val_loss: 0.3563\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9266 - loss: 0.3624 - val_accuracy: 0.9520 - val_loss: 0.3066\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9358 - loss: 0.3234 - val_accuracy: 0.9556 - val_loss: 0.2873\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9337 - loss: 0.3002 - val_accuracy: 0.9536 - val_loss: 0.2597\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9413 - loss: 0.2743 - val_accuracy: 0.9567 - val_loss: 0.2410\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9417 - loss: 0.2556 - val_accuracy: 0.9616 - val_loss: 0.2179\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9469 - loss: 0.2392 - val_accuracy: 0.9548 - val_loss: 0.2142\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9488 - loss: 0.2286 - val_accuracy: 0.9629 - val_loss: 0.1982\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9476 - loss: 0.2167 - val_accuracy: 0.9551 - val_loss: 0.1951\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9511 - loss: 0.2102 - val_accuracy: 0.9620 - val_loss: 0.1768\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9524 - loss: 0.2006 - val_accuracy: 0.9638 - val_loss: 0.1678\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9565 - loss: 0.1884 - val_accuracy: 0.9653 - val_loss: 0.1650\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 35ms/step - accuracy: 0.9508 - loss: 0.1893 - val_accuracy: 0.9653 - val_loss: 0.1554\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9545 - loss: 0.1803 - val_accuracy: 0.9691 - val_loss: 0.1432\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9597 - loss: 0.1720 - val_accuracy: 0.9634 - val_loss: 0.1496\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9614 - loss: 0.1617 - val_accuracy: 0.9651 - val_loss: 0.1467\n\n\n\nvalidation_plot(history)\n\n\n\n\n\n\n\n\nAs we can see, our our validation accuracy is much better than our first model, sitting at around 96 percent. This is expected as text reveals a lot more than just the title because there are more words to analyze. Let us see if we can improve this model further through taking into account both text and title variables."
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#text-and-title-model",
    "href": "posts/Fakenews/Fakenews.html#text-and-title-model",
    "title": "Fake News?",
    "section": "Text and Title Model",
    "text": "Text and Title Model\nOur third model will use both the article text and title. Because we don’t want to share an embedding layer, we must make one for both title and text.\n\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\n\n# Because we do not want to share an embedding layer, we make one for title and text\ntitle_embedding = layers.Embedding(size_vocabulary, output_dim = 4)\ntext_embedding = layers.Embedding(size_vocabulary, output_dim = 4)\ntitle_features = title_embedding(title_features)\ntext_features = text_embedding(text_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n#cocatenate the two features together\ntogether = layers.concatenate([title_features, text_features], axis = 1)\ntogether = layers.Dropout(0.2)(together)\ntogether = layers.GlobalAveragePooling1D()(together)\ntogether = layers.Dropout(0.2)(together)\ntogether = layers.Dense(2, activation='relu', name = 'fake')(together)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = together\n)\n\nmodel3.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0]            │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ text[0][0]             │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 4)         │          8,000 │ text_vectorization[0]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (Embedding)   │ (None, 500, 4)         │          8,000 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (Dense)             │ (None, 500, 32)        │            160 │ embedding[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (Dense)           │ (None, 500, 32)        │            160 │ embedding_1[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 1000, 32)       │              0 │ dense[0][0],           │\n│                           │                        │                │ dense_1[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (Dropout)       │ (None, 1000, 32)       │              0 │ concatenate[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 32)             │              0 │ dropout_4[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (Dropout)       │ (None, 32)             │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 2)              │             66 │ dropout_5[0][0]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 16,386 (64.01 KB)\n\n\n\n Trainable params: 16,386 (64.01 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nkeras.utils.plot_model(model3)\n\n\n\n\n\n\n\n\n\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 36ms/step - accuracy: 0.5355 - loss: 0.6872 - val_accuracy: 0.6209 - val_loss: 0.6489\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.6459 - loss: 0.6257 - val_accuracy: 0.7578 - val_loss: 0.5518\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.7702 - loss: 0.5481 - val_accuracy: 0.8231 - val_loss: 0.4878\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.8349 - loss: 0.4935 - val_accuracy: 0.8611 - val_loss: 0.4487\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.8684 - loss: 0.4598 - val_accuracy: 0.9138 - val_loss: 0.4278\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.8981 - loss: 0.4384 - val_accuracy: 0.9367 - val_loss: 0.4080\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9174 - loss: 0.4223 - val_accuracy: 0.9391 - val_loss: 0.4061\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9238 - loss: 0.4122 - val_accuracy: 0.9511 - val_loss: 0.3966\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9313 - loss: 0.4066 - val_accuracy: 0.9480 - val_loss: 0.3873\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9379 - loss: 0.3980 - val_accuracy: 0.9484 - val_loss: 0.3827\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9417 - loss: 0.3957 - val_accuracy: 0.9496 - val_loss: 0.3776\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9456 - loss: 0.3849 - val_accuracy: 0.9622 - val_loss: 0.3814\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9498 - loss: 0.3868 - val_accuracy: 0.9629 - val_loss: 0.3722\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 18ms/step - accuracy: 0.9545 - loss: 0.3797 - val_accuracy: 0.9660 - val_loss: 0.3797\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9553 - loss: 0.3782 - val_accuracy: 0.9747 - val_loss: 0.3596\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9581 - loss: 0.3733 - val_accuracy: 0.9733 - val_loss: 0.3663\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9590 - loss: 0.3739 - val_accuracy: 0.9727 - val_loss: 0.3602\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 17ms/step - accuracy: 0.9648 - loss: 0.3644 - val_accuracy: 0.9737 - val_loss: 0.3554\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 19ms/step - accuracy: 0.9625 - loss: 0.3728 - val_accuracy: 0.9736 - val_loss: 0.3633\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9663 - loss: 0.3629 - val_accuracy: 0.9782 - val_loss: 0.3550\n\n\nAgain, let us look at our validation accuracy by calling validation_plot.\n\nvalidation_plot(history)\n\n\n\n\n\n\n\n\nAs we can see, our validation accuracy is very good! With an accuracy of 97.82 percent, the classification of fake news is nearly perfect!"
  },
  {
    "objectID": "posts/Fakenews/Fakenews.html#model-evaluation-on-testing-data",
    "href": "posts/Fakenews/Fakenews.html#model-evaluation-on-testing-data",
    "title": "Fake News?",
    "section": "4. Model Evaluation on Testing Data",
    "text": "4. Model Evaluation on Testing Data\nNow we test our best model on unseen data.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest = make_dataset(test_df)\n\n\nmodel3.evaluate(test)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9722 - loss: 0.3648\n\n\n[0.3625478148460388, 0.9716691374778748]\n\n\nOur final accuracy is 97.17 percent which is very very good!"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html",
    "href": "posts/final3 - Copy/FinalProject (1).html",
    "title": "Trail Recommendations",
    "section": "",
    "text": "Using the internet, it’s very easy to find information about hiking trails throughout the United States, but there’s a problem: it’s a lot easier to browse through lists of trails available online, like on the website trailforks.com, and filtering by location. What are you supposed to do if you don’t know much about an area but still want to go hiking, or what if you want to do certain things on your hike but don’t know where to go? You could do lots of research online yourself, reading articles or sifting through different locations, but this process can be very difficult, especially if you want to go somewhere that you’ve never been.\nOur project addresses finding new places to visit based upon what type of hiking trails the user wants to see (which the user would describe) based upon previously hiked trails. For instance, if the user previously hiked Half Dome in Yosemite, then we give the user other trails and locations in the United States most similar to Half Dome according to reviews on TripAdvisor. This would be mainly for tourists who want to visit parts of the country they have not been to.\nThe below code is for importing images.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#load-data",
    "href": "posts/final3 - Copy/FinalProject (1).html#load-data",
    "title": "Trail Recommendations",
    "section": "Load Data",
    "text": "Load Data\nNow let us load the data we scraped by TripAdvisor as well as a Excel file containing coordinate points of our national parks so that we can create a geograpical plot later\n\ndf = pd.read_csv('https://raw.githubusercontent.com/torwar02/trails/main/trails/national_parks.csv')\n\n\ndf2 = pd.read_excel('https://raw.githubusercontent.com/torwar02/trails/main/trails/coords.xlsx')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nPark\nState(s)\nPark Established\nArea\nVisitors (2018)\n\n\n\n\n0\n44.35\n-68.21\nAcadia\nMaine\nFebruary 26, 1919\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\n-14.25\n-170.68\nAmerican Samoa\nAmerican Samoa\nOctober 31, 1988\n8,256.67 acres (33.4 km2)\n28626\n\n\n2\n38.68\n-109.57\nArches\nUtah\nNovember 12, 1971\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3\n43.75\n-102.50\nBadlands\nSouth Dakota\nNovember 10, 1978\n242,755.94 acres (982.4 km2)\n1008942\n\n\n4\n29.25\n-103.25\nBig Bend\nTexas\nJune 12, 1944\n801,163.21 acres (3,242.2 km2)\n440091\n\n\n\n\n\n\n\nTo merge the two files together, we utilize regex. Get string preceding ‘National Park’ in df such that we can merge with df2 on National Park name\n\nimport re\npattern = r'(.*?)(?:\\s+National Park)?$'\nresult = re.findall(pattern, df['national_park'].iloc[0])\npark = []\nfor row in df['national_park']:\n    test_park = re.findall(pattern, row)\n    park.append(test_park[0])\ndf['park'] = park\nnational_parks = pd.merge(df, df2, left_on='park', right_on='Park')\nnational_parks = national_parks.drop(columns = ['park', 'Park', 'State(s)', 'Park Established'])\nnational_parks.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#word-embedding-and-comment-similarity-score",
    "href": "posts/final3 - Copy/FinalProject (1).html#word-embedding-and-comment-similarity-score",
    "title": "Trail Recommendations",
    "section": "Word Embedding and Comment Similarity Score",
    "text": "Word Embedding and Comment Similarity Score\nFirst let us go over what Word Embedding is. Word embedding in NLP is an important technique that is used for representing words for text analysis in the form of real-valued vectors. In this approach, words and documents are represented in the form of numeric vectors allowing similar words to have similar vector representations. The extracted features are fed into a machine learning model so as to work with text data and preserve the semantic and syntactic information. This information once received in its converted form is used by NLP algorithms that easily digest these learned representations and process textual information.\n\nComment Similarity Function\nNow let us create a function called comment_similarity which takes in our national_parks.csv file we just created via the park_data parameter, a comment_index parameter, and an all_comments parameter which is our word embedding vector representation of all comments in our csv file.\n\nall_docs = [nlp(row) for row in national_parks['comment_text']] #getting vector representation of all comments in our csv file\n\n\ndef comment_similarity(parks_data, comment_index, all_comments):\n    example_comment = parks_data.loc[comment_index, 'comment_text']\n    reference_comment = nlp(example_comment) #vectorize our reference sentence\n    simularity_score = []\n    row_id = []\n    for i in range(len(all_comments)):\n        sim_score = all_comments[i].similarity(reference_comment)\n        simularity_score.append(sim_score)\n        row_id.append(i)\n    simularity_docs = pd.DataFrame(list(zip(row_id, simularity_score)), columns = ['Comment_ID', 'sims'])\n    simularity_docs_sorted = simularity_docs.sort_values(by = 'sims', ascending = False)\n    most_similar_comments = simularity_docs_sorted['Comment_ID'][1:2]\n    new_reviews = national_parks.iloc[most_similar_comments.values]\n    return(new_reviews)\n\nNow let us show what our returned dataframe looks like\n\nshowcase = comment_similarity(national_parks, 0, all_docs)\nshowcase\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1552\nGrand Canyon National Park\nArizona (AZ)\nGrand Canyon South Rim\nCanyons\n5.0\nThe views do not disappoint!\n5.0 of 5 bubbles\nWe were staying with family in Sun City (near ...\n36.06\n-112.14\n1,201,647.03 acres (4,862.9 km2)\n6380495\n\n\n\n\n\n\n\nAs we can see, we return a dataframe of the most similar review to the review with the index 999. To see how similar this similar review is to our inputted review let us output both comments.\nFirst the original comment\n\nexample_comment = national_parks.loc[0, 'comment_text']\nexample_comment\n\n\"I have hiked to the fire tower a few times. Its a great hike, and not too strenuous elevation gains.  If the NO rangers are up there ( in the summer) they used to allow you to go up the tower. We had to turn back on 3/20 because of hard pack solid ice. We had our Katoohla micro spikes on, and solid hiking poles, and knew they simply  wouldn't be enough if the ice was on the steeper sections.  We walked into the trailhead because the access road gate is still closed. After deciding to cross the lot and hike Beech Cliff Loop, which was much more clear of ice, and has excellent views of Echo Lake and the ocean out toward  Southwest Harbor. We returned to BH to hear of the recovery of a young couple from Rutland Massachusetts  who had fallen 100 feet to their death on Dorr Mountain Gorge Trail. The tragedy attributed to ice on the trails. Anyone not experienced with full crampon travel, and ice climbing training should never attempt to hike or climb on solid ice. The danger is severe.. \"\n\n\nNow the similar comment.\n\nshowcase['comment_text'].iloc[0]\n\n'We were staying with family in Sun City (near the Phoenix airport) and drove in our rental vehicle the approximate 3.5 hour drive to the south entrance of the Grand Canyon.  The park entrance was easy to find.  Parking this year was $35/vehicle.  I was skeptical going in, as several friends had this excursion on their \"bucket list\" while others simply raved.  I worried I would be disappointed.  However, the views absolutely spectacular!  We self-guided/toured.  We both experienced some vertigo and were careful to hang on to the railings provided, or sit on available benches as needed. Also bring water.  With the high elevation, it is easier to get winded, and water helps. We did have a hiker in front of us fall a few times from experiencing vertigo,and with assistance from others were able to help him get off the stairs and onto level ground to sit down.  He was embarrassed but grateful.  It could (and does) happen to anyone.  There were some areas that were roped off due to ice and snow and I was amazed how many people stupidly ignored the warnings and bypassed the barriers to get closer to the edge of the Canyon for selfies!   Check the weather in advance and dress appropriately.  The temperature was 30 degrees cooler in the Canyon than in the Phoenix area.  There were many families present and some pushing young ones in strollers.  On Feb 10, it was a chilly, windy, 40 degrees F.   There are lots of signs at various points educating you on the history of rocks, the Colorado river running through the Canyon, etc., and a small museum you can enter about 1.5 hours into the walk.  After our hike, we were exhausted and wind blown, and caught a shuttle back to the parking lot.  Kudos to those who can manage to walk the entire thing.  We didn\\'t see everything the south side had to offer.  In our vehicle, we exited the park from the east side and for some 50+ miles, still saw the Grand Canyon from out the driver\\'s side window. There were several spots along the way to stop and take more photos.  All in all, it was a physically and mentally stimulating journey that I highly recommend.'\n\n\nAs we can see the comments are very similar! They both talk about the dangers of the trail and how they both saw people fall.\n\n\nTotal Trail Simularity\nNow let us create a function called total_similarity which takes in the same parameters as our last function except takes in the trail name instead of comment_index. We do so because we want to get all 10 comments per trail. Our total_similarity function calls comment_similarity to get the most similar comment per each individual comment of the 10 trails. As a result, we get 10 total similar trails returned to us.\n\ndef total_similarity(trail, parks_data, all_comments):\n    trail_subset = parks_data[parks_data['trail'] == trail].index\n    total_df = []\n    for number in trail_subset:\n        total_df.append(comment_similarity(national_parks, number, all_docs))\n    df = pd.concat(total_df)\n    return(df)\n\n\noutput = total_similarity(\"Landscape Arch\", national_parks, all_docs)\noutput\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nAs we can see we get 10 similar trails to our desired trail Landscape Arch\n\n\nPlotly Function\nNow let us construct a geographical plot function called plotting_parks to get the location of these trails on a map. This is so that the user can better visualize where in the United States they may have to travel to. The function also analyzes other metrics from national_parks.csv such as visitors in 2018, type of activity, trail name, and overall TripAdvisor rating. This function calls total_similarity in order to get the dataframe with the most similar reviews!\n\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\ndef plotting_parks(trail, parks_data, all_comments, **kwargs):\n    output = total_similarity(trail, parks_data, all_comments)\n    fig = px.scatter_mapbox(output, lon = \"Longitude\", lat = \"Latitude\", color = \"overall_rating\",\n                        color_continuous_midpoint = 2.5, hover_name = \"national_park\", height = 600,\n                        hover_data = [\"Visitors (2018)\", \"activity\", \"trail\", \"overall_rating\"],\n                        title = \"Recommended National Park Trails\",\n                        size_max=50,\n                        **kwargs,\n                        )\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # produce a color map\nfig = plotting_parks(\"Landscape Arch\", national_parks, all_docs, mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\n\n\nfig.show()\n\n\n\n\nGreat, as we can see, we get a geo plot of the most similar National Park trails in the United States to Landscape Arch!"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#how-do-you-get-started-with-selenium",
    "href": "posts/final3 - Copy/FinalProject (1).html#how-do-you-get-started-with-selenium",
    "title": "Trail Recommendations",
    "section": "How do you get started with Selenium?",
    "text": "How do you get started with Selenium?\nSelenium is able to evade certain anti-bot measures by actually using an instance of a web browser (called a webdriver) that runs on your system while scraping. In fact, once you get the scraper to work, you can actually watch it run in real time! Unfortunately, that makes it a lot slower than scrapy, for instance, because your computer actually has to manually open every page. I used a Google Chrome webdriver. Below is (part of) the head of the scraper.py function which scrapes data from individual trails from TrailForks.\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nimport pandas as pd\nfrom selenium import webdriver\n\n\nchrome_options = webdriver.ChromeOptions()\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\n        \"prefs\", {\n            # block image loading\n            \"profile.managed_default_content_settings.images\": 2,\n            \"profile.managed_default_content_settings.javascript\": 2\n        }\n    )\ndriver = webdriver.Chrome(\n        service=service,\n        options=options\n    )\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--headless')\nOf note are the experimental options under prefs which block both images and javascript content from loading on a website. When I first made the scraper, I did not have these enabled, and a result, sometimes pages would take between 3 and 5 seconds to load (way too long!). Similarly, the --headless argument also make the pages load faster by disabling certain Google Chrome functionalities: https://www.selenium.dev/blog/2023/headless-is-going-away/"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#how-do-you-scrape-using-selenium",
    "href": "posts/final3 - Copy/FinalProject (1).html#how-do-you-scrape-using-selenium",
    "title": "Trail Recommendations",
    "section": "How do you scrape using Selenium?",
    "text": "How do you scrape using Selenium?\nAt its core, Selenium isn’t that different from scrapy in that you can have a scraper download HTML code which you can then filter through in Python as more familiar objects. Both scraper.py and scraper_parks.py (which filters through park-related information on TrailForks) have the same general principles: 1. Look at what state a user has inputted 2. Get links to all of the park/trail pages for that state 3. Get corresponding information from each page, using helper functions if need be 4. Add data to SQL database (see next section for that!)"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#names-and-coordinates",
    "href": "posts/final3 - Copy/FinalProject (1).html#names-and-coordinates",
    "title": "Trail Recommendations",
    "section": "Names and coordinates",
    "text": "Names and coordinates\nHere’s the first part of where we actually scrape:\n\n for url in href_list:\n            no_name_found = False\n            info_dict = {\"Name\":[\"NA\"], \"Location\":[\"NA\"], \"Coords\":[\"NA\"]}\n            stats_dict =  {\"Trails (view details)\":[\"NA\"],\"Total Distance\":[\"NA\"], \"State Ranking\":[\"NA\"],}\n            trail_difficulty_count = {\"Access Road/Trail\":0,\"White\":0,\"Green\":0,\"Blue\":0,\"Black\":0,\"Double Black Diamond\":0, \"Proline\":0}\n            print(url)\n            driver.get(url)\n            area_name_raw = driver.find_element(\"xpath\", \"//span[contains(@class, 'translate')][1]\")\n            info_dict[\"Name\"] = area_name_raw.text\n            try:\n                city_name_raw = driver.find_element(By.CLASS_NAME, \"small.grey2.mobile_hide\")\n                info_dict[\"Location\"] = city_name_raw.text\n            except:\n                no_name_found = True\n                \nWe create our three dictionaries that we want for each URL. The print(url) function is present as a debugging tool since, unfortunately, this scraper crashed multiple times due to unfixed bugs (which I eventually patched out, mostly due to elements not being present).\nWe get the name of the trail by finding a span with class translate (not sure why it’s stored like that, it’s actually within an h1 within a ul called page_title_container). Then, we try to look for the name of the city that it’s in by grabbing a small piece of text that’s next to the park’s name. Sometimes, this isn’t present, which is why we have a bool called no_name_found in case it’s not. There’s a way around this, though, which we’ll show later…\n###Ranking, Distance, and Trail numbers:\n    stats_items = [\"State Ranking\", \"Total Distance\", \"Trails (view details)\"]\n            dict_category = driver.find_elements(\"xpath\", \"//dl//dt\")\n            dict_information = driver.find_elements(\"xpath\", \"//dl//dd\")\n           \n            for idx, terms in enumerate(dict_category):\n                if terms.text in stats_items:\n                    stats_dict[terms.text] = [dict_information[idx].text]\n                    \n            try:\n                difficulty_ul = driver.find_element(By.CLASS_NAME, 'stats.flex.nostyle.inline.clearfix')\n\n                for li in difficulty_ul.find_elements(By.TAG_NAME, 'li'):\n                    difficulty_span = li.find_element(By.XPATH, './/span[contains(@class, \"stat-label clickable\")]/span')\n                    difficulty_name = difficulty_span.get_attribute('title')\n                    if difficulty_name in trail_difficulty_count.keys():\n                        num_trails_span = li.find_element(By.CLASS_NAME, 'stat-num')\n                        num_trails = int(num_trails_span.text)\n                        trail_difficulty_count[difficulty_name] = num_trails\nThe code here is somewhat dense thanks to the fact that all of this information is stored in a dictionary-like object called a dl which, in turn, has something like a key in a dl and something like a value in a dd. Essentially, we update the ranking and trail distances by inspecting these.\nIt’s a little bit harder to get the number of trails per difficulty. Basically, there’s an unordered list with a long class name ('stats.flex.nostyle.inline.clearfix' that sorts the number of trails by difficulty. Each li has the number of trails stored within it, but it also has a graphic that represents the difficulty (it’s a small picture), and it’s the graphic that actually hides the name of the difficulty, which is why we have to extract difficulty_name from a span of class stat-label clickable. Then, we simply grab the actual text that displays how many trails of a given difficulty there are, convert it to an integer, and then add it to our dictionary.\n\nCoordinates\nOne of the unfortunate parts of the parks list is that the coordinates of each park are not present! To get around this, we tell the scraper to go to the first trail in each park and grab its coordinates (remember scraper.py?) and then store it.\n  try:\n                green_link = driver.find_element(\"xpath\",\"//tr//a[contains(@class, 'green')]\")\n                park_link = green_link.get_attribute(\"href\")\n                driver.get(park_link)\n            except:\n                pass\n                \n            try:\n                coord_raw = driver.find_element(\"xpath\", \"//div[contains(@class, 'margin-bottom-15 grey')]/span[contains(@class, 'grey2')][2]\") #Get coords\n                info_dict['Coords'] = [coord_raw.text]\n                if no_name_found:\n                    city_name_raw = driver.find_element(By.CLASS_NAME, \"weather_date bold green\")\n                    info_dict[\"Location\"] = city_name_raw.text\n            except:\n                info_dict['Coords'] = [\"NA\"]\nIt\nFor reference here is how our scraper worked! ’s here where we also resolve the issue of when we can’t find a city’s name. Basically, on each trail’s page, there’s a short infobox containing weather information for the nearest city which is guaranteed to appear, so we can get an approximate location name precisely by grabbing the city name from this box.\nOnce we’re done with that, it’s off to the database again!\n\n\n\nCapture.JPG"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#database_info.py",
    "href": "posts/final3 - Copy/FinalProject (1).html#database_info.py",
    "title": "Trail Recommendations",
    "section": "database_info.py",
    "text": "database_info.py\nEverything relevant to managing the datbase is stored in a different python file called database_info.py. Here I can show you the structure of both databases:\n\nMaking the databases\ndef make_db(state):\n    conn = sqlite3.connect(\"trails.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    name VARCHAR(255),\n    coords VARCHAR(255),\n    Distance VARCHAR(255),\n    'Avg time' VARCHAR(255),\n    Climb VARCHAR(255),\n    Descent VARCHAR(255),\n    Activities VARCHAR(255),\n    'Riding Area' VARCHAR(255),\n    'Difficulty Rating' VARCHAR(255),\n    'Dogs Allowed' VARCHAR(255),\n    'Local Popularity' VARCHAR(255),\n    'Altitude start' VARCHAR(255),\n    'Altitude end' VARCHAR(255),\n    Grade VARCHAR(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\n    \ndef make_db_parks(state):\n    conn = sqlite3.connect(\"trails_new.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    Name VARCHAR(255),\n    Location VARCHAR(255),\n    Coords VARCHAR(255),\n    'Trails (view details)' SMALLINT(255),\n    'Total Distance' VARCHAR(255),\n    'State Ranking' VARCHAR(255),\n    'Access Road/Trail' SMALLINT(255),\n    White SMALLINT(255),\n    Green SMALLINT(255),\n    Blue SMALLINT(255),\n    Black SMALLINT(255),\n    'Double Black Diamond' SMALLINT(255),\n    Proline SMALLINT(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\nThese two functions were run in order to actually create the datbaase for the first time. They contain the variables as mentioned previously, mostly in the form of text.\n\n\nAdding information\nIf you recall from the scraping functions, there was a function call that would add information from each park to the SQL database. Here’s the source code for those functions:\ndef get_db():\n    conn = sqlite3.connect(\"trails.db\")\n    return conn\n    \ndef add_trails(df,state):\n    conn = get_db()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\n    \ndef get_db_new():\n    conn = sqlite3.connect(\"trails_new.db\")\n    return conn\n    \ndef add_trails_new(df,state):\n    conn = get_db_new()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\nThe functions get_db and get_db_new (most things relating to scraper_parks are labeled new since we did this second) establish connections to their respective databases. add_trails and add_trails_new, therefore, are actually responsible for adding entries to each database. Note that they take a df as one input (which contains the scraped info) and a state name, which sends the information to the correct table.\n\n\nMiscellaneous Tables\nThere are several dictionaries and lists that we generated in order to make the functions easier to run:\nstates = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"idaho-3166\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"new-hampshire\", \"new-jersey\", \"new-mexico\", \"new-york\", \"north-carolina\", \"north-dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"rhode-island\", \"south-carolina\", \"south-dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"west-virginia\", \"Wisconsin\", \"Wyoming\"]\n\nstate_name_code_name_dict = {\n    'Alabama': 'Alabama',\n    'Alaska': 'Alaska',\n    'Arizona': 'Arizona',\n    'Arkansas': 'Arkansas',\n    'California': 'California',\n    'Colorado': 'Colorado',\n    'Connecticut': 'Connecticut',\n    'Delaware': 'Delaware',\n    'Florida': 'Florida',\n    'Georgia': 'Georgia',\n    'Hawaii': 'Hawaii',\n    'idaho-3166': 'Idaho',\n    'Illinois': 'Illinois',\n    'Indiana': 'Indiana',\n    'Iowa': 'Iowa',\n    'Kansas': 'Kansas',\n    'Kentucky': 'Kentucky',\n    'Louisiana': 'Louisiana',\n    'Maine': 'Maine',\n    'Maryland': 'Maryland',\n    'Massachusetts': 'Massachusetts',\n    'Michigan': 'Michigan',\n    'Minnesota': 'Minnesota',\n    'Mississippi': 'Mississippi',\n    'Missouri': 'Missouri',\n    'Montana': 'Montana',\n    'Nebraska': 'Nebraska',\n    'Nevada': 'Nevada',\n    'new-hampshire': 'NewHampshire',\n    'new-jersey': 'NewJersey',\n    'new-mexico': 'NewMexico',\n    'new-york': 'NewYork',\n    'north-carolina': 'NorthCarolina',\n    'north-dakota': 'NorthDakota',\n    'Ohio': 'Ohio',\n    'Oklahoma': 'Oklahoma',\n    'Oregon': 'Oregon',\n    'Pennsylvania': 'Pennsylvania',\n    'rhode-island': 'RhodeIsland',\n    'south-carolina': 'SouthCarolina',\n    'south-dakota': 'SouthDakota',\n    'Tennessee': 'Tennessee',\n    'Texas': 'Texas',\n    'Utah': 'Utah',\n    'Vermont': 'Vermont',\n    'Virginia': 'Virginia',\n    'Washington': 'Washington',\n    'west-virginia': 'WestVirginia',\n    'Wisconsin': 'Wisconsin',\n    'Wyoming': 'Wyoming'\n}\n\n\nstate_dictionary = {'Alabama': 11, 'Alaska': 11, 'Arizona': 49, 'Arkansas': 16, 'California': 152, 'Colorado': 69, 'Connecticut': 56, 'Delaware': 4, 'Florida': 18, 'Georgia': 17, 'Hawaii': 5, 'idaho-3166': 31, 'Illinois': 51, 'Indiana': 10, 'Iowa': 8, 'Kansas': 3, 'Kentucky': 9, 'Louisiana': 2, 'Maine': 27, 'Maryland': 16, 'Massachusetts': 146, 'Michigan': 55, 'Minnesota': 36, 'Mississippi': 3, 'Missouri': 11, 'Montana': 41, 'Nebraska': 3, 'Nevada': 16, 'new-hampshire': 41, 'new-jersey': 40, 'new-mexico': 25, 'new-york': 60, 'north-carolina': 26, 'north-dakota': 7, 'Ohio': 29, 'Oklahoma': 4, 'Oregon': 38, 'Pennsylvania': 54, 'rhode-island': 9, 'south-carolina': 6, 'south-dakota': 7, 'Tennessee': 16, 'Texas': 50, 'Utah': 62, 'Vermont': 25, 'Virginia': 27, 'Washington': 92, 'west-virginia': 18, 'Wisconsin': 25, 'Wyoming': 19}\n\nstate_parks_dictionary = {'Alabama': 1, 'Alaska': 1, 'Arizona': 3, 'Arkansas': 2, 'California': 8, 'Colorado': 4, 'Connecticut': 7, 'Delaware': 1, 'Florida': 2, 'Georgia': 2, 'Hawaii': 1, 'idaho-3166': 2, 'Illinois': 10, 'Indiana': 1, 'Iowa': 1, 'Kansas': 1, 'Kentucky': 1, 'Louisiana': 1, 'Maine': 3, 'Maryland': 1, 'Massachusetts': 7, 'Michigan': 5, 'Minnesota': 3, 'Mississippi': 1, 'Missouri': 2, 'Montana': 2, 'Nebraska': 1, 'Nevada': 1, 'new-hampshire': 3, 'new-jersey': 3, 'new-mexico': 2, 'new-york': 5, 'north-carolina': 3, 'north-dakota': 1, 'Ohio': 4, 'Oklahoma': 1, 'Oregon': 3, 'Pennsylvania': 3, 'rhode-island': 1, 'south-carolina': 1, 'south-dakota': 1, 'Tennessee': 2, 'Texas': 4, 'Utah': 3, 'Vermont': 2, 'Virginia': 2, 'Washington': 6, 'west-virginia': 2, 'Wisconsin': 3, 'Wyoming': 1}\nstate_dictionary and state_parks_dictionary store the number of pages required for each state. states simply contains the names of all the states in alphabetical order, and state_name_code_dict helps sort between the name of a state and the way in which it is displayed on TrailForks URLs."
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#connecting-national-parks-to-individual-trailpark-info",
    "href": "posts/final3 - Copy/FinalProject (1).html#connecting-national-parks-to-individual-trailpark-info",
    "title": "Trail Recommendations",
    "section": "Connecting National Parks to Individual Trail/Park Info",
    "text": "Connecting National Parks to Individual Trail/Park Info\nNow we need to make sure to connect the data that we’ve collected here with the actual table generated by the recommender to give the user more information. Let’s take a look at our output from the similarity score model:\n\noutput\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nBecause we have two different SQL databases, one for nation-wide park data (trails_new.db) and one with state-wide trail data (trails.db), let’s split this into two different frames.\n\ncalifornia_df = output[output['state'] == 'California (CA)']\nnon_california_df = output[output['state'] != 'California (CA)']\n\nNow we’ll get our databases in our notebook:\nThere’s a bit of an issue, though. Let’s look at our table names:\n\nimport sqlite3\ndb_path = 'trails_new.db'\n\nconn = sqlite3.connect(db_path) #Establish connection with DB\ncur = conn.cursor()\n\ncur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") #This specifically grabs all table names from our datbaase.\ntables = cur.fetchall()\ntable_names = [table[0] for table in tables] #Places them into a list\nprint(\"List of tables in the database:\", table_names)\nconn.close()\n\nList of tables in the database: ['Maine', 'California', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'idaho-3166', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'new-hampshire', 'new-jersey', 'new-mexico', 'new-york', 'north-carolina', 'north-dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'rhode-island', 'south-carolina', 'south-dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'west-virginia', 'Wisconsin', 'Wyoming']\n\n\nOur tables aren’t completely in alphabetical order (I was testing around with Maine first, for instance). And some of them aren’t two words, like south-dakota, for instance. But if we compare this to what we have in output:\n\nset(output['state'])\n\n{'California (CA)',\n 'Montana (MT)',\n 'South Dakota (SD)',\n 'Utah (UT)',\n 'Washington (WA)',\n 'Wyoming (WY)'}\n\n\nHere we have nice, capitalized state names with two-letter abbreviations. So, then, how are we going to fix this? We’re going to create a dictionary that essentially works as a mapping that takes what we have in output and matches it to what exists in table_names based on some matching criteria:\n\n# Extract unique states and sort them\nunique_states_in_output = sorted(set(output['state']), key=str.lower)\ntable_names = sorted(table_names, key=str.lower)\n\n\n\ndef compare_letters(state_name, table_name):\n    clean_state_name = ''.join(filter(str.isalpha, state_name)).lower() #Eliminate non-alphabetical characters, condense together\n    clean_table_name = ''.join(filter(str.isalpha, table_name)).lower()\n    return sorted(clean_state_name) == sorted(clean_table_name) #Gives a boolean value.\n\nstate_name_to_table_name = {} #Create new dictionary\nfor state_with_abbreviation in unique_states_in_output:\n    state_name = state_with_abbreviation.split(' (')[0]  # Get rid of the parentheses in the abbreviation (like 'South Dakota (SD)')\n    match = next((table for table in table_names if compare_letters(state_name, table)), None) #Generator based on whether or not names are the same\n    if match:\n        state_name_to_table_name[state_with_abbreviation] = match #Update dict if match found\n\nprint(state_name_to_table_name)\n\n{'California (CA)': 'California', 'Montana (MT)': 'Montana', 'South Dakota (SD)': 'south-dakota', 'Utah (UT)': 'Utah', 'Washington (WA)': 'Washington', 'Wyoming (WY)': 'Wyoming'}\n\n\nNow that’s what we’re looking for! We do a few important things here:\nFirstly, we make sure to get both the states that we have in output and the tables in table_names in alphabetical order. The reason why we do key=str.lower is because some of the table names are written in uppercase while others are in lowercase. This makes it case-insensitive.\nThen we create a helper function called compare_letters which takes two state names (one from output, one from the database) and compares them to see if they have the same letters. We do this by filtering out non-alphabetical characters, spaces, and making everything lowercase and just checking if they have the same letters. The function will just return True or False depending on whether or not they match.\nWe actually use state_name_to_table_name in the for loop below this. We go through each of the states in output. Then, we extract just the part of the state name that comes before the two-letter abbreviation, and then we create a generator that individualls calls compare_letters on each of the names. If it returns True, then we have a match, which then causes the dictionary to be updated. Otherwise, nothing happens and we simply move onto the next entry (that’s why the second argument of next is none).\n##Logic for linking databases\nOur goal is to now go through each recommendation, and match up either the park or trail information corresponding to it (assuming that it’s present in the database). One issue that can arise with this, however, is that the name of the park in output might be different from that of the database. To mitigate this, we’re going to instead compare the coordinates of what’s in output to the rows inside of trails_new.db and trails.db. The idea is that if two parks are close enough to each other in terms of their coordinates, then they should represent the same thing. So, we’re going to make two functions that do similar (but different) things. One will be called fetch_park_info_based_on_coords which will look at parks (i.e., outside of California), and the other will be called fetch_trail_info_based_on_coords\n\ndef fetch_park_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor() #Connect to database\n\n    for table_name in state_name_to_table_name.values(): #This is what we made earlier\n        cursor.execute(f\"SELECT * FROM \\\"{table_name}\\\"\") #Grab everything from the table\n        rows = cursor.fetchall()\n\n        for row in rows: #For each row\n            coords_text = row[2]  # Coords are in the third column\n            try:\n                coords = eval(coords_text) #Kept as a tuple, essentially\n                lat_diff = abs(coords[0] - latitude)\n                long_diff = abs(coords[1] - longitude)\n\n                if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                    return row[3:]  # Don't need name and coords\n            except:\n                continue\n\n    conn.close()\n    return None\n\ndef fetch_trail_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n    table_name = 'California'  #Only getting CA trails\n\n    cursor.execute(f\"SELECT * FROM {table_name}\") #Grab everything\n    rows = cursor.fetchall()\n\n    for row in rows:\n        coords_text = row[1]  # Coords are in column 2\n        try:\n            coords = eval(coords_text)\n            lat_diff = abs(coords[0] - latitude)\n            long_diff = abs(coords[1] - longitude)\n\n            if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                return row[2:]\n        except:\n            continue  # Skip rows with invalid 'Coords'\n\n    conn.close()\n    return None\n\nOkay, so, it will make a lot more sense if we actually inspect the structure of our database again. Click the link below to see screenshots of two .csv files: the first of parks in Wyoming, and the second is of trails in California:\nhttps://imgur.com/a/6fCixEt\nWith that out of the way, let’s dive into the code. We go through the mapping dictionary that we made previously and we grab all of the possible parks from each one. Then, we look at the third column (i.e., row[2], which represents the third entry in the row) which corresponds to the coordinates (see screenshot), and we record the absolute difference in the coordinates between a given latitude and longitude (we’ll be taking those from output–they’re individual columns rather than a tuple). If both of them are within a specified margin of error, then we’ve found our match. Note that we’re only going to return everything starting from the third column: the first 2 are just the name and coordinates of the trail.\nFor fetch_trail_info_based_on_coords, we have a very similar set-up except for the fact that the coordinates are in the second column, and we’re interested in returning everything after the first two.\nNow, let’s move on so we can see how we actually use these functions!"
  },
  {
    "objectID": "posts/final3 - Copy/FinalProject (1).html#putting-it-all-together",
    "href": "posts/final3 - Copy/FinalProject (1).html#putting-it-all-together",
    "title": "Trail Recommendations",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe first thing we’re going to do is to specify the names of the new columns that we want to put into california_df and non_california_df. I’ve just grabbed these from the database:\n\nnew_columns = [\n    'Trails (view details)', 'Total Distance', 'State Ranking',\n    'Access Road/Trail', 'White', 'Green', 'Blue', 'Black',\n    'Double Black Diamond', 'Proline'\n]\nnew_trail_columns = [\n    'Distance', 'Avg time', 'Climb', 'Descent', 'Activities',\n    'Riding Area', 'Difficulty Rating', 'Dogs Allowed',\n    'Local Popularity', 'Altitude start', 'Altitude end', 'Grade'\n]\n\nNow, all we need to do is iterate through the rows of non_california_df to match up the entires!\n\nmargin_lat = 0.1  # Decently generous\nmargin_long = 0.1\nfor index, row in non_california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']): #Some parks have NA coordinates\n        continue\n    park_info = fetch_park_info_based_on_coords('trails_new (1).db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n    #Remember, this grabs almost all of the columns if a match is found\n    if park_info:\n        non_california_df.loc[index, new_columns] = park_info #We can mass-add new columns\n\nIn the above code, we use the fetch_park_info_based_on_coords function to essentially create a new data frame that contains the information that we want once we match the coordinates. Then, we insert all of these as new columns, taking advantage of the .loc() method from pandas. Now let’s do the same thing for the California df:\n\n\nfor index, row in california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']):\n        continue\n\n    park_info = fetch_trail_info_based_on_coords('trails.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n\n    if park_info and len(park_info) == len(new_trail_columns):\n        california_df.loc[index, new_trail_columns] = park_info\n    else:\n        pass\n\nOkay, let’s take a look at our results!\n\nnon_california_df\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\n...\nTrails (view details)\nTotal Distance\nState Ranking\nAccess Road/Trail\nWhite\nGreen\nBlue\nBlack\nDouble Black Diamond\nProline\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n...\n60\n194 miles\n#9,609\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n...\n26\n53 miles\n#4,761\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n...\n25\n177 miles\n#9,011\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n9 rows × 22 columns\n\n\n\nSuccess! It looks like we unfortunately have a few NA values. Unfortunately, it’s hard to guarantee precision in the coordinates. We only had one trail for California:\n\ncalifornia_df\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n\n\n\n\n\nWait, really? I thought we would’ve had this for sure in our database…\nOn closer inspection, we actually do, but hte coordinates on TrailForks versus what we got from the National Park data is a bit off. On the TrailForks page for Zabriskie Point, the coordinates are (36.420820, -116.810120), which is just outside the margin of error."
  },
  {
    "objectID": "posts/flask/Flask Tutorial.html",
    "href": "posts/flask/Flask Tutorial.html",
    "title": "Flask Tutorial",
    "section": "",
    "text": "Overview\nToday we are going to build a simple web application with Flask. The app we construct will let us doing the following: 1. Allow the user to submit messages 2. Allow the user to view a sample of submitted messages\n\n\n1. Enable Submissions\nTo start let us create a GitHub repository with our files. Here is the link to the one I created in this tutorial: https://github.com/TylerNguyen25/Flask-Webapp. Now, we create a submit html template so that we can do the following:\n\nA text box for submitting a message\nA text box for submitting the name of the use\nA “submit” button\n\nLet us start with a template called base such that we can extend our submit html template off it.\n\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}Test{% endblock %} - PIC16B Website&lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;PIC16B Example!&lt;/h1&gt;\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;li&gt;&lt;a href=\"{{ url_for('main') }}\"&gt;Submit Message&lt;/a&gt;&lt;/li&gt; #link where the main() function directs\n  &lt;li&gt;&lt;a href=\"{{ url_for('ask')}}\"&gt;View Message&lt;/a&gt;&lt;/li&gt; #link where the ask() function directs\n  &lt;li&gt;&lt;a href=\"{{ url_for('deleted')}}\"&gt;Delete Messages&lt;/a&gt;&lt;/li&gt; #link where the deleted() function directs\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n&lt;!-- Will be used for submit.html, view.html, and delete.html --&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nNow let us create our submit html!\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h2&gt;{% block title %}Submit Messages{% endblock %}&lt;/h2&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;form method=\"post\"&gt;\n      Your message:&lt;br&gt;\n      &lt;input name=\"message\" id=\"message\"&gt; &lt;br&gt; #creates textbox to input our message\n\n      Your name or handle:&lt;br&gt;\n      &lt;input name=\"name\" id=\"name\"&gt;&lt;br&gt; #creates textbok to input our name handle\n\n      &lt;input type=\"submit\" value=\"Submit form\"&gt;\n  &lt;/form&gt;\n  {% if note %}\n  &lt;b class = green&gt;{{note}}&lt;/b&gt;\n  {% endif %}\n{% endblock %}\n\nWith our templates, this is what our page looks like.\n\n\n\nsubmit.jpg\n\n\nNext we create a new .py file named app.py which will store our three Python functions for database management: get_message_db(), insert_message(request), and delete_messages().\nThis is what our first function get_message_db() looks like:\n\nfrom flask import Flask, render_template, request, redirect, url_for, abort, g\nimport sqlite3\n\napp = Flask(__name__)\n\ndef get_message_db():\n    try: #return database if it already exists\n        return g.message_db\n    except: #if it does not exist, create one\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (message TEXT, handle TEXT)' \n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\n\nThe function above returns our database if it already exists, and if it doesn’t we create one through the SQL command cmd.\nThe second funtion we write is insert_message(request), which will handle inserting a user’s message into the database.\n\ndef insert_message(request):\n    db = get_message_db()\n    conn = db.cursor()\n\n    msg = request.form['message'] #store the input message from submit.html\n    handle = request.form['name'] #store the input nameh handle from submit.html\n    cmd = f\"INSERT into messages VALUES ('{msg}','{handle}')\" #insert these values into the database \n    conn.execute(cmd)\n    db.commit()\n    db.close()\n\nThis function inserts the message and handle to the database which the user submitted on the submit.html page.\nAnd the final function we create for database management is delete_messages.\n\ndef delete_messages():\n    db = get_message_db()\n    conn = db.cursor()\n    cmd = 'DELETE FROM messages' #delete all rows in database \n    conn.execute(cmd)\n    db.commit()\n    db.close()\n\nThis function deletes all messages from the database so that the user can verify that their message is in fact displayed in the database.\nIn addition to these functions, we write a function called main() to render_template() the submit.html template. When the instance is a GET method, we simply render submit.html. On the other hand, when the instance is POST, we call insert_message(request) to record the message to the database and then finally render submit.html. We also pass a message letting the user know that their message was received.\n\n@app.route('/', methods = ['POST', 'GET'])\ndef main():\n    if request.method == 'GET': #if the request is GET\n        return render_template('submit.html') #render the submit.html file without doing anything\n    else: #if the request is POST \n        msg = \"Message received\"\n        insert_message(request)\n        return render_template('submit.html', note = msg ) #render the submit.html file while submitting requests.\n\n\n\n2. View Random Submissions\nAs of now, we have the ability to submit and record a message into a database. This section illustrates to users how to view these messages.\nTo do so, we write a function random_messages(n) which returns a collection of n random messages from our database.\n\ndef random_messages(n):\n    db = get_message_db()\n    conn = db.cursor()\n    cmd = f'SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}' #select n random messages from our database \n    messages = conn.execute(cmd).fetchall()\n    db.commit()\n    db.close()\n    return messages \n\nIn order to view these random message, we must create a ask.html file.\n\n{% extends \"base.html\" %}\n\n{% block header %}\n&lt;h2&gt;{% block title %}View Messages{% endblock %}&lt;/h2&gt;\n{% endblock %}\n\n{% block content %}\n&lt;ul&gt;\n    {% for message in messages %} #for each message out of our n random messages\n        &lt;p&gt;{{message[0]}}&lt;/p&gt; #display the message \n        &lt;p&gt;- &lt;em&gt;{{message[1]}}&lt;/em&gt;&lt;/p&gt; #display the name handle \n        &lt;br&gt;\n    {% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\nIn this html file, messages is a list of tuples. Hence in order to access the message and user handle, we must construct a for loop and then index each indiviual message tuple. Here, message[0] contains the message and message[1] contains the user handle.\nFinally, we create ask to render ask.html.\n\n@app.route('/view')\ndef ask():\n    messages = random_messages(5)\n    return render_template('ask.html', messages = messages) #render ask.html with 5 random messages \n\nIn ask() we called random_messages(5) to generate five random messages from the database and then pass it into the render_template as an argument. This is the list of tuples which the ask.html file iterates over.\nThe last function in our file is the deleted() function. This function calls delete_messages() to clean all messages in our databse and returns a message to the user after doing so.\n\n@app.route('/delete')\ndef deleted():\n    delete_messages()\n    msg = \"Database has been cleaned\"\n    return render_template('delete.html', note = msg)\n\n\n\n3. Custom Display\nFinally, we create a style.css file in order to customize the way our application appears to the user. In this csss file, we change the font, font-size, background-colors, and alignment of our displays.\n\nhtml{\n    font-family: Garamond, serif;\n}\n\nh1{\n    font-size: 32px;\n    text-align:center;\n    background-color:#C39E6D;\n}\n\nli{\n    background-color: #55565A;\n    font-size: 22px;\n    text-align:center;\n    list-style-type: none;\n}\n\nli a {\n    color: white;\n}\n\na:visited {\n    background-color: cyan;\n  }\n  \na:hover {\n    background-color: lightgreen;\n  }\n\np{\n    font-size: 22px;\n}\n\n.submit-link:hover,\n.view-link:hover {\n  background-color: #0056b3;\n}\n\n.green {\n    color:white;\n    background-color: green;\n}\n\n.red{\n    color:white;\n    background-color: red;\n}\n\n\n\n4. Demo\nFirst, we run the following command in the directory which holds our file\n\nexport FLASK_ENV=development\nflask run\n\nNow, suppose a user submits the message below.\n\n\n\ndemo1.JPG\n\n\nGoing to the view page by clicking on ‘View Message’, we would get the following output.\n\n\n\nDemo2.JPG\n\n\nSuppose we want to delete all messages from the database. Then when we click on ‘Delete Messages’ we get:\n\n\n\ndemo3.JPG\n\n\nAs we can see, our demo shows that our app runs just how we want it to!"
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html",
    "href": "posts/TMDB_scraper/TMDB.html",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "",
    "text": "What movie or TV shows share actors with your favorite movie or show?\nWe will use webscraping to solve the above question in this blog post.\nThis post has two parts. In the first, we go over webscraping for finding shared actors on TMDB. In the second, we use the webscraper’s results to make a visualization."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#a.-pick-a-movie-or-tv-show",
    "href": "posts/TMDB_scraper/TMDB.html#a.-pick-a-movie-or-tv-show",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "a. Pick a Movie or TV Show",
    "text": "a. Pick a Movie or TV Show\nFirst, we pick a tv show. I used my favorite show The Boys. The link to its TMDB page is here: https://www.themoviedb.org/tv/76479-the-boys"
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#b.-project-initialization",
    "href": "posts/TMDB_scraper/TMDB.html#b.-project-initialization",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "b. Project Initialization",
    "text": "b. Project Initialization\nWe will now create a GitHub repository which will contain our Scrapy files. After, we open a terminal in the location we wish our files to be and type:\nscrapy startproject TMDB_scraper\nscd TMDB_scraper"
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#c.-alter-settings",
    "href": "posts/TMDB_scraper/TMDB.html#c.-alter-settings",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "c. Alter Settings",
    "text": "c. Alter Settings\nNow lets alter the settings.py file. We need to modify the User_Agent variable to equal Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 such that we will not get 403 errors while scraping.\nWe also add the line CLOSESPIDER_PAGECOUNT = 20 to prevent our webscraper from scraping too many pages."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#a.-parseself-response",
    "href": "posts/TMDB_scraper/TMDB.html#a.-parseself-response",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "a. parse(self, response)",
    "text": "a. parse(self, response)\n\ndef parse(self, response):\n        \"\"\"\n        Sends webscraper to the full cast and crew page \n        \"\"\"\n        next_page = response.css(\"a[href*=cast]::attr(href)\").get() #css command to redirect to the full cast page url\n        yield response.follow(next_page, callback = self.parse_full_credits) #redirects to the new url page and executes parse_full_credits\n\nThe response variable inside parse is the page we are inspecting. The variable next_page corresponds to the next page we wish to scrape. The value inside response.css was the css command used to redirect to the full cast url page. Finally we used yield response.follow in order to go to this next url, and the callback parameter we wrote is the function which we next execute."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#b.-parse_full_creditsself-response",
    "href": "posts/TMDB_scraper/TMDB.html#b.-parse_full_creditsself-response",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "b. parse_full_credits(self, response)",
    "text": "b. parse_full_credits(self, response)\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Sends webscraper to the individual actor's page\n        \"\"\"\n        actor_list = response.css('ol.people.credits:not(.crew) div.info  a[href*= \"person/\"]::attr(href)').getall()[0:20]\n        for actor in actor_list: #for every actor in the actor list url, execute the callback command parse_actor_page \n            yield response.follow(actor, callback = self.parse_actor_page)\n\nThe function which parse yields does the same thing parse does except it inspects the new response variable passed which is the full cast page. The variable actor_list corresponds to the next page we wish to next scrape and I choose to include only the first 20 actors in this list. The css command we used was to redirect to each individual actor’s page. Finally, we also used response.follow again in order to go to the individual actor’s url page, and the callback function wish we need to execute next is self.parse_actor_page."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#c.-parse_actor_pageself-response",
    "href": "posts/TMDB_scraper/TMDB.html#c.-parse_actor_pageself-response",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "c. parse_actor_page(self, response)",
    "text": "c. parse_actor_page(self, response)\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Returns a dictionary of every actor and each movie they worked on\n        \"\"\"\n        actor_name = response.css(\"h2.title a::text\").get() #css command which displays actor name \n        movie = response.xpath(\"//div//h3[text() = 'Acting']/following-sibling::table[1]//bdi/text()\").getall() \n        #xpath command which displays each movie/TV show actor worked on\n        for movie_or_TV_name in movie:\n            yield { #create a dictionary with actor name and movie/TV show \n                \"actor\": actor_name,\n                \"movie_or_TV_name\" : movie_or_TV_name\n            }\n\nThis function does the same thing the above two functions except it inspects the new response variable passed which is the individual actors page. We extract the actor names and every movie/TV show this actor worked on and store them in the actor and movies variables. For the movie path, we have to only look under the Acting table and to do this we use the following-sibling pseudo-element to do so. Finally, we create a for loop to go over each work per actor, and uses these variables to create a dictionary of {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}."
  },
  {
    "objectID": "posts/TMDB_scraper/TMDB.html#insights-and-recommendations",
    "href": "posts/TMDB_scraper/TMDB.html#insights-and-recommendations",
    "title": "Webscraping ‘The Boys’ with Scrapy",
    "section": "3. Insights and Recommendations",
    "text": "3. Insights and Recommendations\nWe now run the following command in our terminal to create a csv file named results2.csv containing our data:\nscrapy crawl tmdb_spider -o results2.csv -a subdir=76479-the-boys\nLet us now anaylze the results.\n\nimport pandas as pd \ndf = pd.read_csv(\"results2.csv\")\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nKarl Urban\nARK: The Animated Series\n\n\n1\nKarl Urban\nMortal Kombat 2\n\n\n2\nKarl Urban\nI, Object\n\n\n3\nKarl Urban\nGen V\n\n\n4\nKarl Urban\nThe Sea Beast\n\n\n...\n...\n...\n\n\n813\nJessie T. Usher\nLevel Up\n\n\n814\nJessie T. Usher\nThe Mentalist\n\n\n815\nJessie T. Usher\nHannah Montana\n\n\n816\nJessie T. Usher\nCriminal Minds\n\n\n817\nJessie T. Usher\nWithout a Trace\n\n\n\n\n818 rows × 2 columns\n\n\n\n\npopular = df.groupby(\"movie_or_TV_name\").count().sort_values(ascending = False, by = \"actor\")[0:24]\npopular.columns = [\"Actor Count\"]\npopular\n\n\n\n\n\n\n\n\nActor Count\n\n\nmovie_or_TV_name\n\n\n\n\n\nThe Boys\n21\n\n\nPrime Rewind: Inside The Boys\n10\n\n\nGen V\n8\n\n\nThe Boys Presents: Diabolical\n6\n\n\nLaw & Order\n4\n\n\nLaw & Order: Special Victims Unit\n4\n\n\nMano\n3\n\n\nThe Kelly Clarkson Show\n3\n\n\nThe Show\n3\n\n\nDeception\n3\n\n\nLate Night with Seth Meyers\n3\n\n\nTimeless\n3\n\n\nThe Equalizer\n3\n\n\nGen V - Prime Premiere\n3\n\n\nUndrafted\n2\n\n\nPerson of Interest\n2\n\n\nThe Ellen DeGeneres Show\n2\n\n\nParish\n2\n\n\nBones\n2\n\n\nDylan & Zoey\n2\n\n\nAunty Donna's Big Ol House of Fun\n2\n\n\nLevel Up\n2\n\n\nHalf & Half\n2\n\n\nSoul Food\n2\n\n\n\n\n\n\n\nAs we can see from the subset of the data above, some movies and TV shows are more popular than others. Obviously The Boys will be the most popular one because it is where we got our actor list from. So we create a pie graph which will show the next popular movies/TV shows.\n\nfrom plotly import express as px \nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nfig = px.pie(popular, values='Actor Count', names=popular.index, title='Top Recommendations')\nfig.update_traces(textposition='inside', textinfo='label+text')\nfig.show()\n\n\n\n\nAs we can see the pie graph clearly gives us the most popular movies. Obviously The Boys is first and we can see that the next three most popular are spin offs of the TV Show, Prime Rewind: Inside the Boys, Gen V, and The Boys Presents: Diabolical. The popular ones not related to the boys are Law & Order, Law  Order: Special Victims Unit, and Mano*.\nNice, now we are done!"
  }
]